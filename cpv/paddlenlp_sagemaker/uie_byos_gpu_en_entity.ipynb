{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PaddlePaddle BYOS\n",
    "\n",
    "## Pre-requisites\n",
    "\n",
    "This notebook shows how to use the SageMaker Python SDK to run your code in a local container before deploying to SageMaker's managed training or hosting environments.  This can speed up iterative testing and debugging while using the same familiar Python SDK interface.  Just change your estimator's `train_instance_type` to `local` (or `local_gpu` if you're using an ml.p2 or ml.p3 notebook instance).\n",
    "\n",
    "In order to use this feature you'll need to install docker-compose (and nvidia-docker if training with a GPU).\n",
    "\n",
    "**Note, you can only run a single local notebook at one time.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/bin/bash ./utils/setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The **SageMaker Python SDK** helps you deploy your models for training and hosting in optimized, productions ready containers in SageMaker. The SageMaker Python SDK is easy to use, modular, extensible and compatible with TensorFlow, MXNet, PyTorch and Chainer. This tutorial focuses on how to create a convolutional neural network model to train the [Cifar10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) using **PyTorch in local mode**.\n",
    "\n",
    "### Set up the environment\n",
    "\n",
    "This notebook was created and tested on a single ml.p2.xlarge notebook instance.\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these. Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the sagemaker.get_execution_role() with appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/DEMO-PaddleNLP'\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "\n",
    "# instance_type = 'local'\n",
    "\n",
    "# if subprocess.call('nvidia-smi') == 0:\n",
    "#     ## Set type to GPU if one is present\n",
    "#     instance_type = 'local_gpu'\n",
    "    \n",
    "# print(\"Instance type = \" + instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the data\n",
    "We use the ```sagemaker.Session.upload_data``` function to upload our datasets to an S3 location. The return value inputs identifies the location -- we will use this later when we start the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-west-2-064542430558/sagemaker/DEMO-PaddleNLP'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WORK_DIRECTORY = './shulexv7/'\n",
    "\n",
    "data_location = sagemaker_session.upload_data(WORK_DIRECTORY, key_prefix=prefix)\n",
    "data_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script Functions\n",
    "\n",
    "SageMaker invokes the main function defined within your training script for training. When deploying your trained model to an endpoint, the model_fn() is called to determine how to load your trained model. The model_fn() along with a few other functions list below are called to enable predictions on SageMaker.\n",
    "\n",
    "### [Predicting Functions](https://github.com/aws/sagemaker-pytorch-containers/blob/master/src/sagemaker_pytorch_container/serving.py)\n",
    "* model_fn(model_dir) - loads your model.\n",
    "* input_fn(serialized_input_data, content_type) - deserializes predictions to predict_fn.\n",
    "* output_fn(prediction_output, accept) - serializes predictions from predict_fn.\n",
    "* predict_fn(input_data, model) - calls a model on data deserialized in input_fn.\n",
    "\n",
    "The model_fn() is the only function that doesn't have a default implementation and is required by the user for using PyTorch on SageMaker. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a training job using the sagemaker.PyTorch estimator\n",
    "\n",
    "The `PyTorch` class allows us to run our training function on SageMaker. We need to configure it with our training script, an IAM role, the number of training instances, and the training instance type. For local training with GPU, we could set this to \"local_gpu\".  In this case, `instance_type` was set above based on your whether you're running a GPU instance.\n",
    "\n",
    "After we've constructed our `PyTorch` object, we fit it using the data we uploaded to S3. Even though we're in local mode, using S3 as our data source makes sense because it maintains consistency with how SageMaker's distributed, managed training ingests data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Training using GPU instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training': 's3://sagemaker-us-west-2-064542430558/sagemaker/DEMO-PaddleNLP'}\n"
     ]
    }
   ],
   "source": [
    "inputs = {'training': data_location}\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare pretrained-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://opentuna.cn/pypi/web/simple/\n",
      "Requirement already satisfied: paddlepaddle-gpu in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (2.3.1)\n",
      "Requirement already satisfied: paddlenlp in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (2.4.1)\n",
      "Requirement already satisfied: numpy>=1.13 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from paddlepaddle-gpu->-r requirements.txt (line 2)) (1.21.2)\n",
      "Requirement already satisfied: astor in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from paddlepaddle-gpu->-r requirements.txt (line 2)) (0.8.1)\n",
      "Requirement already satisfied: paddle-bfloat==0.1.7 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from paddlepaddle-gpu->-r requirements.txt (line 2)) (0.1.7)\n",
      "Requirement already satisfied: requests>=2.20.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from paddlepaddle-gpu->-r requirements.txt (line 2)) (2.26.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from paddlepaddle-gpu->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: protobuf<=3.20.0,>=3.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from paddlepaddle-gpu->-r requirements.txt (line 2)) (3.19.4)\n",
      "Requirement already satisfied: opt-einsum==3.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from paddlepaddle-gpu->-r requirements.txt (line 2)) (3.3.0)\n",
      "Requirement already satisfied: decorator in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from paddlepaddle-gpu->-r requirements.txt (line 2)) (5.1.0)\n",
      "Requirement already satisfied: Pillow in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from paddlepaddle-gpu->-r requirements.txt (line 2)) (9.0.1)\n",
      "Requirement already satisfied: jieba in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from paddlenlp->-r requirements.txt (line 3)) (0.42.1)\n",
      "Requirement already satisfied: paddlefsl in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from paddlenlp->-r requirements.txt (line 3)) (1.1.0)\n",
      "Requirement already satisfied: colorama in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from paddlenlp->-r requirements.txt (line 3)) (0.4.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from paddlenlp->-r requirements.txt (line 3)) (2.6.1)\n",
      "Requirement already satisfied: dill<0.3.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from paddlenlp->-r requirements.txt (line 3)) (0.3.4)\n",
      "Requirement already satisfied: sentencepiece in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from paddlenlp->-r requirements.txt (line 3)) (0.1.97)\n",
      "Requirement already satisfied: colorlog in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from paddlenlp->-r requirements.txt (line 3)) (6.7.0)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from paddlenlp->-r requirements.txt (line 3)) (4.62.3)\n",
      "Requirement already satisfied: paddle2onnx in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from paddlenlp->-r requirements.txt (line 3)) (1.0.1)\n",
      "Requirement already satisfied: multiprocess<=0.70.12.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from paddlenlp->-r requirements.txt (line 3)) (0.70.12.2)\n",
      "Requirement already satisfied: seqeval in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from paddlenlp->-r requirements.txt (line 3)) (1.2.2)\n",
      "Requirement already satisfied: visualdl in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from paddlenlp->-r requirements.txt (line 3)) (2.4.1)\n",
      "Requirement already satisfied: responses<0.19 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets>=2.0.0->paddlenlp->-r requirements.txt (line 3)) (0.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets>=2.0.0->paddlenlp->-r requirements.txt (line 3)) (0.10.1)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets>=2.0.0->paddlenlp->-r requirements.txt (line 3)) (7.0.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets>=2.0.0->paddlenlp->-r requirements.txt (line 3)) (2021.11.1)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets>=2.0.0->paddlenlp->-r requirements.txt (line 3)) (1.3.4)\n",
      "Requirement already satisfied: xxhash in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets>=2.0.0->paddlenlp->-r requirements.txt (line 3)) (3.1.0)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets>=2.0.0->paddlenlp->-r requirements.txt (line 3)) (3.8.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets>=2.0.0->paddlenlp->-r requirements.txt (line 3)) (5.4.1)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets>=2.0.0->paddlenlp->-r requirements.txt (line 3)) (21.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests>=2.20.0->paddlepaddle-gpu->-r requirements.txt (line 2)) (2.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests>=2.20.0->paddlepaddle-gpu->-r requirements.txt (line 2)) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests>=2.20.0->paddlepaddle-gpu->-r requirements.txt (line 2)) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests>=2.20.0->paddlepaddle-gpu->-r requirements.txt (line 2)) (2021.10.8)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from seqeval->paddlenlp->-r requirements.txt (line 3)) (1.0.1)\n",
      "Requirement already satisfied: matplotlib in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from visualdl->paddlenlp->-r requirements.txt (line 3)) (3.5.0)\n",
      "Requirement already satisfied: bce-python-sdk in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from visualdl->paddlenlp->-r requirements.txt (line 3)) (0.8.74)\n",
      "Requirement already satisfied: Flask-Babel>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from visualdl->paddlenlp->-r requirements.txt (line 3)) (2.0.0)\n",
      "Requirement already satisfied: flask>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from visualdl->paddlenlp->-r requirements.txt (line 3)) (2.0.2)\n",
      "Requirement already satisfied: Jinja2>=3.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from flask>=1.1.1->visualdl->paddlenlp->-r requirements.txt (line 3)) (3.0.3)\n",
      "Requirement already satisfied: Werkzeug>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from flask>=1.1.1->visualdl->paddlenlp->-r requirements.txt (line 3)) (2.0.3)\n",
      "Requirement already satisfied: click>=7.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from flask>=1.1.1->visualdl->paddlenlp->-r requirements.txt (line 3)) (8.0.3)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from flask>=1.1.1->visualdl->paddlenlp->-r requirements.txt (line 3)) (2.0.1)\n",
      "Requirement already satisfied: pytz in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp->-r requirements.txt (line 3)) (2021.3)\n",
      "Requirement already satisfied: Babel>=2.3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp->-r requirements.txt (line 3)) (2.9.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets>=2.0.0->paddlenlp->-r requirements.txt (line 3)) (4.0.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets>=2.0.0->paddlenlp->-r requirements.txt (line 3)) (3.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from packaging->datasets>=2.0.0->paddlenlp->-r requirements.txt (line 3)) (3.0.6)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp->-r requirements.txt (line 3)) (3.0.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp->-r requirements.txt (line 3)) (1.7.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp->-r requirements.txt (line 3)) (1.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp->-r requirements.txt (line 3)) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp->-r requirements.txt (line 3)) (21.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp->-r requirements.txt (line 3)) (1.7.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp->-r requirements.txt (line 3)) (4.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp->-r requirements.txt (line 3)) (5.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp->-r requirements.txt (line 3)) (1.2.0)\n",
      "Requirement already satisfied: future>=0.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from bce-python-sdk->visualdl->paddlenlp->-r requirements.txt (line 3)) (0.18.2)\n",
      "Requirement already satisfied: pycryptodome>=3.8.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from bce-python-sdk->visualdl->paddlenlp->-r requirements.txt (line 3)) (3.15.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from matplotlib->visualdl->paddlenlp->-r requirements.txt (line 3)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from matplotlib->visualdl->paddlenlp->-r requirements.txt (line 3)) (4.28.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from matplotlib->visualdl->paddlenlp->-r requirements.txt (line 3)) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from matplotlib->visualdl->paddlenlp->-r requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from Jinja2>=3.0->flask>=1.1.1->visualdl->paddlenlp->-r requirements.txt (line 3)) (2.0.1)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p38/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-10-21 03:56:30,132] [    INFO]\u001b[0m - Downloading model_state.pdparams from https://bj.bcebos.com/paddlenlp/taskflow/information_extraction/uie_base_en_v1.1/model_state.pdparams\u001b[0m\n",
      "100%|██████████| 418M/418M [00:50<00:00, 8.66MB/s]   \n",
      "\u001b[32m[2022-10-21 03:57:23,188] [    INFO]\u001b[0m - Downloading model_config.json from https://bj.bcebos.com/paddlenlp/taskflow/information_extraction/uie_base_en/model_config.json\u001b[0m\n",
      "100%|██████████| 347/347 [00:00<00:00, 319kB/s]\n",
      "\u001b[32m[2022-10-21 03:57:24,374] [    INFO]\u001b[0m - Downloading vocab.txt from https://bj.bcebos.com/paddlenlp/taskflow/information_extraction/uie_base_en/vocab.txt\u001b[0m\n",
      "100%|██████████| 226k/226k [00:03<00:00, 61.6kB/s] \n",
      "\u001b[32m[2022-10-21 03:57:29,220] [    INFO]\u001b[0m - Downloading special_tokens_map.json from https://bj.bcebos.com/paddlenlp/taskflow/information_extraction/uie_base_en/special_tokens_map.json\u001b[0m\n",
      "100%|██████████| 112/112 [00:00<00:00, 104kB/s]\n",
      "\u001b[32m[2022-10-21 03:57:30,243] [    INFO]\u001b[0m - Downloading tokenizer_config.json from https://bj.bcebos.com/paddlenlp/taskflow/information_extraction/uie_base_en/tokenizer_config.json\u001b[0m\n",
      "100%|██████████| 172/172 [00:00<00:00, 162kB/s]\n",
      "W1021 03:57:31.276192 17285 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.6, Runtime API Version: 10.2\n",
      "W1021 03:57:31.279335 17285 gpu_resources.cc:91] device: 0, cuDNN Version: 8.0.\n",
      "\u001b[32m[2022-10-21 03:57:32,992] [    INFO]\u001b[0m - Converting to the inference model cost a little time.\u001b[0m\n",
      "\u001b[32m[2022-10-21 03:57:51,313] [    INFO]\u001b[0m - The inference model save in the path:./uie-base-en/taskflow/information_extraction/uie-base-en/static/inference\u001b[0m\n",
      "\u001b[32m[2022-10-21 03:57:52,663] [    INFO]\u001b[0m - We are using <class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'> to load './uie-base-en/taskflow/information_extraction/uie-base-en'.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp import Taskflow\n",
    "schema = ['时间', '选手', '赛事名称']\n",
    "ie = Taskflow(\"information_extraction\", model='uie-base-en', schema=schema, home_path='./uie-base-en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload uie-base-en pretrain\n",
    "\n",
    "uie_en_model_s3 = sagemaker.Session().upload_data(path = \"./uie-base-en/taskflow/information_extraction/uie-base-en\", key_prefix=\"model_uie_base_en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-21 05:26:26 Starting - Starting the training job...\n",
      "2022-10-21 05:26:54 Starting - Preparing the instances for trainingProfilerReport-1666329986: InProgress\n",
      ".........\n",
      "2022-10-21 05:28:21 Downloading - Downloading input data...\n",
      "2022-10-21 05:28:56 Training - Downloading the training image.....................\n",
      "2022-10-21 05:32:23 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-10-21 05:32:26,084 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-10-21 05:32:26,112 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-10-21 05:32:26,119 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-10-21 05:32:40,776 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mLooking in indexes: https://opentuna.cn/pypi/web/simple/\u001b[0m\n",
      "\u001b[34mCollecting paddlepaddle-gpu\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/fb/f2/6b6ae62d5ecd916d61e1f527cb14990038d473cc670c30045f80b557e6e1/paddlepaddle_gpu-2.3.1-cp38-cp38-manylinux1_x86_64.whl (393.9 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 393.9/393.9 MB 1.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting paddlenlp\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/8e/e1/94cdbaca400a57687a8529213776468f003b64b6e35a6f4acf6b6539f543/paddlenlp-2.3.4-py3-none-any.whl (1.4 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 MB 1.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting opt-einsum==3.3.0\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/bc/19/404708a7e54ad2798907210462fd950c3442ea51acc8790f3da48d2bee8b/opt_einsum-3.3.0-py3-none-any.whl (65 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.5/65.5 kB 6.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting paddle-bfloat==0.1.7\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/76/d7/ba0e1aeec33e20c78af5cf2fdbb7e7cabfe4679557e68759a17c97e03540/paddle_bfloat-0.1.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (385 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 385.5/385.5 kB 8.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.20.0 in /opt/conda/lib/python3.8/site-packages (from paddlepaddle-gpu->-r requirements.txt (line 2)) (2.27.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.13 in /opt/conda/lib/python3.8/site-packages (from paddlepaddle-gpu->-r requirements.txt (line 2)) (1.22.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from paddlepaddle-gpu->-r requirements.txt (line 2)) (5.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Pillow in /opt/conda/lib/python3.8/site-packages (from paddlepaddle-gpu->-r requirements.txt (line 2)) (9.1.1)\u001b[0m\n",
      "\u001b[34mCollecting protobuf<=3.20.0,>=3.1.0\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/88/88/cd55f87e896b82a3aba8e6c0affc077de51f7321cf730622b17ef7b0f69c/protobuf-3.20.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 4.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting astor\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/c3/88/97eef84f48fa04fbd6750e62dcceafba6c63c81b7ac1420856c8dcc0a3f9/astor-0.8.1-py2.py3-none-any.whl (27 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from paddlepaddle-gpu->-r requirements.txt (line 2)) (1.16.0)\u001b[0m\n",
      "\u001b[34mCollecting multiprocess<=0.70.12.2\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/e6/22/b09b8394f8c86ff0cfebd725ea96bba0accd4a4b2be437bcba6a0cf7d1c3/multiprocess-0.70.12.2-py38-none-any.whl (128 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 128.3/128.3 kB 8.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting paddlefsl\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/fb/4a/25d1959a8f1fe5ee400f32fc9fc8b56d4fd6fc25315e23c0171f6e705e2a/paddlefsl-1.1.0-py3-none-any.whl (101 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 101.0/101.0 kB 7.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting jieba\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/c6/cb/18eeb235f833b726522d7ebed54f2278ce28ba9438e3135ab0278d9792a2/jieba-0.42.1.tar.gz (19.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.2/19.2 MB 15.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting colorlog\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/7d/54/e24efe5469ecb2710112055de87a2900e9494810bcfc25c12c7a0723eb64/colorlog-6.6.0-py2.py3-none-any.whl (11 kB)\u001b[0m\n",
      "\u001b[34mCollecting seqeval\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.6/43.6 kB 3.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting dill<0.3.5\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/b6/c3/973676ceb86b60835bb3978c6db67a5dc06be6cfdbd14ef0f5a13e3fc9fd/dill-0.3.4-py2.py3-none-any.whl (86 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.9/86.9 kB 276.0 kB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting datasets>=2.0.0\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/98/29/f381f8a633fed2c4f41c191498c3bc43d91a8e44c5202a8b0b2bd8b1acf3/datasets-2.3.2-py3-none-any.whl (362 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 362.3/362.3 kB 938.2 kB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: colorama in /opt/conda/lib/python3.8/site-packages (from paddlenlp->-r requirements.txt (line 3)) (0.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from paddlenlp->-r requirements.txt (line 3)) (4.61.2)\u001b[0m\n",
      "\u001b[34mCollecting paddle2onnx\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/78/76/811c8c897d68e211bc7ba13fa6161f54747eb717bffae80db0ea09ca2e43/paddle2onnx-0.9.8-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.9 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.9/2.9 MB 5.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/68/91/ded0f64f90abfc5413c620fc345a0aef1e7ff5addda8704cc6b3bf589c64/sentencepiece-0.1.96-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 15.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets>=2.0.0->paddlenlp->-r requirements.txt (line 3)) (6.0.0)\u001b[0m\n",
      "\u001b[34mCollecting responses<0.19\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/79/f3/2b3a6dc5986303b3dd1bbbcf482022acb2583c428cd23f0b6d37b1a1a519/responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from datasets>=2.0.0->paddlenlp->-r requirements.txt (line 3)) (2022.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets>=2.0.0->paddlenlp->-r requirements.txt (line 3)) (21.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets>=2.0.0->paddlenlp->-r requirements.txt (line 3)) (1.2.4)\u001b[0m\n",
      "\u001b[34mCollecting tqdm\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/8a/c4/d15f1e627fff25443ded77ea70a7b5532d6371498f9285d44d62587e209c/tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.4/78.4 kB 12.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting xxhash\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/6a/cf/50f4cfde85d90c2b3e9c98b46e17d190bbdd97b54d3e0876e1d9360e487f/xxhash-3.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.1/212.1 kB 13.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiohttp\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/38/71/e1db3f96fa85f77906ef002a08fa8d02dbdb3292180d41eb1b17ddab72bf/aiohttp-3.8.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 6.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0.0,>=0.1.0\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/d8/2c/9af8451ab780598e3b26a84d4f0e3844841456657401eb6843fdb622bb41/huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 101.5/101.5 kB 12.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests>=2.20.0->paddlepaddle-gpu->-r requirements.txt (line 2)) (2.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.20.0->paddlepaddle-gpu->-r requirements.txt (line 2)) (2022.5.18.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.20.0->paddlepaddle-gpu->-r requirements.txt (line 2)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.20.0->paddlepaddle-gpu->-r requirements.txt (line 2)) (1.26.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.8/site-packages (from seqeval->paddlenlp->-r requirements.txt (line 3)) (1.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets>=2.0.0->paddlenlp->-r requirements.txt (line 3)) (5.4.1)\u001b[0m\n",
      "\u001b[34mCollecting filelock\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/a6/d5/17f02b379525d1ff9678bfa58eb9548f561c8826deb0b85797aa0eed582d/filelock-3.7.1-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets>=2.0.0->paddlenlp->-r requirements.txt (line 3)) (3.10.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing<3,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets>=2.0.0->paddlenlp->-r requirements.txt (line 3)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp->-r requirements.txt (line 3)) (1.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp->-r requirements.txt (line 3)) (1.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp->-r requirements.txt (line 3)) (2.2.0)\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/3b/76/3d7c273b91e6dc914859f8752d42b763f39ae83782ec9a063a526c816977/frozenlist-1.3.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 158.7/158.7 kB 14.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0.0a3\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/d6/c1/8991e7c5385b897b8c020cdaad718c5b087a6626d1d11a23e1ea87e325a7/async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/3b/87/fe94898f2d44a93a35d5aa74671ed28094d80753a1113d68b799fab6dc22/aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/8f/39/a7e04961b4c00d68aba337e3fdef9fd4f666dcd98f41725067a1de5d3399/multidict-6.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.3/121.3 kB 10.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0\u001b[0m\n",
      "\u001b[34mDownloading https://opentuna.cn/pypi/web/packages/aa/a6/a4ddcb1c3d93fc5d77a19b1ec338a3efec65b44345168d8ac9bf8461224a/yarl-1.7.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (308 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 308.6/308.6 kB 10.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp->-r requirements.txt (line 3)) (21.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets>=2.0.0->paddlenlp->-r requirements.txt (line 3)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets>=2.0.0->paddlenlp->-r requirements.txt (line 3)) (2021.3)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: jieba, seqeval\u001b[0m\n",
      "\u001b[34mBuilding wheel for jieba (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for jieba (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314477 sha256=5fc1d8f8cefa957fd095916b8308d5d3a19c860a183c3907124744ccc7c1adf7\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/e9/6d/0d/92e938a9f51144388ebba6a81ebe4206fdd93f9c9de1434ec2\u001b[0m\n",
      "\u001b[34mBuilding wheel for seqeval (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for seqeval (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16170 sha256=9ec5a8fd9874e18146eacbc37b3416e24a5a1611eee90f17b27de0025302f83e\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/6d/ff/58/c7ebcaa099f483531e06ca79ad5802e594d1c97c96c9c0f200\u001b[0m\n",
      "\u001b[34mSuccessfully built jieba seqeval\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sentencepiece, paddle2onnx, paddle-bfloat, jieba, xxhash, tqdm, protobuf, opt-einsum, multidict, frozenlist, filelock, dill, colorlog, async-timeout, astor, yarl, responses, paddlepaddle-gpu, paddlefsl, multiprocess, huggingface-hub, aiosignal, seqeval, aiohttp, datasets, paddlenlp\u001b[0m\n",
      "\u001b[34mAttempting uninstall: tqdm\u001b[0m\n",
      "\u001b[34mFound existing installation: tqdm 4.61.2\u001b[0m\n",
      "\u001b[34mUninstalling tqdm-4.61.2:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled tqdm-4.61.2\u001b[0m\n",
      "\u001b[34mAttempting uninstall: protobuf\u001b[0m\n",
      "\u001b[34mFound existing installation: protobuf 3.20.1\u001b[0m\n",
      "\u001b[34mUninstalling protobuf-3.20.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled protobuf-3.20.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: dill\u001b[0m\n",
      "\u001b[34mFound existing installation: dill 0.3.5.1\u001b[0m\n",
      "\u001b[34mUninstalling dill-0.3.5.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled dill-0.3.5.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: multiprocess\u001b[0m\n",
      "\u001b[34mFound existing installation: multiprocess 0.70.13\u001b[0m\n",
      "\u001b[34mUninstalling multiprocess-0.70.13:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled multiprocess-0.70.13\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34mpathos 0.2.9 requires dill>=0.3.5.1, but you have dill 0.3.4 which is incompatible.\u001b[0m\n",
      "\u001b[34mpathos 0.2.9 requires multiprocess>=0.70.13, but you have multiprocess 0.70.12.2 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed aiohttp-3.8.1 aiosignal-1.2.0 astor-0.8.1 async-timeout-4.0.2 colorlog-6.6.0 datasets-2.3.2 dill-0.3.4 filelock-3.7.1 frozenlist-1.3.0 huggingface-hub-0.8.1 jieba-0.42.1 multidict-6.0.2 multiprocess-0.70.12.2 opt-einsum-3.3.0 paddle-bfloat-0.1.7 paddle2onnx-0.9.8 paddlefsl-1.1.0 paddlenlp-2.3.4 paddlepaddle-gpu-2.3.1 protobuf-3.20.0 responses-0.18.0 sentencepiece-0.1.96 seqeval-1.2.2 tqdm-4.64.0 xxhash-3.0.0 yarl-1.7.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.1.2 -> 22.3\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2022-10-21 05:34:00,490 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2022-10-21 05:34:00,490 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2022-10-21 05:34:00,577 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 16,\n",
      "        \"dev_path\": \"/opt/ml/input/data/training/dev.txt\",\n",
      "        \"device\": \"gpu\",\n",
      "        \"learning_rate\": 1e-05,\n",
      "        \"logging_steps\": 1,\n",
      "        \"max_seq_len\": 512,\n",
      "        \"model\": \"uie-base\",\n",
      "        \"num_epochs\": 10,\n",
      "        \"save_dir\": \"/opt/ml/model\",\n",
      "        \"seed\": 1000,\n",
      "        \"train_path\": \"/opt/ml/input/data/training/train.txt\",\n",
      "        \"valid_steps\": 10\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2022-10-21-05-25-33-950\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-064542430558/pytorch-training-2022-10-21-05-25-33-950/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"finetune\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"finetune.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch_size\":16,\"dev_path\":\"/opt/ml/input/data/training/dev.txt\",\"device\":\"gpu\",\"learning_rate\":1e-05,\"logging_steps\":1,\"max_seq_len\":512,\"model\":\"uie-base\",\"num_epochs\":10,\"save_dir\":\"/opt/ml/model\",\"seed\":1000,\"train_path\":\"/opt/ml/input/data/training/train.txt\",\"valid_steps\":10}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=finetune.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=finetune\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-064542430558/pytorch-training-2022-10-21-05-25-33-950/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":16,\"dev_path\":\"/opt/ml/input/data/training/dev.txt\",\"device\":\"gpu\",\"learning_rate\":1e-05,\"logging_steps\":1,\"max_seq_len\":512,\"model\":\"uie-base\",\"num_epochs\":10,\"save_dir\":\"/opt/ml/model\",\"seed\":1000,\"train_path\":\"/opt/ml/input/data/training/train.txt\",\"valid_steps\":10},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2022-10-21-05-25-33-950\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-064542430558/pytorch-training-2022-10-21-05-25-33-950/source/sourcedir.tar.gz\",\"module_name\":\"finetune\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"finetune.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch_size\",\"16\",\"--dev_path\",\"/opt/ml/input/data/training/dev.txt\",\"--device\",\"gpu\",\"--learning_rate\",\"1e-05\",\"--logging_steps\",\"1\",\"--max_seq_len\",\"512\",\"--model\",\"uie-base\",\"--num_epochs\",\"10\",\"--save_dir\",\"/opt/ml/model\",\"--seed\",\"1000\",\"--train_path\",\"/opt/ml/input/data/training/train.txt\",\"--valid_steps\",\"10\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=16\u001b[0m\n",
      "\u001b[34mSM_HP_DEV_PATH=/opt/ml/input/data/training/dev.txt\u001b[0m\n",
      "\u001b[34mSM_HP_DEVICE=gpu\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=1e-05\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_STEPS=1\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_SEQ_LEN=512\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL=uie-base\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_EPOCHS=10\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=1000\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_PATH=/opt/ml/input/data/training/train.txt\u001b[0m\n",
      "\u001b[34mSM_HP_VALID_STEPS=10\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python finetune.py --batch_size 16 --dev_path /opt/ml/input/data/training/dev.txt --device gpu --learning_rate 1e-05 --logging_steps 1 --max_seq_len 512 --model uie-base --num_epochs 10 --save_dir /opt/ml/model --seed 1000 --train_path /opt/ml/input/data/training/train.txt --valid_steps 10\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:34:15,032] [    INFO]#033[0m - Downloading resource files...#033[0m\u001b[0m\n",
      "\u001b[34m0%|          | 0/460749 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 3/460749 [00:00<9:24:39, 13.60it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 19/460749 [00:00<4:15:36, 30.04it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 35/460749 [00:00<2:52:58, 44.39it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 51/460749 [00:01<2:23:57, 53.34it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 67/460749 [00:01<2:09:47, 59.15it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 83/460749 [00:01<2:01:50, 63.01it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 99/460749 [00:01<1:56:59, 65.62it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 115/460749 [00:02<1:53:55, 67.39it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 131/460749 [00:02<2:25:58, 52.59it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 147/460749 [00:02<2:13:58, 57.30it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 163/460749 [00:02<2:05:48, 61.02it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 179/460749 [00:03<2:00:14, 63.84it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 195/460749 [00:03<1:56:41, 65.77it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 227/460749 [00:03<1:27:31, 87.69it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 243/460749 [00:03<1:32:29, 82.98it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 259/460749 [00:04<1:36:19, 79.67it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 275/460749 [00:04<1:39:22, 77.23it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 307/460749 [00:04<1:19:27, 96.57it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 323/460749 [00:04<1:26:07, 89.09it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 339/460749 [00:04<1:31:28, 83.89it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 371/460749 [00:05<1:15:39, 101.41it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 403/460749 [00:05<1:07:28, 113.70it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 419/460749 [00:05<1:15:47, 101.22it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 451/460749 [00:05<1:07:28, 113.70it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 499/460749 [00:06<53:22, 143.69it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 531/460749 [00:06<53:28, 143.43it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 579/460749 [00:06<46:34, 164.68it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 627/460749 [00:06<42:42, 179.53it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 675/460749 [00:06<40:19, 190.13it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 739/460749 [00:07<35:06, 218.41it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 803/460749 [00:07<32:08, 238.54it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 867/460749 [00:07<30:16, 253.11it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 947/460749 [00:07<26:57, 284.33it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 1043/460749 [00:08<23:24, 327.33it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 1123/460749 [00:08<22:45, 336.54it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 1219/460749 [00:08<21:01, 364.14it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 1331/460749 [00:08<18:54, 404.86it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 1459/460749 [00:08<16:49, 454.89it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 1603/460749 [00:09<14:58, 510.87it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 1747/460749 [00:09<13:53, 550.90it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 1923/460749 [00:09<12:19, 620.73it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 2099/460749 [00:09<11:24, 670.30it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 2307/460749 [00:10<10:12, 748.34it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 2531/460749 [00:10<09:16, 823.85it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 2771/460749 [00:10<08:30, 897.15it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 3027/460749 [00:10<07:51, 970.80it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 3299/460749 [00:10<07:17, 1045.00it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 3603/460749 [00:11<06:41, 1137.25it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 3907/460749 [00:11<06:19, 1204.40it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 4243/460749 [00:11<05:53, 1293.10it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 4595/460749 [00:11<05:31, 1376.65it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 4963/460749 [00:12<05:12, 1456.52it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 5347/460749 [00:12<04:56, 1533.99it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 5747/460749 [00:12<04:42, 1609.55it/s]\u001b[0m\n",
      "\u001b[34m1%|▏         | 6195/460749 [00:12<04:23, 1725.36it/s]\u001b[0m\n",
      "\u001b[34m1%|▏         | 6659/460749 [00:12<04:08, 1828.84it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 7155/460749 [00:13<03:53, 1944.06it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 7667/460749 [00:13<03:41, 2046.77it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 8211/460749 [00:13<03:29, 2163.08it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 8803/460749 [00:13<03:16, 2305.10it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 9427/460749 [00:14<03:04, 2449.08it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 10083/460749 [00:14<02:53, 2592.54it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 10771/460749 [00:14<02:44, 2736.55it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 11507/460749 [00:14<02:35, 2898.33it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 12275/460749 [00:15<02:26, 3053.85it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 13059/460749 [00:15<02:20, 3195.78it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 13891/460749 [00:15<02:13, 3351.14it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 14755/460749 [00:15<02:07, 3502.70it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 15667/460749 [00:15<02:01, 3673.66it/s]\u001b[0m\n",
      "\u001b[34m4%|▎         | 16627/460749 [00:16<01:55, 3852.29it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 17619/460749 [00:16<01:50, 4024.93it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 18659/460749 [00:16<01:45, 4209.57it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 19763/460749 [00:16<01:39, 4421.09it/s]\u001b[0m\n",
      "\u001b[34m5%|▍         | 20899/460749 [00:17<01:35, 4613.23it/s]\u001b[0m\n",
      "\u001b[34m5%|▍         | 22067/460749 [00:17<01:31, 4793.05it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 23283/460749 [00:17<01:27, 4982.53it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 24547/460749 [00:17<01:24, 5170.15it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 25843/460749 [00:17<01:21, 5354.30it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 27171/460749 [00:18<01:18, 5513.03it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 28515/460749 [00:18<01:16, 5671.74it/s]\u001b[0m\n",
      "\u001b[34m6%|▋         | 29907/460749 [00:18<01:13, 5834.30it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 31331/460749 [00:18<01:11, 5990.60it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 32787/460749 [00:19<01:09, 6129.11it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 34243/460749 [00:19<01:08, 6254.55it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 35747/460749 [00:19<01:06, 6377.41it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 37267/460749 [00:19<01:05, 6498.73it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 38803/460749 [00:19<01:03, 6606.82it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 40387/460749 [00:20<01:02, 6740.46it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 42003/460749 [00:20<01:00, 6882.12it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 43651/460749 [00:20<00:59, 7018.40it/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 45331/460749 [00:20<00:57, 7169.05it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 47043/460749 [00:21<00:56, 7311.13it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 48803/460749 [00:21<00:55, 7475.21it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 50611/460749 [00:21<00:53, 7648.26it/s]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 52451/460749 [00:21<00:52, 7815.73it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 54355/460749 [00:21<00:50, 8021.27it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 56307/460749 [00:22<00:49, 8227.81it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 58307/460749 [00:22<00:47, 8436.66it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 60403/460749 [00:22<00:46, 8702.26it/s]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 62547/460749 [00:22<00:44, 8970.80it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 64771/460749 [00:23<00:42, 9240.13it/s]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 67059/460749 [00:23<00:41, 9547.82it/s]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 69427/460749 [00:23<00:39, 9854.34it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 71859/460749 [00:23<00:38, 10151.83it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 74339/460749 [00:23<00:37, 10412.72it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 76819/460749 [00:24<00:36, 10612.07it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 79379/460749 [00:24<00:35, 10853.08it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 81923/460749 [00:24<00:34, 11005.57it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 84515/460749 [00:24<00:33, 11173.68it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 87107/460749 [00:25<00:33, 11283.57it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 89731/460749 [00:25<00:32, 11413.75it/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 92371/460749 [00:25<00:31, 11522.80it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 95059/460749 [00:25<00:31, 11624.57it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 97747/460749 [00:25<00:30, 11740.95it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 100467/460749 [00:26<00:30, 11859.27it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 103203/460749 [00:26<00:29, 11945.31it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 105955/460749 [00:26<00:29, 12047.84it/s]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 108707/460749 [00:26<00:29, 12117.35it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 111427/460749 [00:27<00:28, 12124.79it/s]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 114195/460749 [00:27<00:28, 12191.36it/s]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 116931/460749 [00:27<00:28, 12197.65it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 119699/460749 [00:27<00:27, 12246.36it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 122467/460749 [00:28<00:27, 12250.40it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 125235/460749 [00:28<00:27, 12269.29it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 128003/460749 [00:28<00:27, 12261.72it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 130755/460749 [00:28<00:26, 12236.91it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 133523/460749 [00:28<00:26, 12240.27it/s]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 136275/460749 [00:29<00:26, 12204.35it/s]\u001b[0m\n",
      "\u001b[34m30%|███       | 139015/460749 [00:29<00:21, 14626.61it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 140692/460749 [00:29<00:24, 12870.48it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 142130/460749 [00:29<00:27, 11416.47it/s]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 144611/460749 [00:29<00:27, 11334.44it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 147363/460749 [00:30<00:27, 11598.20it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 150147/460749 [00:30<00:26, 11814.57it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 152835/460749 [00:30<00:26, 11817.71it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 155619/460749 [00:30<00:25, 11946.65it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 158371/460749 [00:30<00:25, 12017.00it/s]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 161139/460749 [00:31<00:24, 12085.93it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 163923/460749 [00:31<00:24, 12156.08it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 166755/460749 [00:31<00:23, 12268.90it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 169587/460749 [00:31<00:23, 12344.78it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 172483/460749 [00:32<00:23, 12478.43it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 175363/460749 [00:32<00:22, 12555.43it/s]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 177987/460749 [00:32<00:23, 12250.50it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 180899/460749 [00:32<00:22, 12439.70it/s]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 183843/460749 [00:33<00:21, 12615.59it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 186691/460749 [00:33<00:21, 12609.36it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 189555/460749 [00:33<00:21, 12627.61it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 192307/460749 [00:33<00:21, 12492.99it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 195059/460749 [00:33<00:21, 12371.95it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 198035/460749 [00:34<00:20, 12608.83it/s]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 200915/460749 [00:34<00:20, 12646.89it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 203731/460749 [00:34<00:20, 12592.59it/s]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 206563/460749 [00:34<00:20, 12574.19it/s]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 209347/460749 [00:35<00:20, 12498.01it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 212227/460749 [00:35<00:19, 12569.54it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 215043/460749 [00:35<00:19, 12535.67it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 217843/460749 [00:35<00:19, 12486.99it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 220515/460749 [00:35<00:19, 12286.94it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 223347/460749 [00:36<00:19, 12357.83it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 226163/460749 [00:36<00:18, 12384.91it/s]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 228947/460749 [00:36<00:18, 12362.79it/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 231603/460749 [00:36<00:18, 12158.49it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 234451/460749 [00:37<00:18, 12311.26it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 237363/460749 [00:37<00:17, 12482.19it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 240195/460749 [00:37<00:17, 12499.59it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 242819/460749 [00:37<00:17, 12207.28it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 245747/460749 [00:37<00:17, 12436.86it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 248563/460749 [00:38<00:17, 12439.09it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 251363/460749 [00:38<00:16, 12427.99it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 254227/460749 [00:38<00:16, 12497.97it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 257107/460749 [00:38<00:16, 12571.30it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 259971/460749 [00:39<00:15, 12594.48it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 262771/460749 [00:39<00:15, 12537.06it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 265395/460749 [00:39<00:15, 12238.45it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 268451/460749 [00:39<00:15, 12594.92it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 271411/460749 [00:40<00:14, 12772.77it/s]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 274339/460749 [00:40<00:14, 12822.58it/s]\u001b[0m\n",
      "\u001b[34m60%|██████    | 277187/460749 [00:40<00:14, 12757.26it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 279795/460749 [00:40<00:14, 12393.45it/s]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 282691/460749 [00:40<00:14, 12515.32it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 285587/460749 [00:41<00:13, 12605.83it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 288419/460749 [00:41<00:13, 12579.33it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 291155/460749 [00:41<00:13, 12437.20it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 294051/460749 [00:41<00:13, 12554.93it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 295747/460749 [00:42<00:15, 10411.47it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 298691/460749 [00:42<00:14, 11129.14it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 300915/460749 [00:42<00:14, 10790.82it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 303251/460749 [00:42<00:14, 10658.67it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 305779/460749 [00:43<00:14, 10811.50it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 308339/460749 [00:43<00:13, 10962.42it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 310755/460749 [00:43<00:13, 10881.71it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 313267/460749 [00:43<00:13, 10949.95it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 315747/460749 [00:43<00:13, 10957.75it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 318195/460749 [00:44<00:13, 10914.08it/s]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 320723/460749 [00:44<00:12, 10994.90it/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 323235/460749 [00:44<00:12, 11025.84it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 325763/460749 [00:44<00:12, 11076.26it/s]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 328291/460749 [00:45<00:11, 11105.37it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 330787/460749 [00:45<00:11, 11089.77it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 333363/460749 [00:45<00:11, 11177.14it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 335843/460749 [00:45<00:11, 11114.13it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 338419/460749 [00:45<00:10, 11195.18it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 341027/460749 [00:46<00:10, 11301.42it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 343491/460749 [00:46<00:10, 11174.80it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 346067/460749 [00:46<00:10, 11245.31it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 348579/460749 [00:46<00:10, 11203.89it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 351139/460749 [00:47<00:09, 11238.79it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 353731/460749 [00:47<00:09, 11305.29it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 356259/460749 [00:47<00:09, 11268.40it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 358851/460749 [00:47<00:08, 11328.19it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 361123/460749 [00:47<00:09, 10941.42it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 363603/460749 [00:48<00:08, 10951.26it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 366275/460749 [00:48<00:08, 11213.05it/s]\u001b[0m\n",
      "\u001b[34m80%|████████  | 368739/460749 [00:48<00:08, 11116.13it/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 371171/460749 [00:48<00:08, 11014.70it/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 373667/460749 [00:49<00:07, 11019.22it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 376355/460749 [00:49<00:07, 11279.97it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 378899/460749 [00:49<00:07, 11274.83it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 381667/460749 [00:49<00:06, 11535.97it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 384515/460749 [00:50<00:06, 11857.65it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 387267/460749 [00:50<00:06, 11951.94it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 389891/460749 [00:50<00:05, 11850.21it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 392547/460749 [00:50<00:05, 11821.84it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 395475/460749 [00:50<00:05, 12159.34it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 398275/460749 [00:51<00:05, 12228.59it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 401011/460749 [00:51<00:04, 12191.45it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 403747/460749 [00:51<00:04, 12163.73it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 406499/460749 [00:51<00:04, 12166.27it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 409283/460749 [00:52<00:04, 12214.36it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 412131/460749 [00:52<00:03, 12314.58it/s]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 414915/460749 [00:52<00:03, 12314.63it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 417651/460749 [00:52<00:03, 12251.61it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 420323/460749 [00:52<00:03, 12118.65it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 423171/460749 [00:53<00:03, 12261.97it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 425827/460749 [00:53<00:02, 12110.65it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 428563/460749 [00:53<00:02, 12109.39it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 431283/460749 [00:53<00:02, 12082.72it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 434083/460749 [00:54<00:02, 12174.46it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 436963/460749 [00:54<00:01, 12345.60it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 439667/460749 [00:54<00:01, 12227.82it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 442467/460749 [00:54<00:01, 12276.91it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 445203/460749 [00:54<00:01, 12199.39it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 448067/460749 [00:55<00:01, 12341.87it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 449302/460749 [00:55<00:01, 9355.53it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 450290/460749 [00:55<00:01, 6275.41it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 453331/460749 [00:56<00:00, 7935.45it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 455507/460749 [00:56<00:00, 8341.44it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 457699/460749 [00:56<00:00, 8746.38it/s]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 459907/460749 [00:56<00:00, 8981.56it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 460749/460749 [00:56<00:00, 8098.03it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1/1 [00:00<00:00, 736.88it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/183 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 3/183 [00:00<00:14, 12.44it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 19/183 [00:00<00:05, 27.69it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 35/183 [00:00<00:03, 40.96it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 51/183 [00:01<00:02, 49.26it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 67/183 [00:01<00:02, 54.65it/s]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 83/183 [00:01<00:01, 58.22it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 99/183 [00:01<00:01, 60.63it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 115/183 [00:02<00:01, 47.66it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 131/183 [00:02<00:00, 52.28it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 163/183 [00:02<00:00, 73.34it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 179/183 [00:03<00:00, 71.34it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 183/183 [00:03<00:00, 58.07it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1/1 [00:00<00:00, 763.57it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1/1 [00:00<00:00, 747.51it/s]\u001b[0m\n",
      "\u001b[34m<<<< load model from uie-base-en!!!\u001b[0m\n",
      "\u001b[34mW1021 05:35:20.366938    82 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.6, Runtime API Version: 10.2\u001b[0m\n",
      "\u001b[34mW1021 05:35:21.071836    82 gpu_resources.cc:91] device: 0, cuDNN Version: 8.0.\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:26,135] [    INFO]#033[0m - We are using <class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'> to load '/opt/ml/checkpoints/'.#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:28,954] [    INFO]#033[0m - global step 1, epoch: 1, loss: 0.01822, speed: 0.36 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:29,414] [    INFO]#033[0m - global step 2, epoch: 1, loss: 0.01443, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:29,875] [    INFO]#033[0m - global step 3, epoch: 1, loss: 0.01225, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:30,327] [    INFO]#033[0m - global step 4, epoch: 1, loss: 0.01070, speed: 2.22 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:30,786] [    INFO]#033[0m - global step 5, epoch: 1, loss: 0.01029, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:31,241] [    INFO]#033[0m - global step 6, epoch: 1, loss: 0.00955, speed: 2.20 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:31,699] [    INFO]#033[0m - global step 7, epoch: 1, loss: 0.00896, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:32,160] [    INFO]#033[0m - global step 8, epoch: 1, loss: 0.00859, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:32,619] [    INFO]#033[0m - global step 9, epoch: 1, loss: 0.00821, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:33,081] [    INFO]#033[0m - global step 10, epoch: 1, loss: 0.00808, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:36,011] [    INFO]#033[0m - Evaluation precision: 0.39344, recall: 0.12245, F1: 0.18677#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:36,011] [    INFO]#033[0m - best F1 performence has been updated: 0.00000 --> 0.18677#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:36,996] [    INFO]#033[0m - tokenizer config file saved in /opt/ml/model/model_best/tokenizer_config.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:36,996] [    INFO]#033[0m - Special tokens file saved in /opt/ml/model/model_best/special_tokens_map.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:37,449] [    INFO]#033[0m - global step 11, epoch: 1, loss: 0.00795, speed: 2.21 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:37,910] [    INFO]#033[0m - global step 12, epoch: 1, loss: 0.00759, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:38,370] [    INFO]#033[0m - global step 13, epoch: 1, loss: 0.00741, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:38,823] [    INFO]#033[0m - global step 14, epoch: 1, loss: 0.00738, speed: 2.21 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:39,287] [    INFO]#033[0m - global step 15, epoch: 1, loss: 0.00718, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:39,747] [    INFO]#033[0m - global step 16, epoch: 1, loss: 0.00705, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:40,207] [    INFO]#033[0m - global step 17, epoch: 1, loss: 0.00686, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:40,670] [    INFO]#033[0m - global step 18, epoch: 1, loss: 0.00678, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:41,127] [    INFO]#033[0m - global step 19, epoch: 1, loss: 0.00667, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:41,587] [    INFO]#033[0m - global step 20, epoch: 1, loss: 0.00670, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:44,550] [    INFO]#033[0m - Evaluation precision: 0.52239, recall: 0.17857, F1: 0.26616#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:44,551] [    INFO]#033[0m - best F1 performence has been updated: 0.18677 --> 0.26616#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:47,513] [    INFO]#033[0m - tokenizer config file saved in /opt/ml/model/model_best/tokenizer_config.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:47,513] [    INFO]#033[0m - Special tokens file saved in /opt/ml/model/model_best/special_tokens_map.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:47,972] [    INFO]#033[0m - global step 21, epoch: 1, loss: 0.00670, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:48,427] [    INFO]#033[0m - global step 22, epoch: 1, loss: 0.00667, speed: 2.20 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:48,884] [    INFO]#033[0m - global step 23, epoch: 1, loss: 0.00662, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:49,342] [    INFO]#033[0m - global step 24, epoch: 1, loss: 0.00653, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:49,799] [    INFO]#033[0m - global step 25, epoch: 1, loss: 0.00643, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:50,256] [    INFO]#033[0m - global step 26, epoch: 1, loss: 0.00642, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:50,710] [    INFO]#033[0m - global step 27, epoch: 1, loss: 0.00641, speed: 2.20 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:51,174] [    INFO]#033[0m - global step 28, epoch: 1, loss: 0.00641, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:51,629] [    INFO]#033[0m - global step 29, epoch: 1, loss: 0.00641, speed: 2.20 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:52,088] [    INFO]#033[0m - global step 30, epoch: 1, loss: 0.00634, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:55,010] [    INFO]#033[0m - Evaluation precision: 0.56061, recall: 0.18878, F1: 0.28244#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:55,010] [    INFO]#033[0m - best F1 performence has been updated: 0.26616 --> 0.28244#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:57,969] [    INFO]#033[0m - tokenizer config file saved in /opt/ml/model/model_best/tokenizer_config.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:57,969] [    INFO]#033[0m - Special tokens file saved in /opt/ml/model/model_best/special_tokens_map.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:58,428] [    INFO]#033[0m - global step 31, epoch: 1, loss: 0.00630, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:58,892] [    INFO]#033[0m - global step 32, epoch: 1, loss: 0.00630, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:59,346] [    INFO]#033[0m - global step 33, epoch: 1, loss: 0.00632, speed: 2.21 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:35:59,802] [    INFO]#033[0m - global step 34, epoch: 1, loss: 0.00630, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:00,264] [    INFO]#033[0m - global step 35, epoch: 1, loss: 0.00621, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:00,721] [    INFO]#033[0m - global step 36, epoch: 1, loss: 0.00619, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:01,175] [    INFO]#033[0m - global step 37, epoch: 1, loss: 0.00614, speed: 2.20 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:01,642] [    INFO]#033[0m - global step 38, epoch: 1, loss: 0.00613, speed: 2.14 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:02,110] [    INFO]#033[0m - global step 39, epoch: 1, loss: 0.00608, speed: 2.14 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:02,569] [    INFO]#033[0m - global step 40, epoch: 1, loss: 0.00610, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:05,572] [    INFO]#033[0m - Evaluation precision: 0.51351, recall: 0.19388, F1: 0.28148#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:06,028] [    INFO]#033[0m - global step 41, epoch: 1, loss: 0.00608, speed: 2.20 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:06,483] [    INFO]#033[0m - global step 42, epoch: 1, loss: 0.00604, speed: 2.20 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:06,946] [    INFO]#033[0m - global step 43, epoch: 1, loss: 0.00604, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:07,407] [    INFO]#033[0m - global step 44, epoch: 1, loss: 0.00602, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:07,873] [    INFO]#033[0m - global step 45, epoch: 1, loss: 0.00602, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:08,315] [    INFO]#033[0m - global step 46, epoch: 1, loss: 0.00600, speed: 2.27 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:08,743] [    INFO]#033[0m - global step 47, epoch: 1, loss: 0.00597, speed: 2.34 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:09,172] [    INFO]#033[0m - global step 48, epoch: 1, loss: 0.00597, speed: 2.33 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:09,602] [    INFO]#033[0m - global step 49, epoch: 1, loss: 0.00593, speed: 2.33 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:10,033] [    INFO]#033[0m - global step 50, epoch: 1, loss: 0.00594, speed: 2.32 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:12,957] [    INFO]#033[0m - Evaluation precision: 0.52500, recall: 0.21429, F1: 0.30435#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:12,957] [    INFO]#033[0m - best F1 performence has been updated: 0.28244 --> 0.30435#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:15,905] [    INFO]#033[0m - tokenizer config file saved in /opt/ml/model/model_best/tokenizer_config.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:15,906] [    INFO]#033[0m - Special tokens file saved in /opt/ml/model/model_best/special_tokens_map.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:16,105] [    INFO]#033[0m - global step 51, epoch: 1, loss: 0.00590, speed: 5.04 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:16,706] [    INFO]#033[0m - global step 52, epoch: 2, loss: 0.00588, speed: 1.67 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:17,166] [    INFO]#033[0m - global step 53, epoch: 2, loss: 0.00586, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:17,622] [    INFO]#033[0m - global step 54, epoch: 2, loss: 0.00582, speed: 2.20 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:18,081] [    INFO]#033[0m - global step 55, epoch: 2, loss: 0.00577, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:18,538] [    INFO]#033[0m - global step 56, epoch: 2, loss: 0.00573, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:19,000] [    INFO]#033[0m - global step 57, epoch: 2, loss: 0.00569, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:19,460] [    INFO]#033[0m - global step 58, epoch: 2, loss: 0.00570, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:19,918] [    INFO]#033[0m - global step 59, epoch: 2, loss: 0.00571, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:20,374] [    INFO]#033[0m - global step 60, epoch: 2, loss: 0.00572, speed: 2.20 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:23,290] [    INFO]#033[0m - Evaluation precision: 0.51282, recall: 0.20408, F1: 0.29197#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:23,746] [    INFO]#033[0m - global step 61, epoch: 2, loss: 0.00569, speed: 2.20 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:24,202] [    INFO]#033[0m - global step 62, epoch: 2, loss: 0.00566, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:24,665] [    INFO]#033[0m - global step 63, epoch: 2, loss: 0.00566, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:25,120] [    INFO]#033[0m - global step 64, epoch: 2, loss: 0.00562, speed: 2.20 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:25,577] [    INFO]#033[0m - global step 65, epoch: 2, loss: 0.00559, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:26,032] [    INFO]#033[0m - global step 66, epoch: 2, loss: 0.00555, speed: 2.20 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:26,488] [    INFO]#033[0m - global step 67, epoch: 2, loss: 0.00555, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:26,942] [    INFO]#033[0m - global step 68, epoch: 2, loss: 0.00554, speed: 2.21 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:27,400] [    INFO]#033[0m - global step 69, epoch: 2, loss: 0.00550, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:27,856] [    INFO]#033[0m - global step 70, epoch: 2, loss: 0.00548, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:30,772] [    INFO]#033[0m - Evaluation precision: 0.51724, recall: 0.22959, F1: 0.31802#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:30,773] [    INFO]#033[0m - best F1 performence has been updated: 0.30435 --> 0.31802#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:33,718] [    INFO]#033[0m - tokenizer config file saved in /opt/ml/model/model_best/tokenizer_config.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:33,719] [    INFO]#033[0m - Special tokens file saved in /opt/ml/model/model_best/special_tokens_map.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:34,178] [    INFO]#033[0m - global step 71, epoch: 2, loss: 0.00545, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:34,635] [    INFO]#033[0m - global step 72, epoch: 2, loss: 0.00542, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:35,092] [    INFO]#033[0m - global step 73, epoch: 2, loss: 0.00540, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:35,547] [    INFO]#033[0m - global step 74, epoch: 2, loss: 0.00539, speed: 2.20 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:36,005] [    INFO]#033[0m - global step 75, epoch: 2, loss: 0.00538, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:36,463] [    INFO]#033[0m - global step 76, epoch: 2, loss: 0.00536, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:36,925] [    INFO]#033[0m - global step 77, epoch: 2, loss: 0.00538, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:37,384] [    INFO]#033[0m - global step 78, epoch: 2, loss: 0.00536, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:37,840] [    INFO]#033[0m - global step 79, epoch: 2, loss: 0.00536, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:38,300] [    INFO]#033[0m - global step 80, epoch: 2, loss: 0.00534, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:41,215] [    INFO]#033[0m - Evaluation precision: 0.53488, recall: 0.23469, F1: 0.32624#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:41,216] [    INFO]#033[0m - best F1 performence has been updated: 0.31802 --> 0.32624#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:44,162] [    INFO]#033[0m - tokenizer config file saved in /opt/ml/model/model_best/tokenizer_config.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:44,163] [    INFO]#033[0m - Special tokens file saved in /opt/ml/model/model_best/special_tokens_map.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:44,622] [    INFO]#033[0m - global step 81, epoch: 2, loss: 0.00536, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:45,083] [    INFO]#033[0m - global step 82, epoch: 2, loss: 0.00537, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:45,543] [    INFO]#033[0m - global step 83, epoch: 2, loss: 0.00534, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:46,001] [    INFO]#033[0m - global step 84, epoch: 2, loss: 0.00533, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:46,460] [    INFO]#033[0m - global step 85, epoch: 2, loss: 0.00535, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:46,920] [    INFO]#033[0m - global step 86, epoch: 2, loss: 0.00534, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:47,374] [    INFO]#033[0m - global step 87, epoch: 2, loss: 0.00532, speed: 2.20 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:47,833] [    INFO]#033[0m - global step 88, epoch: 2, loss: 0.00530, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:48,291] [    INFO]#033[0m - global step 89, epoch: 2, loss: 0.00529, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:48,749] [    INFO]#033[0m - global step 90, epoch: 2, loss: 0.00528, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:51,648] [    INFO]#033[0m - Evaluation precision: 0.55405, recall: 0.20918, F1: 0.30370#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:52,103] [    INFO]#033[0m - global step 91, epoch: 2, loss: 0.00527, speed: 2.20 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:52,566] [    INFO]#033[0m - global step 92, epoch: 2, loss: 0.00525, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:53,028] [    INFO]#033[0m - global step 93, epoch: 2, loss: 0.00523, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:53,492] [    INFO]#033[0m - global step 94, epoch: 2, loss: 0.00521, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:53,954] [    INFO]#033[0m - global step 95, epoch: 2, loss: 0.00521, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:54,412] [    INFO]#033[0m - global step 96, epoch: 2, loss: 0.00520, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:54,854] [    INFO]#033[0m - global step 97, epoch: 2, loss: 0.00517, speed: 2.26 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:55,285] [    INFO]#033[0m - global step 98, epoch: 2, loss: 0.00517, speed: 2.33 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:55,716] [    INFO]#033[0m - global step 99, epoch: 2, loss: 0.00518, speed: 2.32 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:56,146] [    INFO]#033[0m - global step 100, epoch: 2, loss: 0.00516, speed: 2.33 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:59,087] [    INFO]#033[0m - Evaluation precision: 0.57282, recall: 0.30102, F1: 0.39465#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:36:59,088] [    INFO]#033[0m - best F1 performence has been updated: 0.32624 --> 0.39465#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:02,076] [    INFO]#033[0m - tokenizer config file saved in /opt/ml/model/model_best/tokenizer_config.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:02,076] [    INFO]#033[0m - Special tokens file saved in /opt/ml/model/model_best/special_tokens_map.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:02,508] [    INFO]#033[0m - global step 101, epoch: 2, loss: 0.00517, speed: 2.32 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:02,710] [    INFO]#033[0m - global step 102, epoch: 2, loss: 0.00514, speed: 4.97 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:03,319] [    INFO]#033[0m - global step 103, epoch: 3, loss: 0.00512, speed: 1.64 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:03,785] [    INFO]#033[0m - global step 104, epoch: 3, loss: 0.00514, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:04,251] [    INFO]#033[0m - global step 105, epoch: 3, loss: 0.00511, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:04,714] [    INFO]#033[0m - global step 106, epoch: 3, loss: 0.00510, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:05,179] [    INFO]#033[0m - global step 107, epoch: 3, loss: 0.00507, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:05,642] [    INFO]#033[0m - global step 108, epoch: 3, loss: 0.00508, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:06,100] [    INFO]#033[0m - global step 109, epoch: 3, loss: 0.00507, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:06,558] [    INFO]#033[0m - global step 110, epoch: 3, loss: 0.00507, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:09,512] [    INFO]#033[0m - Evaluation precision: 0.53608, recall: 0.26531, F1: 0.35495#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:09,968] [    INFO]#033[0m - global step 111, epoch: 3, loss: 0.00506, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:10,425] [    INFO]#033[0m - global step 112, epoch: 3, loss: 0.00505, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:10,890] [    INFO]#033[0m - global step 113, epoch: 3, loss: 0.00505, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:11,347] [    INFO]#033[0m - global step 114, epoch: 3, loss: 0.00504, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:11,815] [    INFO]#033[0m - global step 115, epoch: 3, loss: 0.00503, speed: 2.14 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:12,279] [    INFO]#033[0m - global step 116, epoch: 3, loss: 0.00500, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:12,735] [    INFO]#033[0m - global step 117, epoch: 3, loss: 0.00500, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:13,201] [    INFO]#033[0m - global step 118, epoch: 3, loss: 0.00499, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:13,668] [    INFO]#033[0m - global step 119, epoch: 3, loss: 0.00498, speed: 2.14 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:14,128] [    INFO]#033[0m - global step 120, epoch: 3, loss: 0.00498, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:17,103] [    INFO]#033[0m - Evaluation precision: 0.53763, recall: 0.25510, F1: 0.34602#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:17,562] [    INFO]#033[0m - global step 121, epoch: 3, loss: 0.00497, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:18,024] [    INFO]#033[0m - global step 122, epoch: 3, loss: 0.00497, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:18,496] [    INFO]#033[0m - global step 123, epoch: 3, loss: 0.00495, speed: 2.12 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:18,957] [    INFO]#033[0m - global step 124, epoch: 3, loss: 0.00495, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:19,424] [    INFO]#033[0m - global step 125, epoch: 3, loss: 0.00494, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:19,879] [    INFO]#033[0m - global step 126, epoch: 3, loss: 0.00493, speed: 2.20 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:20,335] [    INFO]#033[0m - global step 127, epoch: 3, loss: 0.00492, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:20,794] [    INFO]#033[0m - global step 128, epoch: 3, loss: 0.00490, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:21,260] [    INFO]#033[0m - global step 129, epoch: 3, loss: 0.00490, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:21,722] [    INFO]#033[0m - global step 130, epoch: 3, loss: 0.00488, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:24,613] [    INFO]#033[0m - Evaluation precision: 0.53774, recall: 0.29082, F1: 0.37748#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:25,071] [    INFO]#033[0m - global step 131, epoch: 3, loss: 0.00486, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:25,528] [    INFO]#033[0m - global step 132, epoch: 3, loss: 0.00485, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:25,995] [    INFO]#033[0m - global step 133, epoch: 3, loss: 0.00484, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:26,455] [    INFO]#033[0m - global step 134, epoch: 3, loss: 0.00483, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:26,917] [    INFO]#033[0m - global step 135, epoch: 3, loss: 0.00482, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:27,379] [    INFO]#033[0m - global step 136, epoch: 3, loss: 0.00481, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:27,837] [    INFO]#033[0m - global step 137, epoch: 3, loss: 0.00480, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:28,308] [    INFO]#033[0m - global step 138, epoch: 3, loss: 0.00480, speed: 2.12 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:28,770] [    INFO]#033[0m - global step 139, epoch: 3, loss: 0.00479, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:29,239] [    INFO]#033[0m - global step 140, epoch: 3, loss: 0.00478, speed: 2.13 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:32,178] [    INFO]#033[0m - Evaluation precision: 0.56364, recall: 0.31633, F1: 0.40523#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:32,178] [    INFO]#033[0m - best F1 performence has been updated: 0.39465 --> 0.40523#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:35,291] [    INFO]#033[0m - tokenizer config file saved in /opt/ml/model/model_best/tokenizer_config.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:35,291] [    INFO]#033[0m - Special tokens file saved in /opt/ml/model/model_best/special_tokens_map.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:35,745] [    INFO]#033[0m - global step 141, epoch: 3, loss: 0.00478, speed: 2.21 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:36,205] [    INFO]#033[0m - global step 142, epoch: 3, loss: 0.00477, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:36,670] [    INFO]#033[0m - global step 143, epoch: 3, loss: 0.00476, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:37,127] [    INFO]#033[0m - global step 144, epoch: 3, loss: 0.00476, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:37,589] [    INFO]#033[0m - global step 145, epoch: 3, loss: 0.00477, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:38,050] [    INFO]#033[0m - global step 146, epoch: 3, loss: 0.00477, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:38,506] [    INFO]#033[0m - global step 147, epoch: 3, loss: 0.00475, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:38,952] [    INFO]#033[0m - global step 148, epoch: 3, loss: 0.00474, speed: 2.25 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:39,383] [    INFO]#033[0m - global step 149, epoch: 3, loss: 0.00473, speed: 2.32 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:39,815] [    INFO]#033[0m - global step 150, epoch: 3, loss: 0.00473, speed: 2.32 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:42,761] [    INFO]#033[0m - Evaluation precision: 0.57292, recall: 0.28061, F1: 0.37671#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:43,190] [    INFO]#033[0m - global step 151, epoch: 3, loss: 0.00472, speed: 2.33 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:43,620] [    INFO]#033[0m - global step 152, epoch: 3, loss: 0.00472, speed: 2.32 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:43,820] [    INFO]#033[0m - global step 153, epoch: 3, loss: 0.00472, speed: 5.02 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:44,404] [    INFO]#033[0m - global step 154, epoch: 4, loss: 0.00472, speed: 1.71 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:44,859] [    INFO]#033[0m - global step 155, epoch: 4, loss: 0.00471, speed: 2.20 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:45,316] [    INFO]#033[0m - global step 156, epoch: 4, loss: 0.00470, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:45,774] [    INFO]#033[0m - global step 157, epoch: 4, loss: 0.00468, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:46,234] [    INFO]#033[0m - global step 158, epoch: 4, loss: 0.00468, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:46,701] [    INFO]#033[0m - global step 159, epoch: 4, loss: 0.00468, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:47,161] [    INFO]#033[0m - global step 160, epoch: 4, loss: 0.00467, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:50,062] [    INFO]#033[0m - Evaluation precision: 0.48739, recall: 0.29592, F1: 0.36825#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:50,525] [    INFO]#033[0m - global step 161, epoch: 4, loss: 0.00466, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:50,986] [    INFO]#033[0m - global step 162, epoch: 4, loss: 0.00464, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:51,452] [    INFO]#033[0m - global step 163, epoch: 4, loss: 0.00463, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:51,910] [    INFO]#033[0m - global step 164, epoch: 4, loss: 0.00462, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:52,379] [    INFO]#033[0m - global step 165, epoch: 4, loss: 0.00462, speed: 2.14 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:52,844] [    INFO]#033[0m - global step 166, epoch: 4, loss: 0.00461, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:53,304] [    INFO]#033[0m - global step 167, epoch: 4, loss: 0.00461, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:53,765] [    INFO]#033[0m - global step 168, epoch: 4, loss: 0.00461, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:54,231] [    INFO]#033[0m - global step 169, epoch: 4, loss: 0.00460, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:54,693] [    INFO]#033[0m - global step 170, epoch: 4, loss: 0.00460, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:57,617] [    INFO]#033[0m - Evaluation precision: 0.52427, recall: 0.27551, F1: 0.36120#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:58,075] [    INFO]#033[0m - global step 171, epoch: 4, loss: 0.00459, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:58,539] [    INFO]#033[0m - global step 172, epoch: 4, loss: 0.00459, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:58,996] [    INFO]#033[0m - global step 173, epoch: 4, loss: 0.00459, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:59,456] [    INFO]#033[0m - global step 174, epoch: 4, loss: 0.00457, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:37:59,923] [    INFO]#033[0m - global step 175, epoch: 4, loss: 0.00456, speed: 2.14 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:00,381] [    INFO]#033[0m - global step 176, epoch: 4, loss: 0.00455, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:00,846] [    INFO]#033[0m - global step 177, epoch: 4, loss: 0.00455, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:01,310] [    INFO]#033[0m - global step 178, epoch: 4, loss: 0.00454, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:01,776] [    INFO]#033[0m - global step 179, epoch: 4, loss: 0.00453, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:02,245] [    INFO]#033[0m - global step 180, epoch: 4, loss: 0.00452, speed: 2.14 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:05,250] [    INFO]#033[0m - Evaluation precision: 0.49573, recall: 0.29592, F1: 0.37061#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:05,709] [    INFO]#033[0m - global step 181, epoch: 4, loss: 0.00451, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:06,172] [    INFO]#033[0m - global step 182, epoch: 4, loss: 0.00451, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:06,636] [    INFO]#033[0m - global step 183, epoch: 4, loss: 0.00450, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:07,096] [    INFO]#033[0m - global step 184, epoch: 4, loss: 0.00450, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:07,561] [    INFO]#033[0m - global step 185, epoch: 4, loss: 0.00449, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:08,023] [    INFO]#033[0m - global step 186, epoch: 4, loss: 0.00448, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:08,488] [    INFO]#033[0m - global step 187, epoch: 4, loss: 0.00447, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:08,950] [    INFO]#033[0m - global step 188, epoch: 4, loss: 0.00446, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:09,410] [    INFO]#033[0m - global step 189, epoch: 4, loss: 0.00445, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:09,875] [    INFO]#033[0m - global step 190, epoch: 4, loss: 0.00446, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:12,819] [    INFO]#033[0m - Evaluation precision: 0.50794, recall: 0.32653, F1: 0.39752#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:13,284] [    INFO]#033[0m - global step 191, epoch: 4, loss: 0.00447, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:13,751] [    INFO]#033[0m - global step 192, epoch: 4, loss: 0.00446, speed: 2.14 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:14,209] [    INFO]#033[0m - global step 193, epoch: 4, loss: 0.00445, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:14,669] [    INFO]#033[0m - global step 194, epoch: 4, loss: 0.00444, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:15,129] [    INFO]#033[0m - global step 195, epoch: 4, loss: 0.00443, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:15,590] [    INFO]#033[0m - global step 196, epoch: 4, loss: 0.00443, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:16,050] [    INFO]#033[0m - global step 197, epoch: 4, loss: 0.00443, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:16,509] [    INFO]#033[0m - global step 198, epoch: 4, loss: 0.00443, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:16,953] [    INFO]#033[0m - global step 199, epoch: 4, loss: 0.00443, speed: 2.25 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:17,385] [    INFO]#033[0m - global step 200, epoch: 4, loss: 0.00443, speed: 2.32 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:20,332] [    INFO]#033[0m - Evaluation precision: 0.55652, recall: 0.32653, F1: 0.41158#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:20,332] [    INFO]#033[0m - best F1 performence has been updated: 0.40523 --> 0.41158#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:23,198] [    INFO]#033[0m - tokenizer config file saved in /opt/ml/model/model_best/tokenizer_config.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:23,198] [    INFO]#033[0m - Special tokens file saved in /opt/ml/model/model_best/special_tokens_map.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:23,629] [    INFO]#033[0m - global step 201, epoch: 4, loss: 0.00442, speed: 2.33 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:24,059] [    INFO]#033[0m - global step 202, epoch: 4, loss: 0.00442, speed: 2.32 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:24,490] [    INFO]#033[0m - global step 203, epoch: 4, loss: 0.00441, speed: 2.32 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:24,690] [    INFO]#033[0m - global step 204, epoch: 4, loss: 0.00441, speed: 5.01 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:25,281] [    INFO]#033[0m - global step 205, epoch: 5, loss: 0.00441, speed: 1.69 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:25,742] [    INFO]#033[0m - global step 206, epoch: 5, loss: 0.00440, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:26,203] [    INFO]#033[0m - global step 207, epoch: 5, loss: 0.00440, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:26,669] [    INFO]#033[0m - global step 208, epoch: 5, loss: 0.00440, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:27,128] [    INFO]#033[0m - global step 209, epoch: 5, loss: 0.00439, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:27,588] [    INFO]#033[0m - global step 210, epoch: 5, loss: 0.00438, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:30,532] [    INFO]#033[0m - Evaluation precision: 0.53659, recall: 0.33673, F1: 0.41379#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:30,533] [    INFO]#033[0m - best F1 performence has been updated: 0.41158 --> 0.41379#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:33,624] [    INFO]#033[0m - tokenizer config file saved in /opt/ml/model/model_best/tokenizer_config.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:33,624] [    INFO]#033[0m - Special tokens file saved in /opt/ml/model/model_best/special_tokens_map.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:34,080] [    INFO]#033[0m - global step 211, epoch: 5, loss: 0.00438, speed: 2.20 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:34,540] [    INFO]#033[0m - global step 212, epoch: 5, loss: 0.00438, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:35,000] [    INFO]#033[0m - global step 213, epoch: 5, loss: 0.00437, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:35,455] [    INFO]#033[0m - global step 214, epoch: 5, loss: 0.00436, speed: 2.20 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:35,917] [    INFO]#033[0m - global step 215, epoch: 5, loss: 0.00435, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:36,378] [    INFO]#033[0m - global step 216, epoch: 5, loss: 0.00435, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:36,843] [    INFO]#033[0m - global step 217, epoch: 5, loss: 0.00434, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:37,303] [    INFO]#033[0m - global step 218, epoch: 5, loss: 0.00433, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:37,760] [    INFO]#033[0m - global step 219, epoch: 5, loss: 0.00432, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:38,224] [    INFO]#033[0m - global step 220, epoch: 5, loss: 0.00432, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:41,196] [    INFO]#033[0m - Evaluation precision: 0.50725, recall: 0.35714, F1: 0.41916#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:41,197] [    INFO]#033[0m - best F1 performence has been updated: 0.41379 --> 0.41916#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:44,116] [    INFO]#033[0m - tokenizer config file saved in /opt/ml/model/model_best/tokenizer_config.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:44,116] [    INFO]#033[0m - Special tokens file saved in /opt/ml/model/model_best/special_tokens_map.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:44,575] [    INFO]#033[0m - global step 221, epoch: 5, loss: 0.00432, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:45,035] [    INFO]#033[0m - global step 222, epoch: 5, loss: 0.00431, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:45,496] [    INFO]#033[0m - global step 223, epoch: 5, loss: 0.00431, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:45,960] [    INFO]#033[0m - global step 224, epoch: 5, loss: 0.00430, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:46,424] [    INFO]#033[0m - global step 225, epoch: 5, loss: 0.00430, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:46,887] [    INFO]#033[0m - global step 226, epoch: 5, loss: 0.00428, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:47,348] [    INFO]#033[0m - global step 227, epoch: 5, loss: 0.00427, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:47,813] [    INFO]#033[0m - global step 228, epoch: 5, loss: 0.00427, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:48,273] [    INFO]#033[0m - global step 229, epoch: 5, loss: 0.00427, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:48,729] [    INFO]#033[0m - global step 230, epoch: 5, loss: 0.00426, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:51,666] [    INFO]#033[0m - Evaluation precision: 0.52941, recall: 0.32143, F1: 0.40000#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:52,124] [    INFO]#033[0m - global step 231, epoch: 5, loss: 0.00425, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:52,588] [    INFO]#033[0m - global step 232, epoch: 5, loss: 0.00424, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:53,045] [    INFO]#033[0m - global step 233, epoch: 5, loss: 0.00423, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:53,505] [    INFO]#033[0m - global step 234, epoch: 5, loss: 0.00423, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:53,964] [    INFO]#033[0m - global step 235, epoch: 5, loss: 0.00422, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:54,426] [    INFO]#033[0m - global step 236, epoch: 5, loss: 0.00422, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:54,885] [    INFO]#033[0m - global step 237, epoch: 5, loss: 0.00423, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:55,348] [    INFO]#033[0m - global step 238, epoch: 5, loss: 0.00421, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:55,810] [    INFO]#033[0m - global step 239, epoch: 5, loss: 0.00421, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:56,272] [    INFO]#033[0m - global step 240, epoch: 5, loss: 0.00421, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:59,215] [    INFO]#033[0m - Evaluation precision: 0.49612, recall: 0.32653, F1: 0.39385#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:38:59,675] [    INFO]#033[0m - global step 241, epoch: 5, loss: 0.00420, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:00,134] [    INFO]#033[0m - global step 242, epoch: 5, loss: 0.00419, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:00,590] [    INFO]#033[0m - global step 243, epoch: 5, loss: 0.00419, speed: 2.20 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:01,052] [    INFO]#033[0m - global step 244, epoch: 5, loss: 0.00418, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:01,509] [    INFO]#033[0m - global step 245, epoch: 5, loss: 0.00418, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:01,971] [    INFO]#033[0m - global step 246, epoch: 5, loss: 0.00419, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:02,438] [    INFO]#033[0m - global step 247, epoch: 5, loss: 0.00417, speed: 2.14 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:02,896] [    INFO]#033[0m - global step 248, epoch: 5, loss: 0.00417, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:03,364] [    INFO]#033[0m - global step 249, epoch: 5, loss: 0.00417, speed: 2.14 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:03,811] [    INFO]#033[0m - global step 250, epoch: 5, loss: 0.00417, speed: 2.24 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:06,849] [    INFO]#033[0m - Evaluation precision: 0.51220, recall: 0.32143, F1: 0.39498#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:07,280] [    INFO]#033[0m - global step 251, epoch: 5, loss: 0.00416, speed: 2.32 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:07,713] [    INFO]#033[0m - global step 252, epoch: 5, loss: 0.00415, speed: 2.32 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:08,145] [    INFO]#033[0m - global step 253, epoch: 5, loss: 0.00414, speed: 2.31 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:08,577] [    INFO]#033[0m - global step 254, epoch: 5, loss: 0.00415, speed: 2.32 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:08,779] [    INFO]#033[0m - global step 255, epoch: 5, loss: 0.00415, speed: 4.97 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:09,398] [    INFO]#033[0m - global step 256, epoch: 6, loss: 0.00415, speed: 1.62 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:09,855] [    INFO]#033[0m - global step 257, epoch: 6, loss: 0.00415, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:10,319] [    INFO]#033[0m - global step 258, epoch: 6, loss: 0.00414, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:10,781] [    INFO]#033[0m - global step 259, epoch: 6, loss: 0.00413, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:11,245] [    INFO]#033[0m - global step 260, epoch: 6, loss: 0.00413, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:14,173] [    INFO]#033[0m - Evaluation precision: 0.55172, recall: 0.32653, F1: 0.41026#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:14,639] [    INFO]#033[0m - global step 261, epoch: 6, loss: 0.00413, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:15,105] [    INFO]#033[0m - global step 262, epoch: 6, loss: 0.00412, speed: 2.14 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:15,567] [    INFO]#033[0m - global step 263, epoch: 6, loss: 0.00412, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:16,027] [    INFO]#033[0m - global step 264, epoch: 6, loss: 0.00412, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:16,485] [    INFO]#033[0m - global step 265, epoch: 6, loss: 0.00411, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:16,958] [    INFO]#033[0m - global step 266, epoch: 6, loss: 0.00410, speed: 2.11 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:17,417] [    INFO]#033[0m - global step 267, epoch: 6, loss: 0.00410, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:17,878] [    INFO]#033[0m - global step 268, epoch: 6, loss: 0.00409, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:18,338] [    INFO]#033[0m - global step 269, epoch: 6, loss: 0.00409, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:18,800] [    INFO]#033[0m - global step 270, epoch: 6, loss: 0.00408, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:21,725] [    INFO]#033[0m - Evaluation precision: 0.50000, recall: 0.35204, F1: 0.41317#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:22,186] [    INFO]#033[0m - global step 271, epoch: 6, loss: 0.00409, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:22,646] [    INFO]#033[0m - global step 272, epoch: 6, loss: 0.00408, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:23,109] [    INFO]#033[0m - global step 273, epoch: 6, loss: 0.00408, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:23,568] [    INFO]#033[0m - global step 274, epoch: 6, loss: 0.00407, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:24,027] [    INFO]#033[0m - global step 275, epoch: 6, loss: 0.00406, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:24,491] [    INFO]#033[0m - global step 276, epoch: 6, loss: 0.00406, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:24,947] [    INFO]#033[0m - global step 277, epoch: 6, loss: 0.00406, speed: 2.20 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:25,406] [    INFO]#033[0m - global step 278, epoch: 6, loss: 0.00405, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:25,873] [    INFO]#033[0m - global step 279, epoch: 6, loss: 0.00405, speed: 2.14 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:26,335] [    INFO]#033[0m - global step 280, epoch: 6, loss: 0.00404, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:29,266] [    INFO]#033[0m - Evaluation precision: 0.50376, recall: 0.34184, F1: 0.40729#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:29,732] [    INFO]#033[0m - global step 281, epoch: 6, loss: 0.00404, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:30,195] [    INFO]#033[0m - global step 282, epoch: 6, loss: 0.00403, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:30,657] [    INFO]#033[0m - global step 283, epoch: 6, loss: 0.00403, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:31,120] [    INFO]#033[0m - global step 284, epoch: 6, loss: 0.00403, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:31,581] [    INFO]#033[0m - global step 285, epoch: 6, loss: 0.00403, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:32,048] [    INFO]#033[0m - global step 286, epoch: 6, loss: 0.00403, speed: 2.14 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:32,512] [    INFO]#033[0m - global step 287, epoch: 6, loss: 0.00402, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:32,973] [    INFO]#033[0m - global step 288, epoch: 6, loss: 0.00401, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:33,436] [    INFO]#033[0m - global step 289, epoch: 6, loss: 0.00402, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:33,893] [    INFO]#033[0m - global step 290, epoch: 6, loss: 0.00401, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:36,813] [    INFO]#033[0m - Evaluation precision: 0.54472, recall: 0.34184, F1: 0.42006#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:36,813] [    INFO]#033[0m - best F1 performence has been updated: 0.41916 --> 0.42006#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:39,737] [    INFO]#033[0m - tokenizer config file saved in /opt/ml/model/model_best/tokenizer_config.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:39,737] [    INFO]#033[0m - Special tokens file saved in /opt/ml/model/model_best/special_tokens_map.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:40,201] [    INFO]#033[0m - global step 291, epoch: 6, loss: 0.00400, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:40,668] [    INFO]#033[0m - global step 292, epoch: 6, loss: 0.00399, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:41,132] [    INFO]#033[0m - global step 293, epoch: 6, loss: 0.00399, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:41,592] [    INFO]#033[0m - global step 294, epoch: 6, loss: 0.00398, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:42,051] [    INFO]#033[0m - global step 295, epoch: 6, loss: 0.00398, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:42,511] [    INFO]#033[0m - global step 296, epoch: 6, loss: 0.00398, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:42,967] [    INFO]#033[0m - global step 297, epoch: 6, loss: 0.00397, speed: 2.20 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:43,428] [    INFO]#033[0m - global step 298, epoch: 6, loss: 0.00397, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:43,891] [    INFO]#033[0m - global step 299, epoch: 6, loss: 0.00396, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:44,349] [    INFO]#033[0m - global step 300, epoch: 6, loss: 0.00396, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:47,310] [    INFO]#033[0m - Evaluation precision: 0.49333, recall: 0.37755, F1: 0.42775#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:47,310] [    INFO]#033[0m - best F1 performence has been updated: 0.42006 --> 0.42775#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:50,228] [    INFO]#033[0m - tokenizer config file saved in /opt/ml/model/model_best/tokenizer_config.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:50,228] [    INFO]#033[0m - Special tokens file saved in /opt/ml/model/model_best/special_tokens_map.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:50,674] [    INFO]#033[0m - global step 301, epoch: 6, loss: 0.00395, speed: 2.25 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:51,105] [    INFO]#033[0m - global step 302, epoch: 6, loss: 0.00395, speed: 2.32 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:51,536] [    INFO]#033[0m - global step 303, epoch: 6, loss: 0.00394, speed: 2.32 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:51,968] [    INFO]#033[0m - global step 304, epoch: 6, loss: 0.00394, speed: 2.32 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:52,400] [    INFO]#033[0m - global step 305, epoch: 6, loss: 0.00394, speed: 2.32 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:52,601] [    INFO]#033[0m - global step 306, epoch: 6, loss: 0.00393, speed: 4.98 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:53,197] [    INFO]#033[0m - global step 307, epoch: 7, loss: 0.00392, speed: 1.68 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:53,656] [    INFO]#033[0m - global step 308, epoch: 7, loss: 0.00392, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:54,113] [    INFO]#033[0m - global step 309, epoch: 7, loss: 0.00392, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:54,573] [    INFO]#033[0m - global step 310, epoch: 7, loss: 0.00391, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:57,477] [    INFO]#033[0m - Evaluation precision: 0.51128, recall: 0.34694, F1: 0.41337#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:57,939] [    INFO]#033[0m - global step 311, epoch: 7, loss: 0.00390, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:58,409] [    INFO]#033[0m - global step 312, epoch: 7, loss: 0.00389, speed: 2.13 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:58,868] [    INFO]#033[0m - global step 313, epoch: 7, loss: 0.00389, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:59,336] [    INFO]#033[0m - global step 314, epoch: 7, loss: 0.00389, speed: 2.14 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:39:59,798] [    INFO]#033[0m - global step 315, epoch: 7, loss: 0.00388, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:00,260] [    INFO]#033[0m - global step 316, epoch: 7, loss: 0.00388, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:00,721] [    INFO]#033[0m - global step 317, epoch: 7, loss: 0.00388, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:01,183] [    INFO]#033[0m - global step 318, epoch: 7, loss: 0.00388, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:01,647] [    INFO]#033[0m - global step 319, epoch: 7, loss: 0.00387, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:02,113] [    INFO]#033[0m - global step 320, epoch: 7, loss: 0.00386, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:05,128] [    INFO]#033[0m - Evaluation precision: 0.51799, recall: 0.36735, F1: 0.42985#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:05,129] [    INFO]#033[0m - best F1 performence has been updated: 0.42775 --> 0.42985#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:08,271] [    INFO]#033[0m - tokenizer config file saved in /opt/ml/model/model_best/tokenizer_config.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:08,271] [    INFO]#033[0m - Special tokens file saved in /opt/ml/model/model_best/special_tokens_map.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:08,729] [    INFO]#033[0m - global step 321, epoch: 7, loss: 0.00386, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:09,192] [    INFO]#033[0m - global step 322, epoch: 7, loss: 0.00386, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:09,660] [    INFO]#033[0m - global step 323, epoch: 7, loss: 0.00385, speed: 2.14 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:10,121] [    INFO]#033[0m - global step 324, epoch: 7, loss: 0.00385, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:10,580] [    INFO]#033[0m - global step 325, epoch: 7, loss: 0.00384, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:11,038] [    INFO]#033[0m - global step 326, epoch: 7, loss: 0.00383, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:11,502] [    INFO]#033[0m - global step 327, epoch: 7, loss: 0.00384, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:11,968] [    INFO]#033[0m - global step 328, epoch: 7, loss: 0.00383, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:12,426] [    INFO]#033[0m - global step 329, epoch: 7, loss: 0.00383, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:12,884] [    INFO]#033[0m - global step 330, epoch: 7, loss: 0.00383, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:15,824] [    INFO]#033[0m - Evaluation precision: 0.51880, recall: 0.35204, F1: 0.41945#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:16,284] [    INFO]#033[0m - global step 331, epoch: 7, loss: 0.00382, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:16,748] [    INFO]#033[0m - global step 332, epoch: 7, loss: 0.00382, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:17,216] [    INFO]#033[0m - global step 333, epoch: 7, loss: 0.00382, speed: 2.14 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:17,679] [    INFO]#033[0m - global step 334, epoch: 7, loss: 0.00381, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:18,140] [    INFO]#033[0m - global step 335, epoch: 7, loss: 0.00381, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:18,613] [    INFO]#033[0m - global step 336, epoch: 7, loss: 0.00381, speed: 2.12 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:19,078] [    INFO]#033[0m - global step 337, epoch: 7, loss: 0.00381, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:19,543] [    INFO]#033[0m - global step 338, epoch: 7, loss: 0.00381, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:20,003] [    INFO]#033[0m - global step 339, epoch: 7, loss: 0.00381, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:20,473] [    INFO]#033[0m - global step 340, epoch: 7, loss: 0.00380, speed: 2.13 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:23,388] [    INFO]#033[0m - Evaluation precision: 0.51408, recall: 0.37245, F1: 0.43195#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:23,389] [    INFO]#033[0m - best F1 performence has been updated: 0.42985 --> 0.43195#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:26,483] [    INFO]#033[0m - tokenizer config file saved in /opt/ml/model/model_best/tokenizer_config.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:26,484] [    INFO]#033[0m - Special tokens file saved in /opt/ml/model/model_best/special_tokens_map.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:26,945] [    INFO]#033[0m - global step 341, epoch: 7, loss: 0.00380, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:27,413] [    INFO]#033[0m - global step 342, epoch: 7, loss: 0.00379, speed: 2.14 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:27,875] [    INFO]#033[0m - global step 343, epoch: 7, loss: 0.00379, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:28,335] [    INFO]#033[0m - global step 344, epoch: 7, loss: 0.00378, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:28,792] [    INFO]#033[0m - global step 345, epoch: 7, loss: 0.00379, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:29,252] [    INFO]#033[0m - global step 346, epoch: 7, loss: 0.00378, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:29,711] [    INFO]#033[0m - global step 347, epoch: 7, loss: 0.00378, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:30,172] [    INFO]#033[0m - global step 348, epoch: 7, loss: 0.00377, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:30,639] [    INFO]#033[0m - global step 349, epoch: 7, loss: 0.00377, speed: 2.14 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:31,102] [    INFO]#033[0m - global step 350, epoch: 7, loss: 0.00376, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:34,050] [    INFO]#033[0m - Evaluation precision: 0.51020, recall: 0.38265, F1: 0.43732#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:34,051] [    INFO]#033[0m - best F1 performence has been updated: 0.43195 --> 0.43732#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:37,178] [    INFO]#033[0m - tokenizer config file saved in /opt/ml/model/model_best/tokenizer_config.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:37,178] [    INFO]#033[0m - Special tokens file saved in /opt/ml/model/model_best/special_tokens_map.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:37,640] [    INFO]#033[0m - global step 351, epoch: 7, loss: 0.00375, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:38,085] [    INFO]#033[0m - global step 352, epoch: 7, loss: 0.00375, speed: 2.25 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:38,516] [    INFO]#033[0m - global step 353, epoch: 7, loss: 0.00374, speed: 2.32 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:38,947] [    INFO]#033[0m - global step 354, epoch: 7, loss: 0.00374, speed: 2.32 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:39,380] [    INFO]#033[0m - global step 355, epoch: 7, loss: 0.00374, speed: 2.31 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:39,813] [    INFO]#033[0m - global step 356, epoch: 7, loss: 0.00373, speed: 2.31 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:40,014] [    INFO]#033[0m - global step 357, epoch: 7, loss: 0.00373, speed: 4.97 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:40,623] [    INFO]#033[0m - global step 358, epoch: 8, loss: 0.00373, speed: 1.64 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:41,090] [    INFO]#033[0m - global step 359, epoch: 8, loss: 0.00372, speed: 2.14 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:41,549] [    INFO]#033[0m - global step 360, epoch: 8, loss: 0.00372, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:44,478] [    INFO]#033[0m - Evaluation precision: 0.52857, recall: 0.37755, F1: 0.44048#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:44,478] [    INFO]#033[0m - best F1 performence has been updated: 0.43732 --> 0.44048#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:47,575] [    INFO]#033[0m - tokenizer config file saved in /opt/ml/model/model_best/tokenizer_config.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:47,575] [    INFO]#033[0m - Special tokens file saved in /opt/ml/model/model_best/special_tokens_map.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:48,031] [    INFO]#033[0m - global step 361, epoch: 8, loss: 0.00371, speed: 2.20 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:48,496] [    INFO]#033[0m - global step 362, epoch: 8, loss: 0.00371, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:48,963] [    INFO]#033[0m - global step 363, epoch: 8, loss: 0.00371, speed: 2.14 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:49,424] [    INFO]#033[0m - global step 364, epoch: 8, loss: 0.00371, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:49,882] [    INFO]#033[0m - global step 365, epoch: 8, loss: 0.00370, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:50,343] [    INFO]#033[0m - global step 366, epoch: 8, loss: 0.00370, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:50,803] [    INFO]#033[0m - global step 367, epoch: 8, loss: 0.00370, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:51,268] [    INFO]#033[0m - global step 368, epoch: 8, loss: 0.00369, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:51,730] [    INFO]#033[0m - global step 369, epoch: 8, loss: 0.00369, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:52,192] [    INFO]#033[0m - global step 370, epoch: 8, loss: 0.00368, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:55,154] [    INFO]#033[0m - Evaluation precision: 0.50000, recall: 0.35714, F1: 0.41667#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:55,612] [    INFO]#033[0m - global step 371, epoch: 8, loss: 0.00369, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:56,076] [    INFO]#033[0m - global step 372, epoch: 8, loss: 0.00369, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:56,536] [    INFO]#033[0m - global step 373, epoch: 8, loss: 0.00368, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:56,994] [    INFO]#033[0m - global step 374, epoch: 8, loss: 0.00368, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:57,458] [    INFO]#033[0m - global step 375, epoch: 8, loss: 0.00367, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:57,922] [    INFO]#033[0m - global step 376, epoch: 8, loss: 0.00367, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:58,388] [    INFO]#033[0m - global step 377, epoch: 8, loss: 0.00367, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:58,847] [    INFO]#033[0m - global step 378, epoch: 8, loss: 0.00366, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:59,307] [    INFO]#033[0m - global step 379, epoch: 8, loss: 0.00366, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:40:59,768] [    INFO]#033[0m - global step 380, epoch: 8, loss: 0.00365, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:02,739] [    INFO]#033[0m - Evaluation precision: 0.49606, recall: 0.32143, F1: 0.39009#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:03,198] [    INFO]#033[0m - global step 381, epoch: 8, loss: 0.00365, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:03,666] [    INFO]#033[0m - global step 382, epoch: 8, loss: 0.00364, speed: 2.14 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:04,127] [    INFO]#033[0m - global step 383, epoch: 8, loss: 0.00364, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:04,585] [    INFO]#033[0m - global step 384, epoch: 8, loss: 0.00364, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:05,048] [    INFO]#033[0m - global step 385, epoch: 8, loss: 0.00363, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:05,527] [    INFO]#033[0m - global step 386, epoch: 8, loss: 0.00363, speed: 2.09 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:05,999] [    INFO]#033[0m - global step 387, epoch: 8, loss: 0.00362, speed: 2.12 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:06,459] [    INFO]#033[0m - global step 388, epoch: 8, loss: 0.00362, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:06,916] [    INFO]#033[0m - global step 389, epoch: 8, loss: 0.00361, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:07,376] [    INFO]#033[0m - global step 390, epoch: 8, loss: 0.00361, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:10,298] [    INFO]#033[0m - Evaluation precision: 0.47712, recall: 0.37245, F1: 0.41834#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:10,757] [    INFO]#033[0m - global step 391, epoch: 8, loss: 0.00360, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:11,219] [    INFO]#033[0m - global step 392, epoch: 8, loss: 0.00361, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:11,680] [    INFO]#033[0m - global step 393, epoch: 8, loss: 0.00360, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:12,145] [    INFO]#033[0m - global step 394, epoch: 8, loss: 0.00360, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:12,605] [    INFO]#033[0m - global step 395, epoch: 8, loss: 0.00359, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:13,067] [    INFO]#033[0m - global step 396, epoch: 8, loss: 0.00359, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:13,536] [    INFO]#033[0m - global step 397, epoch: 8, loss: 0.00360, speed: 2.13 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:14,002] [    INFO]#033[0m - global step 398, epoch: 8, loss: 0.00359, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:14,460] [    INFO]#033[0m - global step 399, epoch: 8, loss: 0.00359, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:14,922] [    INFO]#033[0m - global step 400, epoch: 8, loss: 0.00358, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:17,877] [    INFO]#033[0m - Evaluation precision: 0.52344, recall: 0.34184, F1: 0.41358#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:18,340] [    INFO]#033[0m - global step 401, epoch: 8, loss: 0.00358, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:18,799] [    INFO]#033[0m - global step 402, epoch: 8, loss: 0.00358, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:19,247] [    INFO]#033[0m - global step 403, epoch: 8, loss: 0.00357, speed: 2.23 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:19,681] [    INFO]#033[0m - global step 404, epoch: 8, loss: 0.00357, speed: 2.31 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:20,115] [    INFO]#033[0m - global step 405, epoch: 8, loss: 0.00357, speed: 2.31 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:20,548] [    INFO]#033[0m - global step 406, epoch: 8, loss: 0.00357, speed: 2.31 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:20,981] [    INFO]#033[0m - global step 407, epoch: 8, loss: 0.00356, speed: 2.31 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:21,182] [    INFO]#033[0m - global step 408, epoch: 8, loss: 0.00356, speed: 4.99 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:21,778] [    INFO]#033[0m - global step 409, epoch: 9, loss: 0.00355, speed: 1.68 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:22,237] [    INFO]#033[0m - global step 410, epoch: 9, loss: 0.00355, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:25,174] [    INFO]#033[0m - Evaluation precision: 0.51034, recall: 0.37755, F1: 0.43402#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:25,643] [    INFO]#033[0m - global step 411, epoch: 9, loss: 0.00354, speed: 2.14 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:26,106] [    INFO]#033[0m - global step 412, epoch: 9, loss: 0.00353, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:26,570] [    INFO]#033[0m - global step 413, epoch: 9, loss: 0.00353, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:27,035] [    INFO]#033[0m - global step 414, epoch: 9, loss: 0.00353, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:27,497] [    INFO]#033[0m - global step 415, epoch: 9, loss: 0.00353, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:27,957] [    INFO]#033[0m - global step 416, epoch: 9, loss: 0.00353, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:28,419] [    INFO]#033[0m - global step 417, epoch: 9, loss: 0.00352, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:28,882] [    INFO]#033[0m - global step 418, epoch: 9, loss: 0.00353, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:29,346] [    INFO]#033[0m - global step 419, epoch: 9, loss: 0.00352, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:29,813] [    INFO]#033[0m - global step 420, epoch: 9, loss: 0.00352, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:32,754] [    INFO]#033[0m - Evaluation precision: 0.51351, recall: 0.38776, F1: 0.44186#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:32,755] [    INFO]#033[0m - best F1 performence has been updated: 0.44048 --> 0.44186#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:35,700] [    INFO]#033[0m - tokenizer config file saved in /opt/ml/model/model_best/tokenizer_config.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:35,700] [    INFO]#033[0m - Special tokens file saved in /opt/ml/model/model_best/special_tokens_map.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:36,155] [    INFO]#033[0m - global step 421, epoch: 9, loss: 0.00352, speed: 2.20 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:36,617] [    INFO]#033[0m - global step 422, epoch: 9, loss: 0.00351, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:37,078] [    INFO]#033[0m - global step 423, epoch: 9, loss: 0.00351, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:37,540] [    INFO]#033[0m - global step 424, epoch: 9, loss: 0.00350, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:38,000] [    INFO]#033[0m - global step 425, epoch: 9, loss: 0.00350, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:38,461] [    INFO]#033[0m - global step 426, epoch: 9, loss: 0.00349, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:38,922] [    INFO]#033[0m - global step 427, epoch: 9, loss: 0.00349, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:39,383] [    INFO]#033[0m - global step 428, epoch: 9, loss: 0.00348, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:39,849] [    INFO]#033[0m - global step 429, epoch: 9, loss: 0.00348, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:40,310] [    INFO]#033[0m - global step 430, epoch: 9, loss: 0.00348, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:43,265] [    INFO]#033[0m - Evaluation precision: 0.49640, recall: 0.35204, F1: 0.41194#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:43,721] [    INFO]#033[0m - global step 431, epoch: 9, loss: 0.00348, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:44,178] [    INFO]#033[0m - global step 432, epoch: 9, loss: 0.00348, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:44,640] [    INFO]#033[0m - global step 433, epoch: 9, loss: 0.00347, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:45,099] [    INFO]#033[0m - global step 434, epoch: 9, loss: 0.00347, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:45,564] [    INFO]#033[0m - global step 435, epoch: 9, loss: 0.00346, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:46,029] [    INFO]#033[0m - global step 436, epoch: 9, loss: 0.00346, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:46,491] [    INFO]#033[0m - global step 437, epoch: 9, loss: 0.00346, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:46,956] [    INFO]#033[0m - global step 438, epoch: 9, loss: 0.00345, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:47,416] [    INFO]#033[0m - global step 439, epoch: 9, loss: 0.00345, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:47,881] [    INFO]#033[0m - global step 440, epoch: 9, loss: 0.00344, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:50,822] [    INFO]#033[0m - Evaluation precision: 0.50658, recall: 0.39286, F1: 0.44253#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:50,822] [    INFO]#033[0m - best F1 performence has been updated: 0.44186 --> 0.44253#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:53,764] [    INFO]#033[0m - tokenizer config file saved in /opt/ml/model/model_best/tokenizer_config.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:53,764] [    INFO]#033[0m - Special tokens file saved in /opt/ml/model/model_best/special_tokens_map.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:54,226] [    INFO]#033[0m - global step 441, epoch: 9, loss: 0.00345, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:54,685] [    INFO]#033[0m - global step 442, epoch: 9, loss: 0.00344, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:55,145] [    INFO]#033[0m - global step 443, epoch: 9, loss: 0.00344, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:55,604] [    INFO]#033[0m - global step 444, epoch: 9, loss: 0.00344, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:56,061] [    INFO]#033[0m - global step 445, epoch: 9, loss: 0.00343, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:56,527] [    INFO]#033[0m - global step 446, epoch: 9, loss: 0.00343, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:56,987] [    INFO]#033[0m - global step 447, epoch: 9, loss: 0.00343, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:57,449] [    INFO]#033[0m - global step 448, epoch: 9, loss: 0.00343, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:57,916] [    INFO]#033[0m - global step 449, epoch: 9, loss: 0.00343, speed: 2.14 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:41:58,389] [    INFO]#033[0m - global step 450, epoch: 9, loss: 0.00342, speed: 2.12 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:01,311] [    INFO]#033[0m - Evaluation precision: 0.50694, recall: 0.37245, F1: 0.42941#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:01,774] [    INFO]#033[0m - global step 451, epoch: 9, loss: 0.00342, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:02,239] [    INFO]#033[0m - global step 452, epoch: 9, loss: 0.00342, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:02,699] [    INFO]#033[0m - global step 453, epoch: 9, loss: 0.00342, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:03,146] [    INFO]#033[0m - global step 454, epoch: 9, loss: 0.00341, speed: 2.24 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:03,580] [    INFO]#033[0m - global step 455, epoch: 9, loss: 0.00341, speed: 2.31 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:04,014] [    INFO]#033[0m - global step 456, epoch: 9, loss: 0.00341, speed: 2.31 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:04,447] [    INFO]#033[0m - global step 457, epoch: 9, loss: 0.00340, speed: 2.31 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:04,881] [    INFO]#033[0m - global step 458, epoch: 9, loss: 0.00340, speed: 2.31 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:05,084] [    INFO]#033[0m - global step 459, epoch: 9, loss: 0.00340, speed: 4.94 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:05,735] [    INFO]#033[0m - global step 460, epoch: 10, loss: 0.00339, speed: 1.54 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:08,691] [    INFO]#033[0m - Evaluation precision: 0.51678, recall: 0.39286, F1: 0.44638#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:08,692] [    INFO]#033[0m - best F1 performence has been updated: 0.44253 --> 0.44638#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:11,793] [    INFO]#033[0m - tokenizer config file saved in /opt/ml/model/model_best/tokenizer_config.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:11,793] [    INFO]#033[0m - Special tokens file saved in /opt/ml/model/model_best/special_tokens_map.json#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:12,249] [    INFO]#033[0m - global step 461, epoch: 10, loss: 0.00339, speed: 2.20 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:12,709] [    INFO]#033[0m - global step 462, epoch: 10, loss: 0.00339, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:13,166] [    INFO]#033[0m - global step 463, epoch: 10, loss: 0.00338, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:13,625] [    INFO]#033[0m - global step 464, epoch: 10, loss: 0.00338, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:14,085] [    INFO]#033[0m - global step 465, epoch: 10, loss: 0.00338, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:14,548] [    INFO]#033[0m - global step 466, epoch: 10, loss: 0.00338, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:15,010] [    INFO]#033[0m - global step 467, epoch: 10, loss: 0.00338, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:15,472] [    INFO]#033[0m - global step 468, epoch: 10, loss: 0.00337, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:15,930] [    INFO]#033[0m - global step 469, epoch: 10, loss: 0.00337, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:16,393] [    INFO]#033[0m - global step 470, epoch: 10, loss: 0.00336, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:19,348] [    INFO]#033[0m - Evaluation precision: 0.51049, recall: 0.37245, F1: 0.43068#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:19,816] [    INFO]#033[0m - global step 471, epoch: 10, loss: 0.00336, speed: 2.14 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:20,282] [    INFO]#033[0m - global step 472, epoch: 10, loss: 0.00336, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:20,745] [    INFO]#033[0m - global step 473, epoch: 10, loss: 0.00335, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:21,205] [    INFO]#033[0m - global step 474, epoch: 10, loss: 0.00335, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:21,671] [    INFO]#033[0m - global step 475, epoch: 10, loss: 0.00335, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:22,134] [    INFO]#033[0m - global step 476, epoch: 10, loss: 0.00335, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:22,597] [    INFO]#033[0m - global step 477, epoch: 10, loss: 0.00335, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:23,058] [    INFO]#033[0m - global step 478, epoch: 10, loss: 0.00334, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:23,518] [    INFO]#033[0m - global step 479, epoch: 10, loss: 0.00334, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:23,980] [    INFO]#033[0m - global step 480, epoch: 10, loss: 0.00334, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:26,906] [    INFO]#033[0m - Evaluation precision: 0.50323, recall: 0.39796, F1: 0.44444#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:27,369] [    INFO]#033[0m - global step 481, epoch: 10, loss: 0.00334, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:27,829] [    INFO]#033[0m - global step 482, epoch: 10, loss: 0.00333, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:28,291] [    INFO]#033[0m - global step 483, epoch: 10, loss: 0.00333, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:28,756] [    INFO]#033[0m - global step 484, epoch: 10, loss: 0.00332, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:29,219] [    INFO]#033[0m - global step 485, epoch: 10, loss: 0.00332, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:29,684] [    INFO]#033[0m - global step 486, epoch: 10, loss: 0.00332, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:30,145] [    INFO]#033[0m - global step 487, epoch: 10, loss: 0.00332, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:30,604] [    INFO]#033[0m - global step 488, epoch: 10, loss: 0.00333, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:31,064] [    INFO]#033[0m - global step 489, epoch: 10, loss: 0.00332, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:31,531] [    INFO]#033[0m - global step 490, epoch: 10, loss: 0.00332, speed: 2.14 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:34,484] [    INFO]#033[0m - Evaluation precision: 0.51799, recall: 0.36735, F1: 0.42985#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:34,955] [    INFO]#033[0m - global step 491, epoch: 10, loss: 0.00332, speed: 2.13 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:35,417] [    INFO]#033[0m - global step 492, epoch: 10, loss: 0.00331, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:35,875] [    INFO]#033[0m - global step 493, epoch: 10, loss: 0.00331, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:36,339] [    INFO]#033[0m - global step 494, epoch: 10, loss: 0.00331, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:36,804] [    INFO]#033[0m - global step 495, epoch: 10, loss: 0.00330, speed: 2.15 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:37,265] [    INFO]#033[0m - global step 496, epoch: 10, loss: 0.00330, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:37,723] [    INFO]#033[0m - global step 497, epoch: 10, loss: 0.00330, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:38,180] [    INFO]#033[0m - global step 498, epoch: 10, loss: 0.00330, speed: 2.19 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:38,643] [    INFO]#033[0m - global step 499, epoch: 10, loss: 0.00329, speed: 2.16 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:39,102] [    INFO]#033[0m - global step 500, epoch: 10, loss: 0.00329, speed: 2.18 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:42,075] [    INFO]#033[0m - Evaluation precision: 0.49296, recall: 0.35714, F1: 0.41420#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:42,544] [    INFO]#033[0m - global step 501, epoch: 10, loss: 0.00329, speed: 2.13 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:43,006] [    INFO]#033[0m - global step 502, epoch: 10, loss: 0.00328, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:43,468] [    INFO]#033[0m - global step 503, epoch: 10, loss: 0.00328, speed: 2.17 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:43,943] [    INFO]#033[0m - global step 504, epoch: 10, loss: 0.00328, speed: 2.11 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:44,394] [    INFO]#033[0m - global step 505, epoch: 10, loss: 0.00327, speed: 2.22 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:44,828] [    INFO]#033[0m - global step 506, epoch: 10, loss: 0.00327, speed: 2.30 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:45,261] [    INFO]#033[0m - global step 507, epoch: 10, loss: 0.00327, speed: 2.31 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:45,696] [    INFO]#033[0m - global step 508, epoch: 10, loss: 0.00326, speed: 2.30 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:46,130] [    INFO]#033[0m - global step 509, epoch: 10, loss: 0.00326, speed: 2.31 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:46,333] [    INFO]#033[0m - global step 510, epoch: 10, loss: 0.00326, speed: 4.95 step/s#033[0m\u001b[0m\n",
      "\u001b[34m#033[32m[2022-10-21 05:42:49,276] [    INFO]#033[0m - Evaluation precision: 0.49669, recall: 0.38265, F1: 0.43228#033[0m\u001b[0m\n",
      "\u001b[34m2022-10-21 05:42:57,776 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2022-10-21 05:42:57,776 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2022-10-21 05:42:57,777 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-10-21 05:43:15 Uploading - Uploading generated training model\n",
      "2022-10-21 05:45:21 Completed - Training job completed\n",
      "Training seconds: 1020\n",
      "Billable seconds: 1020\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "hyperparameters = {'train_path': '/opt/ml/input/data/training/train.txt', \n",
    "                   'dev_path': '/opt/ml/input/data/training/dev.txt', \n",
    "                   'save_dir': '/opt/ml/model', \n",
    "                   'learning_rate': 1e-5, \n",
    "                   'batch_size': 16, \n",
    "                   'max_seq_len':512, \n",
    "                   'num_epochs': 10, \n",
    "                   'model': 'uie-base',\n",
    "                   'seed': 1000,\n",
    "                   'logging_steps': 1,\n",
    "                   'valid_steps': 10,\n",
    "                   'device': 'gpu'}\n",
    "\n",
    "instance_type = 'ml.p3.2xlarge'  # 'ml.p3.2xlarge' or 'ml.p3.8xlarge' or ...\n",
    "\n",
    "#git_config = {'repo': 'https://github.com/whn09/paddlenlp_sagemaker.git', 'branch': 'main'}\n",
    "\n",
    "estimator = PyTorch(entry_point='finetune.py',\n",
    "                    source_dir='./',\n",
    "                           # git_config=git_config,\n",
    "                    role=role,\n",
    "                    hyperparameters=hyperparameters,\n",
    "                    framework_version='1.9.1',\n",
    "                    py_version='py38',\n",
    "                    script_mode=True,\n",
    "                    instance_count=1,  # 1 or 2 or ...\n",
    "                    instance_type=instance_type,\n",
    "                    # Parameters required to enable checkpointing\n",
    "                    checkpoint_s3_uri=uie_en_model_s3, #使用你自己用来保存/加载模型的s3桶地址, 注意桶需要在us-east-1\n",
    "                    checkpoint_local_path=\"/opt/ml/checkpoints\")\n",
    "\n",
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch-training-2022-10-21-05-25-33-950\n"
     ]
    }
   ],
   "source": [
    "training_job_name = estimator.latest_training_job.name\n",
    "# training_job_name = 'xxx'\n",
    "print(training_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy the trained model to prepare for predictions\n",
    "\n",
    "The deploy() method creates an endpoint (in this case locally) which serves prediction requests in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf model.tar.gz\n",
    "!rm -rf model_*\n",
    "!rm -rf inference.*\n",
    "!aws s3 cp s3://$bucket/$training_job_name/output/model.tar.gz .\n",
    "!tar -xvf model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code/\n",
      "code/infer.py\n",
      "code/.ipynb_checkpoints/\n",
      "code/.ipynb_checkpoints/infer_gpu-checkpoint.py\n",
      "code/.ipynb_checkpoints/model-checkpoint.py\n",
      "code/.ipynb_checkpoints/uie_predictor-checkpoint.py\n",
      "code/uie_predictor.py\n",
      "code/infer_cpu.py\n",
      "code/requirements.txt\n",
      "code/requirements_gpu.txt\n",
      "code/model.py\n",
      "code/infer_gpu.py\n",
      "code/requirements_cpu.txt\n",
      "inference.pdiparams\n",
      "inference.pdiparams.info\n",
      "inference.pdmodel\n",
      "model_config.json\n",
      "model_state.pdparams\n",
      "special_tokens_map.json\n",
      "tokenizer_config.json\n",
      "vocab.txt\n",
      "upload: ./model-inference-gpu.tar.gz to s3://sagemaker-us-west-2-064542430558/pytorch-training-2022-10-21-05-25-33-950/output/model-inference-gpu.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!rm -rf model-inference-gpu.tar.gz\n",
    "# !cp inference.* model/\n",
    "!cp ./model_best/* model/\n",
    "# !cp model/code/requirements_gpu.txt model/code/requirements.txt\n",
    "!cd model && tar -czvf ../model-inference-gpu.tar.gz *\n",
    "!aws s3 cp model-inference-gpu.tar.gz s3://$bucket/$training_job_name/output/model-inference-gpu.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "# instance_type = 'local'\n",
    "# instance_type = 'ml.m5.xlarge'\n",
    "instance_type = 'ml.g4dn.xlarge'\n",
    "\n",
    "# predictor = estimator.deploy(initial_instance_count=1, instance_type=instance_type)\n",
    "\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "pytorch_model = PyTorchModel(model_data='s3://{}/{}/output/model-inference-gpu.tar.gz'.format(bucket, training_job_name), role=role,\n",
    "                             entry_point='infer_gpu.py', framework_version='1.9.0', py_version='py38', model_server_workers=4)  # TODO [For GPU], model_server_workers=6\n",
    "\n",
    "predictor = pytorch_model.deploy(instance_type=instance_type, initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invoking the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "predictor.serializer = JSONSerializer()\n",
    "predictor.deserializer = JSONDeserializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs:  [{'person': [{'text': 'She', 'start': 0, 'end': 3, 'probability': 0.5287898182868958}], 'occupation': [{'text': 'She', 'start': 0, 'end': 3, 'probability': 0.5269210934638977}], 'abuse': [{'text': 'hair', 'start': 33, 'end': 37, 'probability': 0.32205232977867126}, {'text': 'She', 'start': 0, 'end': 3, 'probability': 0.6202351450920105}], 'status': [{'text': 'She', 'start': 0, 'end': 3, 'probability': 0.5892147421836853}], 'location': [{'text': 'She', 'start': 0, 'end': 3, 'probability': 0.4177761673927307}, {'text': 'hair', 'start': 33, 'end': 37, 'probability': 0.7394289970397949}], 'age': [{'text': 'She', 'start': 0, 'end': 3, 'probability': 0.31551605463027954}], 'family': [{'text': 'She', 'start': 0, 'end': 3, 'probability': 0.38014349341392517}], 'parts of body': [{'text': 'hair', 'start': 33, 'end': 37, 'probability': 0.6396911144256592}], 'color': [{'text': 'white', 'start': 27, 'end': 32, 'probability': 0.5680398941040039}], 'gender': [{'text': 'She', 'start': 0, 'end': 3, 'probability': 0.5801038146018982}]}]\n",
      "time: 0.6276085376739502\n"
     ]
    }
   ],
   "source": [
    "texts = ['She had long silky-looking white hair']\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "outputs = predictor.predict(texts)\n",
    "end = time.time()\n",
    "print('outputs: ', outputs)\n",
    "print('time:', end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean-up\n",
    "\n",
    "Deleting the local endpoint when you're finished is important since you can only run one local endpoint at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimator.delete_endpoint()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
