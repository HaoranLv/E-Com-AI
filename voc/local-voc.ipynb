{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c20c88a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.6.0\n",
      "  Using cached transformers-4.6.0-py3-none-any.whl (2.3 MB)\n",
      "Collecting datasets==1.11.0\n",
      "  Using cached datasets-1.11.0-py3-none-any.whl (264 kB)\n",
      "Collecting sentencepiece==0.1.91\n",
      "  Using cached sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1 MB)\n",
      "Collecting pytorch_lightning==0.8.1\n",
      "  Using cached pytorch_lightning-0.8.1-py3-none-any.whl (293 kB)\n",
      "Collecting jieba\n",
      "  Using cached jieba-0.42.1-py3-none-any.whl\n",
      "Collecting editdistance\n",
      "  Using cached editdistance-0.6.0-cp36-cp36m-manylinux2010_x86_64.whl (284 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers==4.6.0->-r requirements.txt (line 1)) (2020.11.13)\n",
      "Collecting huggingface-hub==0.0.8\n",
      "  Using cached huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.53-py3-none-any.whl\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers==4.6.0->-r requirements.txt (line 1)) (3.7.0)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers==4.6.0->-r requirements.txt (line 1)) (21.3)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Using cached tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers==4.6.0->-r requirements.txt (line 1)) (1.19.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers==4.6.0->-r requirements.txt (line 1)) (4.62.3)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers==4.6.0->-r requirements.txt (line 1)) (0.8)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers==4.6.0->-r requirements.txt (line 1)) (2.26.0)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.4.1-py3-none-any.whl (9.9 kB)\n",
      "Collecting fsspec>=2021.05.0\n",
      "  Using cached fsspec-2022.1.0-py3-none-any.whl (133 kB)\n",
      "Requirement already satisfied: dill in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from datasets==1.11.0->-r requirements.txt (line 2)) (0.3.4)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from datasets==1.11.0->-r requirements.txt (line 2)) (0.70.12.2)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from datasets==1.11.0->-r requirements.txt (line 2)) (6.0.1)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from datasets==1.11.0->-r requirements.txt (line 2)) (1.1.5)\n",
      "Collecting xxhash\n",
      "  Using cached xxhash-3.0.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from pytorch_lightning==0.8.1->-r requirements.txt (line 4)) (5.4.1)\n",
      "Requirement already satisfied: torch>=1.3 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from pytorch_lightning==0.8.1->-r requirements.txt (line 4)) (1.7.1)\n",
      "Requirement already satisfied: future>=0.17.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from pytorch_lightning==0.8.1->-r requirements.txt (line 4)) (0.18.2)\n",
      "Collecting tensorboard>=1.14\n",
      "  Using cached tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->transformers==4.6.0->-r requirements.txt (line 1)) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->transformers==4.6.0->-r requirements.txt (line 1)) (2.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->transformers==4.6.0->-r requirements.txt (line 1)) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->transformers==4.6.0->-r requirements.txt (line 1)) (1.26.8)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting grpcio>=1.24.3\n",
      "  Using cached grpcio-1.46.3-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
      "Collecting absl-py>=0.4\n",
      "  Using cached absl_py-1.1.0-py3-none-any.whl (123 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 4)) (2.0.2)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 4)) (3.19.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 4)) (49.6.0.post20210108)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 4)) (0.36.2)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.3.7-py3-none-any.whl (97 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.8.0-py2.py3-none-any.whl (164 kB)\n",
      "     |████████████████████████████████| 164 kB 21.7 MB/s            \n",
      "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Requirement already satisfied: typing_extensions in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from torch>=1.3->pytorch_lightning==0.8.1->-r requirements.txt (line 4)) (4.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from importlib-metadata->transformers==4.6.0->-r requirements.txt (line 1)) (3.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from packaging->transformers==4.6.0->-r requirements.txt (line 1)) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from pandas->datasets==1.11.0->-r requirements.txt (line 2)) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from pandas->datasets==1.11.0->-r requirements.txt (line 2)) (2021.1)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sacremoses->transformers==4.6.0->-r requirements.txt (line 1)) (7.1.2)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sacremoses->transformers==4.6.0->-r requirements.txt (line 1)) (1.15.0)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sacremoses->transformers==4.6.0->-r requirements.txt (line 1)) (1.0.1)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "     |████████████████████████████████| 155 kB 32.7 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 4)) (4.7.2)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata\n",
      "  Downloading importlib_metadata-4.8.3-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.8.1->-r requirements.txt (line 4)) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Installing collected packages: pyasn1-modules, oauthlib, cachetools, requests-oauthlib, importlib-metadata, google-auth, tensorboard-plugin-wit, tensorboard-data-server, markdown, grpcio, google-auth-oauthlib, filelock, absl-py, xxhash, tokenizers, tensorboard, sacremoses, huggingface-hub, fsspec, transformers, sentencepiece, pytorch-lightning, jieba, editdistance, datasets\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 3.7.0\n",
      "    Uninstalling importlib-metadata-3.7.0:\n",
      "      Successfully uninstalled importlib-metadata-3.7.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2021.4.0\n",
      "    Uninstalling fsspec-2021.4.0:\n",
      "      Successfully uninstalled fsspec-2021.4.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "s3fs 2021.4.0 requires fsspec==2021.04.0, but you have fsspec 2022.1.0 which is incompatible.\u001b[0m\n",
      "Successfully installed absl-py-1.1.0 cachetools-4.2.4 datasets-1.11.0 editdistance-0.6.0 filelock-3.4.1 fsspec-2022.1.0 google-auth-2.8.0 google-auth-oauthlib-0.4.6 grpcio-1.46.3 huggingface-hub-0.0.8 importlib-metadata-4.8.3 jieba-0.42.1 markdown-3.3.7 oauthlib-3.2.0 pyasn1-modules-0.2.8 pytorch-lightning-0.8.1 requests-oauthlib-1.3.1 sacremoses-0.0.53 sentencepiece-0.1.91 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tokenizers-0.10.3 transformers-4.6.0 xxhash-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1af0a8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp.so.1 library.\n",
    "#\tTry to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\n",
    "\n",
    "os.environ['MKL_THREADING_LAYER'] = 'GNU'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d38032",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8d9f7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ============================== NEW EXP: TASD on haofang ============================== \n",
      "\n",
      "Here is an example (from dev set) under `extraction` paradigm:\n",
      "Total examples = 697 for data/tasd/haofang/dev.txt\n",
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Input : 商品存在瑕疵,无法运作\n",
      "Output: (不工作,有缺陷, 商品存在瑕疵)\n",
      "\n",
      "****** Conduct Training ******\n",
      "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n",
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: Checkpoint directory ./outputs/tasd/haofang/extraction exists and is not empty with save_top_k != 0.All files in this directory will be deleted when a checkpoint is saved!\n",
      "  warnings.warn(*args, **kwargs)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 252 M \n",
      "Total examples = 697 for data/tasd/haofang/dev.txt\n",
      "Total examples = 5572 for data/tasd/haofang/train.txt                           \n",
      "Total examples = 697 for data/tasd/haofang/dev.txt\n",
      "Epoch 1:  89%|███████▉ | 2786/3135 [09:15<01:09,  5.01it/s, loss=0.102, v_num=2]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1:  89%|████████ | 2787/3135 [09:15<01:09,  5.01it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  89%|████████ | 2789/3135 [09:15<01:08,  5.02it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  89%|████████ | 2791/3135 [09:16<01:08,  5.02it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  89%|████████ | 2793/3135 [09:16<01:08,  5.02it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  89%|████████ | 2795/3135 [09:16<01:07,  5.02it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  89%|████████ | 2797/3135 [09:16<01:07,  5.03it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  89%|████████ | 2799/3135 [09:16<01:06,  5.03it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  89%|████████ | 2801/3135 [09:16<01:06,  5.03it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  89%|████████ | 2803/3135 [09:16<01:05,  5.04it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  89%|████████ | 2805/3135 [09:16<01:05,  5.04it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  90%|████████ | 2807/3135 [09:16<01:05,  5.04it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  90%|████████ | 2809/3135 [09:17<01:04,  5.04it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  90%|████████ | 2811/3135 [09:17<01:04,  5.05it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  90%|████████ | 2813/3135 [09:17<01:03,  5.05it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  90%|████████ | 2815/3135 [09:17<01:03,  5.05it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  90%|████████ | 2817/3135 [09:17<01:02,  5.05it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  90%|████████ | 2819/3135 [09:17<01:02,  5.06it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  90%|████████ | 2821/3135 [09:17<01:02,  5.06it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  90%|████████ | 2823/3135 [09:17<01:01,  5.06it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  90%|████████ | 2825/3135 [09:17<01:01,  5.06it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  90%|████████ | 2827/3135 [09:18<01:00,  5.07it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  90%|████████ | 2829/3135 [09:18<01:00,  5.07it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  90%|████████▏| 2831/3135 [09:18<00:59,  5.07it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  90%|████████▏| 2833/3135 [09:18<00:59,  5.07it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  90%|████████▏| 2835/3135 [09:18<00:59,  5.08it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  90%|████████▏| 2837/3135 [09:18<00:58,  5.08it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2839/3135 [09:18<00:58,  5.08it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2841/3135 [09:18<00:57,  5.08it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2843/3135 [09:18<00:57,  5.09it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2845/3135 [09:18<00:56,  5.09it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2847/3135 [09:19<00:56,  5.09it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2849/3135 [09:19<00:56,  5.09it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2851/3135 [09:19<00:55,  5.10it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2853/3135 [09:19<00:55,  5.10it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2855/3135 [09:19<00:54,  5.10it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2857/3135 [09:19<00:54,  5.11it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2859/3135 [09:19<00:54,  5.11it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2861/3135 [09:19<00:53,  5.11it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2863/3135 [09:19<00:53,  5.11it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2865/3135 [09:20<00:52,  5.12it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  91%|████████▏| 2867/3135 [09:20<00:52,  5.12it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  92%|████████▏| 2869/3135 [09:20<00:51,  5.12it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  92%|████████▏| 2871/3135 [09:20<00:51,  5.12it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  92%|████████▏| 2873/3135 [09:20<00:51,  5.13it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2875/3135 [09:20<00:50,  5.13it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2877/3135 [09:20<00:50,  5.13it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2879/3135 [09:20<00:49,  5.13it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2881/3135 [09:20<00:49,  5.14it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2883/3135 [09:21<00:49,  5.14it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2885/3135 [09:21<00:48,  5.14it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2887/3135 [09:21<00:48,  5.14it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2889/3135 [09:21<00:47,  5.15it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2891/3135 [09:21<00:47,  5.15it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2893/3135 [09:21<00:46,  5.15it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2895/3135 [09:21<00:46,  5.15it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2897/3135 [09:21<00:46,  5.16it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  92%|████████▎| 2899/3135 [09:21<00:45,  5.16it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  93%|████████▎| 2901/3135 [09:22<00:45,  5.16it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  93%|████████▎| 2903/3135 [09:22<00:44,  5.16it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  93%|████████▎| 2905/3135 [09:22<00:44,  5.17it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  93%|████████▎| 2907/3135 [09:22<00:44,  5.17it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  93%|████████▎| 2909/3135 [09:22<00:43,  5.17it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  93%|████████▎| 2911/3135 [09:22<00:43,  5.17it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  93%|████████▎| 2913/3135 [09:22<00:42,  5.18it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  93%|████████▎| 2915/3135 [09:22<00:42,  5.18it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  93%|████████▎| 2917/3135 [09:22<00:42,  5.18it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  93%|████████▍| 2919/3135 [09:22<00:41,  5.18it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  93%|████████▍| 2921/3135 [09:23<00:41,  5.19it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  93%|████████▍| 2923/3135 [09:23<00:40,  5.19it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  93%|████████▍| 2925/3135 [09:23<00:40,  5.19it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  93%|████████▍| 2927/3135 [09:23<00:40,  5.20it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  93%|████████▍| 2929/3135 [09:23<00:39,  5.20it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  93%|████████▍| 2931/3135 [09:23<00:39,  5.20it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 2933/3135 [09:23<00:38,  5.20it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 2935/3135 [09:23<00:38,  5.21it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 2937/3135 [09:23<00:38,  5.21it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 2939/3135 [09:24<00:37,  5.21it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 2941/3135 [09:24<00:37,  5.21it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 2943/3135 [09:24<00:36,  5.22it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 2945/3135 [09:24<00:36,  5.22it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 2947/3135 [09:24<00:36,  5.22it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 2949/3135 [09:24<00:35,  5.22it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 2951/3135 [09:24<00:35,  5.23it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 2953/3135 [09:24<00:34,  5.23it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 2955/3135 [09:24<00:34,  5.23it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 2957/3135 [09:25<00:34,  5.23it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  94%|████████▍| 2959/3135 [09:25<00:33,  5.24it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  94%|████████▌| 2961/3135 [09:25<00:33,  5.24it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 2963/3135 [09:25<00:32,  5.24it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 2965/3135 [09:25<00:32,  5.24it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 2967/3135 [09:25<00:32,  5.25it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 2969/3135 [09:25<00:31,  5.25it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 2971/3135 [09:25<00:31,  5.25it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 2973/3135 [09:25<00:30,  5.25it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 2975/3135 [09:26<00:30,  5.26it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 2977/3135 [09:26<00:30,  5.26it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 2979/3135 [09:26<00:29,  5.26it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 2981/3135 [09:26<00:29,  5.26it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 2983/3135 [09:26<00:28,  5.27it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 2985/3135 [09:26<00:28,  5.27it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 2987/3135 [09:26<00:28,  5.27it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 2989/3135 [09:26<00:27,  5.27it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 2991/3135 [09:26<00:27,  5.28it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  95%|████████▌| 2993/3135 [09:26<00:26,  5.28it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  96%|████████▌| 2995/3135 [09:27<00:26,  5.28it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  96%|████████▌| 2997/3135 [09:27<00:26,  5.28it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  96%|████████▌| 2999/3135 [09:27<00:25,  5.29it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  96%|████████▌| 3001/3135 [09:27<00:25,  5.29it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  96%|████████▌| 3003/3135 [09:27<00:24,  5.29it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  96%|████████▋| 3005/3135 [09:27<00:24,  5.29it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  96%|████████▋| 3007/3135 [09:27<00:24,  5.30it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  96%|████████▋| 3009/3135 [09:27<00:23,  5.30it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  96%|████████▋| 3011/3135 [09:27<00:23,  5.30it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  96%|████████▋| 3013/3135 [09:28<00:23,  5.30it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  96%|████████▋| 3015/3135 [09:28<00:22,  5.31it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  96%|████████▋| 3017/3135 [09:28<00:22,  5.31it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  96%|████████▋| 3019/3135 [09:28<00:21,  5.31it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  96%|████████▋| 3021/3135 [09:28<00:21,  5.31it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  96%|████████▋| 3023/3135 [09:28<00:21,  5.32it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  96%|████████▋| 3025/3135 [09:28<00:20,  5.32it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  97%|████████▋| 3027/3135 [09:28<00:20,  5.32it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  97%|████████▋| 3029/3135 [09:28<00:19,  5.32it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  97%|████████▋| 3031/3135 [09:29<00:19,  5.33it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  97%|████████▋| 3033/3135 [09:29<00:19,  5.33it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  97%|████████▋| 3035/3135 [09:29<00:18,  5.33it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  97%|████████▋| 3037/3135 [09:29<00:18,  5.33it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  97%|████████▋| 3039/3135 [09:29<00:17,  5.34it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  97%|████████▋| 3041/3135 [09:29<00:17,  5.34it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  97%|████████▋| 3043/3135 [09:29<00:17,  5.34it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  97%|████████▋| 3045/3135 [09:29<00:16,  5.34it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  97%|████████▋| 3047/3135 [09:29<00:16,  5.35it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  97%|████████▊| 3049/3135 [09:30<00:16,  5.35it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  97%|████████▊| 3051/3135 [09:30<00:15,  5.35it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  97%|████████▊| 3053/3135 [09:30<00:15,  5.35it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  97%|████████▊| 3055/3135 [09:30<00:14,  5.36it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3057/3135 [09:30<00:14,  5.36it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3059/3135 [09:30<00:14,  5.36it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3061/3135 [09:30<00:13,  5.36it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3063/3135 [09:30<00:13,  5.37it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3065/3135 [09:30<00:13,  5.37it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3067/3135 [09:31<00:12,  5.37it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3069/3135 [09:31<00:12,  5.37it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3071/3135 [09:31<00:11,  5.38it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3073/3135 [09:31<00:11,  5.38it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3075/3135 [09:31<00:11,  5.38it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3077/3135 [09:31<00:10,  5.38it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3079/3135 [09:31<00:10,  5.39it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3081/3135 [09:31<00:10,  5.39it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3083/3135 [09:31<00:09,  5.39it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3085/3135 [09:31<00:09,  5.39it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  98%|████████▊| 3087/3135 [09:32<00:08,  5.40it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  99%|████████▊| 3089/3135 [09:32<00:08,  5.40it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  99%|████████▊| 3091/3135 [09:32<00:08,  5.40it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 3093/3135 [09:32<00:07,  5.40it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 3095/3135 [09:32<00:07,  5.41it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 3097/3135 [09:32<00:07,  5.41it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 3099/3135 [09:32<00:06,  5.41it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 3101/3135 [09:32<00:06,  5.41it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 3103/3135 [09:32<00:05,  5.42it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 3105/3135 [09:33<00:05,  5.42it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 3107/3135 [09:33<00:05,  5.42it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 3109/3135 [09:33<00:04,  5.42it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 3111/3135 [09:33<00:04,  5.43it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 3113/3135 [09:33<00:04,  5.43it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 3115/3135 [09:33<00:03,  5.43it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 3117/3135 [09:33<00:03,  5.43it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1:  99%|████████▉| 3119/3135 [09:33<00:02,  5.44it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1: 100%|████████▉| 3121/3135 [09:33<00:02,  5.44it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1: 100%|████████▉| 3123/3135 [09:34<00:02,  5.44it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1: 100%|████████▉| 3125/3135 [09:34<00:01,  5.44it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1: 100%|████████▉| 3127/3135 [09:34<00:01,  5.45it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1: 100%|████████▉| 3129/3135 [09:34<00:01,  5.45it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1: 100%|████████▉| 3131/3135 [09:34<00:00,  5.45it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1: 100%|████████▉| 3133/3135 [09:34<00:00,  5.45it/s, loss=0.102, v_num=2]\u001b[A\n",
      "Epoch 1: 100%|█| 3135/3135 [09:34<00:00,  5.45it/s, loss=0.102, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|█| 3135/3135 [09:41<00:00,  5.39it/s, loss=0.102, v_num=2, val_los\u001b[A\n",
      "Finish training and saving the model!\n",
      "CPU times: user 9.21 s, sys: 2.46 s, total: 11.7 s\n",
      "Wall time: 9min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python -u main.py --task tasd \\\n",
    "            --dataset haofang \\\n",
    "            --paradigm extraction \\\n",
    "            --n_gpu '0' \\\n",
    "            --model_name_or_path 'lemon234071/t5-base-Chinese' \\\n",
    "            --do_train \\\n",
    "            --train_batch_size 2 \\\n",
    "            --gradient_accumulation_steps 2 \\\n",
    "            --eval_batch_size 2 \\\n",
    "            --learning_rate 3e-4 \\\n",
    "            --num_train_epochs 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086f90b6",
   "metadata": {},
   "source": [
    "# eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2787ddd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ============================== NEW EXP: TASD on haofang ============================== \n",
      "\n",
      "Here is an example (from dev set) under `extraction` paradigm:\n",
      "Total examples = 697 for data/tasd/haofang/dev.txt\n",
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Input : 商品存在瑕疵,无法运作\n",
      "Output: (不工作,有缺陷, 商品存在瑕疵)\n",
      "\n",
      "****** Conduct Evaluating with the last state ******\n",
      "\n",
      "Load the trained model from outputs/tasd/haofang/extraction/cktepoch=1.ckpt...\n",
      "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n",
      "<<< read lines\n",
      "Total examples = 697 for data/tasd/haofang/test.txt\n",
      "<<< load test data\n",
      "Total examples = 697 for data/tasd/haofang/test.txt\n",
      "<<<< start evaluate\n",
      "100%|█████████████████████████████████████████████| 6/6 [00:06<00:00,  1.10s/it]\n",
      "\n",
      "Results of raw output, only tag category\n",
      "<<<< res {'消费者体验': {'n_tp': 0, 'n_gold': 1, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, '商品存在瑕疵': {'n_tp': 696, 'n_gold': 696, 'n_pred': 697, 'precision': 0.9985652797704447, 'recall': 1.0, 'f1': 0.9992821249102656}}\n",
      "{'total precision': 0.9985652797704447, 'total recall': 0.9985652797704447, 'total f1': 0.9985652797704447}\n",
      "\n",
      "Results of raw output, total\n",
      "{'total precision': 0.6441893830703013, 'total recall': 0.6441893830703013, 'total f1': 0.6441893830703013}\n",
      "raw results:  [['商品存在瑕疵'], ['商品存在瑕疵'], ['商品存在瑕疵'], ['商品存在瑕疵'], ['商品存在瑕疵']]\n",
      "all_predictions_fixed:  [['商品存在瑕疵'], ['商品存在瑕疵'], ['商品存在瑕疵'], ['商品存在瑕疵'], ['商品存在瑕疵']]\n",
      "\n",
      "Results of fixed output\n",
      "<<<< res {'消费者体验': {'n_tp': 0, 'n_gold': 1, 'n_pred': 0, 'precision': 0, 'recall': 0.0, 'f1': 0}, '商品存在瑕疵': {'n_tp': 696, 'n_gold': 696, 'n_pred': 697, 'precision': 0.9985652797704447, 'recall': 1.0, 'f1': 0.9992821249102656}}\n",
      "{'total precision': 0.9985652797704447, 'total recall': 0.9985652797704447, 'total f1': 0.9985652797704447}\n"
     ]
    }
   ],
   "source": [
    "!python main.py --task tasd \\\n",
    "            --dataset haofang \\\n",
    "            --ckpoint_path outputs/tasd/haofang/extraction/cktepoch=1.ckpt \\\n",
    "            --model_name_or_path 'lemon234071/t5-base-Chinese' \\\n",
    "            --paradigm extraction \\\n",
    "            --n_gpu '0' \\\n",
    "            --do_direct_eval \\\n",
    "            --eval_batch_size 128 \\\n",
    "            --customer_jj False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d0bc01",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b276b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ============================== NEW EXP: TASD on haofang ============================== \n",
      "\n",
      "Here is an example (from dev set) under `extraction` paradigm:\n",
      "Total examples = 697 for data/tasd/haofang/dev.txt\n",
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Input : 商品存在瑕疵,无法运作\n",
      "Output: (不工作,有缺陷, 商品存在瑕疵)\n",
      "\n",
      "****** Conduct predicting with the last state ******\n",
      "\n",
      "Load the trained model from outputs/tasd/haofang/extraction/cktepoch=1.ckpt...\n",
      "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python main.py --task tasd \\\n",
    "            --dataset haofang \\\n",
    "            --ckpoint_path outputs/tasd/haofang/extraction/cktepoch=1.ckpt \\\n",
    "            --model_name_or_path 'lemon234071/t5-base-Chinese' \\\n",
    "            --text \"商品存在瑕疵,最初的2到3次工作非常好，然后停止工作，不知道为什么\" \\\n",
    "            --paradigm extraction \\\n",
    "            --n_gpu 0 \\\n",
    "            --do_direct_predict \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2d93dd",
   "metadata": {},
   "source": [
    "## output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c761b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "f = open('./outputs/tasd/haofang/extraction/results-tasd-haofang-extraction-pred.pickle','rb')\n",
    "x = pickle.load(f)\n",
    "\n",
    "sent = [' '.join(i) for i in x['sent']]\n",
    "res_table = pd.DataFrame({'sentence':sent,\"label\":x['label'],\"prediction\":x['pred']})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
