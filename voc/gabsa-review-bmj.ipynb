{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5606b6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker as sage\n",
    "import pandas as pd\n",
    "from time import gmtime, strftime\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa5dd0c",
   "metadata": {},
   "source": [
    "# data prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc07083",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python data_prepare.py\\\n",
    "--label_dir '../data/good'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8889dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python data_prepare.py\\\n",
    "--label_dir '../data/bad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d0c21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge data\n",
    "good = '../data/good/aspect_category.csv'\n",
    "bad = '../data/bad/aspect_category.csv'\n",
    "df1 = pd.read_csv(good)\n",
    "df2 = pd.read_csv(bad)\n",
    "df_res = pd.concat([df1,df2])\n",
    "df_res.to_csv('./aspect_category.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93eca644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  Unnamed: 0.1  sent_num  \\\n",
      "0           0             0         0   \n",
      "1           1             0         0   \n",
      "2           2             0         0   \n",
      "3           3             0         0   \n",
      "4           4             0         0   \n",
      "\n",
      "                                                text  sent_start  sent_end  \\\n",
      "0  I like the shorts. They're comfortable. But I ...           0       313   \n",
      "1  Absolutely love these shorts! The green color ...           0       215   \n",
      "2  I ordered the purple tie dye color and was sen...           0       322   \n",
      "3  I am 5’1 and around 110-112 lbs. I love these ...           0       453   \n",
      "4  I am obsessed with these shorts. I’ve ordered ...           0       351   \n",
      "\n",
      "   sent_len                                              label  \n",
      "0       313  [('comfortable', 'feelings'), ('being able to ...  \n",
      "1       215  [('green color was perfect', 'color'), ('runni...  \n",
      "2       322  [('Just wish I was given the correct color', '...  \n",
      "3       453  [('I love these shorts', 'feelings'), ('perfec...  \n",
      "4       351  [('I’ve ordered multiple colors', 'purchase_be...  \n",
      "<< path valid!\n",
      "training size:  (826, 8)\n",
      "test size:  (104, 8)\n",
      "validate size:  (103, 8)\n",
      "<<<finish data preparing!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd   \n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "#preprocess data\n",
    "def write_txt(df,path):\n",
    "    '''\n",
    "    write back to txt\n",
    "    '''\n",
    "    #output txt file\n",
    "    df = df.reset_index()\n",
    "    with open(path,'a')as f:\n",
    "        for i in range(len(df)):\n",
    "            f.write(\"{} #### {}\".format(df.loc[i,'text'].strip(),df.loc[i,'label']))\n",
    "            f.write('\\n')\n",
    "            \n",
    "            \n",
    "def mkdir_rm(folder):\n",
    "    '''\n",
    "    make directory if not exists\n",
    "    '''\n",
    "    if os.path.exists(folder):\n",
    "        shutil.rmtree(folder) \n",
    "    os.mkdir(folder)\n",
    "    print (\"<< path valid!\")\n",
    "    \n",
    "\n",
    "def preprocess_data(input_file,output_path,over_sample=True):\n",
    "    jsonObj = pd.read_csv(input_file)\n",
    "    jsonObj = jsonObj[jsonObj['label']!='[]']\n",
    "    print (jsonObj.head())\n",
    "    \n",
    "    #remove & remake the output folder \n",
    "    mkdir_rm(output_path)\n",
    "    \n",
    "    #generate tag.txt\n",
    "    #a_list = ['consumer','zone','target','consequence','product','product_spec']\n",
    "    #with open('tag.txt', 'w') as filehandle:\n",
    "     #   filehandle.writelines(\"%s\\n\" % tag for tag in a_list)\n",
    "    \n",
    "    #train/test/val split\n",
    "    train, validate, test = np.split(jsonObj.sample(frac=1), [int(.8*len(jsonObj)), int(.9*len(jsonObj))])\n",
    "   \n",
    "    print (\"training size: \",train.shape)\n",
    "    print (\"test size: \",test.shape)\n",
    "    print (\"validate size: \",validate.shape)\n",
    "    \n",
    "    # write train/test/dev\n",
    "    write_txt(train,os.path.join(output_path,'train.txt'))\n",
    "    write_txt(test,os.path.join(output_path,'test.txt'))\n",
    "    write_txt(validate,os.path.join(output_path,'dev.txt'))\n",
    "    print (\"<<<finish data preparing!\")\n",
    "    \n",
    "input_file = './aspect_category.csv'\n",
    "output_path = './data/tasd/bmjl'\n",
    "preprocess_data(input_file,output_path,over_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe8a99e",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ee4a232",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sage.Session()\n",
    "\n",
    "WORK_DIRECTORY = \"./data\"\n",
    "\n",
    "# S3 prefix\n",
    "prefix = \"bmjl\"\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "data_location = sess.upload_data(WORK_DIRECTORY, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9659e719",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"task\" : \"tasd\", \n",
    "    \"dataset\" : \"bmjl\", \n",
    "    \"model_name_or_path\" : \"t5-base\", \n",
    "    \"paradigm\": \"extraction\",\n",
    "    \"eval_batch_size\" :\"16\",\n",
    "    \"train_batch_size\" :\"2\",\n",
    "    \"learning_rate\" :\"3e-4\",\n",
    "    \"num_train_epochs\":\"30\",\n",
    "    \"n_gpu\": \"1\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25607343",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_point = 'finetune.py'\n",
    "source_dir = './'\n",
    "git_config = None\n",
    "role = get_execution_role()\n",
    "framework_version = '1.7.1'\n",
    "py_version='py36'\n",
    "instance_type='ml.p3.2xlarge'\n",
    "#instance_type='local_gpu'\n",
    "instance_count=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5df29a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(\n",
    "    entry_point = entry_point,\n",
    "    source_dir = source_dir,\n",
    "    git_config = git_config,\n",
    "    role = role,\n",
    "    debugger_hook_config=False,\n",
    "    hyperparameters = hyperparameters,\n",
    "    framework_version = framework_version, \n",
    "    py_version = py_version,\n",
    "    instance_type = instance_type,\n",
    "    instance_count = instance_count\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49bffdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {'tasd': data_location+'/tasd/'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a570005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-12 07:19:56 Starting - Starting the training job...\n",
      "2022-08-12 07:20:25 Starting - Preparing the instances for trainingProfilerReport-1660288795: InProgress\n",
      ".........\n",
      "2022-08-12 07:21:52 Downloading - Downloading input data...\n",
      "2022-08-12 07:22:22 Training - Downloading the training image................"
     ]
    }
   ],
   "source": [
    "response = estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20608ab",
   "metadata": {},
   "source": [
    "# deploy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04da6cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "instance_type = 'ml.m5.4xlarge'\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aebd32ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_model = estimator.model_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aae92654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "pytorch_model = PyTorchModel(model_data=s3_model, \n",
    "                             role=role,\n",
    "                             entry_point='inference.py', \n",
    "                             source_dir='./', \n",
    "                             framework_version='1.7.1', \n",
    "                             py_version='py36'\n",
    "                ) # TODO set model_server_workers=1 to avoid torchhub bug\n",
    "\n",
    "predictor = pytorch_model.deploy(instance_type=instance_type, initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7250f3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28116/2181014825.py\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mruntime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"runtime.sagemaker\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m response = runtime.invoke_endpoint(\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mEndpointName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mContentType\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"application/json\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mBody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictor' is not defined"
     ]
    }
   ],
   "source": [
    "from boto3.session import Session\n",
    "import json\n",
    "\n",
    "body = {\"inputs\": \"I am pretty new to pickleball and finally decided to try out some different paddles.\"}\n",
    "\n",
    "session = Session()\n",
    "runtime = session.client(\"runtime.sagemaker\")\n",
    "response = runtime.invoke_endpoint(\n",
    "    EndpointName=predictor.endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(body),\n",
    ")\n",
    "result = json.loads(response[\"Body\"].read())\n",
    "print (result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c231f998",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.2 ms, sys: 376 µs, total: 12.5 ms\n",
      "Wall time: 3.54 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'result': '(pretty new to pickleball, scene); (finally decided to try out some different paddles, purchase_behavior); (pretty new to pickleball, scene); (pretty new to pickleball, scene); (pretty new to pickleball, scene); (pretty new to pickleball, scene); (pretty new to pickleball, scene); (pretty new to pickleball, scene); (pretty new to pickleball, scene)'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "predictor.serializer = sagemaker.serializers.JSONSerializer()\n",
    "predictor.deserializer = sagemaker.deserializers.JSONDeserializer()\n",
    "\n",
    "body = {\"inputs\": \"I am pretty new to pickleball and finally decided to try out some different paddles.\"}\n",
    "\n",
    "predictor.predict(body,initial_args={\"ContentType\":\"application/json\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdd2763",
   "metadata": {},
   "source": [
    "# batch transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90362e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict.jsonl uploaded to s3://sagemaker-us-east-1-726335585155/batch_transform/input/predict.jsonl\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from sagemaker.s3 import S3Uploader,s3_path_join\n",
    "\n",
    "# get the s3 bucket\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "#prepare data\n",
    "dataset_csv_file = 'predict_0811.csv'\n",
    "dataset_jsonl_file = \"predict.jsonl\"\n",
    "\n",
    "\n",
    "i = 0\n",
    "with open(dataset_csv_file, \"r+\") as infile, open(dataset_jsonl_file, \"w+\") as outfile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    for row in reader:\n",
    "        if i <5:\n",
    "            json.dump({\"inputs\":row[\"0\"]}, outfile)\n",
    "            outfile.write('\\n')\n",
    "        i = i+1\n",
    "                \n",
    "# uploads a given file to S3.\n",
    "input_s3_path = s3_path_join(\"s3://\",sagemaker_session_bucket,\"batch_transform/input\")\n",
    "output_s3_path = s3_path_join(\"s3://\",sagemaker_session_bucket,\"batch_transform/output\")\n",
    "s3_file_uri = S3Uploader.upload(dataset_jsonl_file,input_s3_path)\n",
    "\n",
    "print(f\"{dataset_jsonl_file} uploaded to {s3_file_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "751d8a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create transformer to run a batch job\n",
    "batch_job = pytorch_model.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type='ml.g4dn.xlarge',\n",
    "    output_path=output_s3_path,\n",
    "    strategy='SingleRecord'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "15da8371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................................................\u001b[34mCollecting transformers==4.6.0\n",
      "  Downloading transformers-4.6.0-py3-none-any.whl (2.3 MB)\u001b[0m\n",
      "\u001b[34mCollecting datasets==1.11.0\n",
      "  Downloading datasets-1.11.0-py3-none-any.whl (264 kB)\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece==0.1.91\n",
      "  Downloading sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1 MB)\u001b[0m\n",
      "\u001b[34mCollecting pytorch_lightning==0.8.1\n",
      "  Downloading pytorch_lightning-0.8.1-py3-none-any.whl (293 kB)\u001b[0m\n",
      "\u001b[34mCollecting jieba\n",
      "  Downloading jieba-0.42.1.tar.gz (19.2 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting editdistance\n",
      "  Downloading editdistance-0.6.0-cp36-cp36m-manylinux2010_x86_64.whl (284 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (2.22.0)\u001b[0m\n",
      "\u001b[34mCollecting filelock\n",
      "  Downloading filelock-3.4.1-py3-none-any.whl (9.9 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (4.62.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (20.4)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17\n",
      "  Downloading regex-2022.7.25-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub==0.0.8\n",
      "  Downloading huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\u001b[0m\n",
      "\u001b[34mCollecting importlib-metadata\n",
      "  Downloading importlib_metadata-4.8.3-py3-none-any.whl (17 kB)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting dill\n",
      "  Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from datasets==1.11.0->-r /opt/ml/model/code/requirements.txt (line 2)) (0.25.0)\u001b[0m\n",
      "\u001b[34mCollecting fsspec>=2021.05.0\n",
      "  Downloading fsspec-2022.1.0-py3-none-any.whl (133 kB)\u001b[0m\n",
      "\u001b[34mCollecting multiprocess\n",
      "  Downloading multiprocess-0.70.12.2-py36-none-any.whl (106 kB)\u001b[0m\n",
      "\u001b[34mCollecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyarrow!=4.0.0,>=1.0.0\n",
      "  Downloading pyarrow-6.0.1-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25.6 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: PyYAML>=5.1 in /opt/conda/lib/python3.6/site-packages (from pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.3 in /opt/conda/lib/python3.6/site-packages (from pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (1.7.1)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard>=1.14\n",
      "  Downloading tensorboard-2.10.0-py3-none-any.whl (5.9 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: future>=0.17.1 in /opt/conda/lib/python3.6/site-packages (from pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (0.18.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (2021.5.30)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (49.6.0.post20210108)\u001b[0m\n",
      "\u001b[34mCollecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.4-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\u001b[0m\n",
      "\u001b[34mCollecting absl-py>=0.4\n",
      "  Downloading absl_py-1.2.0-py3-none-any.whl (123 kB)\u001b[0m\n",
      "\u001b[34mCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.0.3-py3-none-any.whl (289 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (0.37.0)\u001b[0m\n",
      "\u001b[34mCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.10.0-py2.py3-none-any.whl (167 kB)\u001b[0m\n",
      "\u001b[34mCollecting grpcio>=1.24.3\n",
      "  Downloading grpcio-1.47.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\u001b[0m\n",
      "\u001b[34mCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.7-py3-none-any.whl (97 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from torch>=1.3->pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (4.0.0)\u001b[0m\n",
      "\u001b[34mCollecting zipp>=0.5\n",
      "  Downloading zipp-3.6.0-py3-none-any.whl (5.3 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from packaging->transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (3.0.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets==1.11.0->-r /opt/ml/model/code/requirements.txt (line 2)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets==1.11.0->-r /opt/ml/model/code/requirements.txt (line 2)) (2021.3)\u001b[0m\n",
      "\u001b[34mCollecting click\n",
      "  Downloading click-8.0.4-py3-none-any.whl (97 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (1.0.1)\u001b[0m\n",
      "\u001b[34mCollecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (4.7.2)\u001b[0m\n",
      "\u001b[34mCollecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\u001b[0m\n",
      "\u001b[34mCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (0.4.8)\u001b[0m\n",
      "\u001b[34mCollecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: jieba, sacremoses\n",
      "  Building wheel for jieba (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for jieba (setup.py): finished with status 'done'\n",
      "  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314476 sha256=8ac49623d58b5a375cb1b347a7a9ca329aa4e1b8959d349df09e0a3e094ddc31\n",
      "  Stored in directory: /root/.cache/pip/wheels/17/a7/8b/a7e03881534e78558920ac68aaeca05180c0e2c3d11c4fce3b\n",
      "  Building wheel for sacremoses (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895253 sha256=1671dca53725f4706cad67b6990f7f05b9e1e74db4e7f222b16f082ef3f9e079\n",
      "  Stored in directory: /root/.cache/pip/wheels/4c/64/31/e9900a234b23fb3e9dc565d6114a9d6ff84a72dbdd356502b4\u001b[0m\n",
      "\u001b[34mSuccessfully built jieba sacremoses\u001b[0m\n",
      "\u001b[34mInstalling collected packages: zipp, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, importlib-metadata, google-auth, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, regex, protobuf, markdown, grpcio, google-auth-oauthlib, filelock, dill, click, absl-py, xxhash, tokenizers, tensorboard, sacremoses, pyarrow, multiprocess, huggingface-hub, fsspec, transformers, sentencepiece, pytorch-lightning, jieba, editdistance, datasets\u001b[0m\n",
      "\u001b[34mSuccessfully installed absl-py-1.2.0 cachetools-4.2.4 click-8.0.4 datasets-1.11.0 dill-0.3.4 editdistance-0.6.0 filelock-3.4.1 fsspec-2022.1.0 google-auth-2.10.0 google-auth-oauthlib-0.4.6 grpcio-1.47.0 huggingface-hub-0.0.8 importlib-metadata-4.8.3 jieba-0.42.1 markdown-3.3.7 multiprocess-0.70.12.2 oauthlib-3.2.0 protobuf-3.19.4 pyarrow-6.0.1 pyasn1-modules-0.2.8 pytorch-lightning-0.8.1 regex-2022.7.25 requests-oauthlib-1.3.1 sacremoses-0.0.53 sentencepiece-0.1.91 tensorboard-2.10.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tokenizers-0.10.3 transformers-4.6.0 werkzeug-2.0.3 xxhash-3.0.0 zipp-3.6.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2022-08-12 07:03:57,057 [INFO ] main org.pytorch.serve.ModelServer - \u001b[0m\n",
      "\u001b[34mTorchserve version: 0.3.1\u001b[0m\n",
      "\u001b[34mTS Home: /opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mCurrent directory: /\u001b[0m\n",
      "\u001b[34mTemp directory: /home/model-server/tmp\u001b[0m\n",
      "\u001b[34mNumber of GPUs: 1\u001b[0m\n",
      "\u001b[34mNumber of CPUs: 4\u001b[0m\n",
      "\u001b[34mMax heap size: 2968 M\u001b[0m\n",
      "\u001b[34mPython executable: /opt/conda/bin/python3.6\u001b[0m\n",
      "\u001b[34mConfig file: /etc/sagemaker-ts.properties\u001b[0m\n",
      "\u001b[34mInference address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mManagement address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mMetrics address: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[34mModel Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[34mInitial Models: model.mar\u001b[0m\n",
      "\u001b[34mLog dir: /logs\u001b[0m\n",
      "\u001b[34mMetrics dir: /logs\u001b[0m\n",
      "\u001b[34mNetty threads: 0\u001b[0m\n",
      "\u001b[34mNetty client threads: 0\u001b[0m\n",
      "\u001b[34mDefault workers per model: 1\u001b[0m\n",
      "\u001b[34mBlacklist Regex: N/A\u001b[0m\n",
      "\u001b[34mMaximum Response Size: 6553500\u001b[0m\n",
      "\u001b[34mMaximum Request Size: 6553500\u001b[0m\n",
      "\u001b[34mPrefer direct buffer: false\u001b[0m\n",
      "\u001b[34mAllowed Urls: [file://.*|http(s)?://.*]\u001b[0m\n",
      "\u001b[34mCustom python dependency for model allowed: false\u001b[0m\n",
      "\u001b[34mMetrics report format: prometheus\u001b[0m\n",
      "\u001b[34mEnable metrics API: true\u001b[0m\n",
      "\u001b[34m2022-08-12 07:03:57,095 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: model.mar\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:52,582 [INFO ] main org.pytorch.serve.archive.ModelArchive - eTag 1d4d2150b0f246cd8e26213df6fd68e7\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:52,594 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:52,608 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:52,716 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:52,716 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:52,718 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:52,582 [INFO ] main org.pytorch.serve.archive.ModelArchive - eTag 1d4d2150b0f246cd8e26213df6fd68e7\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:52,594 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:52,608 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:52,716 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:52,716 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:52,718 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:52,759 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:52,760 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]94\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:52,761 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:52,761 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:52,771 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:52,759 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:52,760 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]94\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:52,761 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:52,761 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:52,771 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:52,791 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:52,919 [INFO ] pool-1-thread-2 ACCESS_LOG - /169.254.255.130:55436 \"GET /ping HTTP/1.1\" 200 16\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:52,924 [INFO ] pool-1-thread-2 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:501da5bbdb65,timestamp:null\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:53,072 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:55452 \"GET /execution-parameters HTTP/1.1\" 404 1\u001b[0m\n",
      "\u001b[34mModel server started.\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:53,072 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:501da5bbdb65,timestamp:null\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:53,211 [INFO ] pool-2-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:501da5bbdb65,timestamp:1660287893\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:53,212 [INFO ] pool-2-thread-1 TS_METRICS - DiskAvailable.Gigabytes:79.44116592407227|#Level:Host|#hostname:501da5bbdb65,timestamp:1660287893\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:53,212 [INFO ] pool-2-thread-1 TS_METRICS - DiskUsage.Gigabytes:29.125835418701172|#Level:Host|#hostname:501da5bbdb65,timestamp:1660287893\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:53,213 [INFO ] pool-2-thread-1 TS_METRICS - DiskUtilization.Percent:26.8|#Level:Host|#hostname:501da5bbdb65,timestamp:1660287893\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:53,213 [INFO ] pool-2-thread-1 TS_METRICS - MemoryAvailable.Megabytes:14272.27734375|#Level:Host|#hostname:501da5bbdb65,timestamp:1660287893\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:53,214 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUsed.Megabytes:1140.94140625|#Level:Host|#hostname:501da5bbdb65,timestamp:1660287893\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:53,214 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUtilization.Percent:9.3|#Level:Host|#hostname:501da5bbdb65,timestamp:1660287893\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:52,791 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:52,919 [INFO ] pool-1-thread-2 ACCESS_LOG - /169.254.255.130:55436 \"GET /ping HTTP/1.1\" 200 16\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:52,924 [INFO ] pool-1-thread-2 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:501da5bbdb65,timestamp:null\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:53,072 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:55452 \"GET /execution-parameters HTTP/1.1\" 404 1\u001b[0m\n",
      "\u001b[35mModel server started.\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:53,072 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:501da5bbdb65,timestamp:null\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:53,211 [INFO ] pool-2-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:501da5bbdb65,timestamp:1660287893\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:53,212 [INFO ] pool-2-thread-1 TS_METRICS - DiskAvailable.Gigabytes:79.44116592407227|#Level:Host|#hostname:501da5bbdb65,timestamp:1660287893\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:53,212 [INFO ] pool-2-thread-1 TS_METRICS - DiskUsage.Gigabytes:29.125835418701172|#Level:Host|#hostname:501da5bbdb65,timestamp:1660287893\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:53,213 [INFO ] pool-2-thread-1 TS_METRICS - DiskUtilization.Percent:26.8|#Level:Host|#hostname:501da5bbdb65,timestamp:1660287893\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:53,213 [INFO ] pool-2-thread-1 TS_METRICS - MemoryAvailable.Megabytes:14272.27734375|#Level:Host|#hostname:501da5bbdb65,timestamp:1660287893\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:53,214 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUsed.Megabytes:1140.94140625|#Level:Host|#hostname:501da5bbdb65,timestamp:1660287893\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:53,214 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUtilization.Percent:9.3|#Level:Host|#hostname:501da5bbdb65,timestamp:1660287893\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:53,988 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: transformers==4.6.0 in /opt/conda/lib/python3.6/site-packages (from -r /opt/ml/model/code/requirements.txt (line 1)) (4.6.0)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:53,989 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: datasets==1.11.0 in /opt/conda/lib/python3.6/site-packages (from -r /opt/ml/model/code/requirements.txt (line 2)) (1.11.0)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:53,990 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: sentencepiece==0.1.91 in /opt/conda/lib/python3.6/site-packages (from -r /opt/ml/model/code/requirements.txt (line 3)) (0.1.91)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:53,990 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: pytorch_lightning==0.8.1 in /opt/conda/lib/python3.6/site-packages (from -r /opt/ml/model/code/requirements.txt (line 4)) (0.8.1)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:53,991 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: jieba in /opt/conda/lib/python3.6/site-packages (from -r /opt/ml/model/code/requirements.txt (line 5)) (0.42.1)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:53,988 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: transformers==4.6.0 in /opt/conda/lib/python3.6/site-packages (from -r /opt/ml/model/code/requirements.txt (line 1)) (4.6.0)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:53,989 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: datasets==1.11.0 in /opt/conda/lib/python3.6/site-packages (from -r /opt/ml/model/code/requirements.txt (line 2)) (1.11.0)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:53,990 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: sentencepiece==0.1.91 in /opt/conda/lib/python3.6/site-packages (from -r /opt/ml/model/code/requirements.txt (line 3)) (0.1.91)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:53,990 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: pytorch_lightning==0.8.1 in /opt/conda/lib/python3.6/site-packages (from -r /opt/ml/model/code/requirements.txt (line 4)) (0.8.1)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:53,991 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: jieba in /opt/conda/lib/python3.6/site-packages (from -r /opt/ml/model/code/requirements.txt (line 5)) (0.42.1)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:53,991 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: editdistance in /opt/conda/lib/python3.6/site-packages (from -r /opt/ml/model/code/requirements.txt (line 6)) (0.6.0)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,150 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (4.62.3)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,152 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (4.8.3)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,152 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: huggingface-hub==0.0.8 in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (0.0.8)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,153 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (2.22.0)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,153 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (0.10.3)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,154 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (2022.7.25)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,154 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (3.4.1)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,155 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (20.4)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,155 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: sacremoses in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (0.0.53)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,156 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,157 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (0.8)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,266 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from datasets==1.11.0->-r /opt/ml/model/code/requirements.txt (line 2)) (0.25.0)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,267 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /opt/conda/lib/python3.6/site-packages (from datasets==1.11.0->-r /opt/ml/model/code/requirements.txt (line 2)) (6.0.1)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,268 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.6/site-packages (from datasets==1.11.0->-r /opt/ml/model/code/requirements.txt (line 2)) (2022.1.0)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,268 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: dill in /opt/conda/lib/python3.6/site-packages (from datasets==1.11.0->-r /opt/ml/model/code/requirements.txt (line 2)) (0.3.4)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,269 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: multiprocess in /opt/conda/lib/python3.6/site-packages (from datasets==1.11.0->-r /opt/ml/model/code/requirements.txt (line 2)) (0.70.12.2)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,269 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: xxhash in /opt/conda/lib/python3.6/site-packages (from datasets==1.11.0->-r /opt/ml/model/code/requirements.txt (line 2)) (3.0.0)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,277 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: torch>=1.3 in /opt/conda/lib/python3.6/site-packages (from pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (1.7.1)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,278 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: tensorboard>=1.14 in /opt/conda/lib/python3.6/site-packages (from pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (2.10.0)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,278 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: PyYAML>=5.1 in /opt/conda/lib/python3.6/site-packages (from pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (5.4.1)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,279 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: future>=0.17.1 in /opt/conda/lib/python3.6/site-packages (from pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (0.18.2)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,342 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (2.8)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,343 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (3.0.4)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,343 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (1.25.11)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,344 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (2021.5.30)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:53,991 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: editdistance in /opt/conda/lib/python3.6/site-packages (from -r /opt/ml/model/code/requirements.txt (line 6)) (0.6.0)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,150 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (4.62.3)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,152 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (4.8.3)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,152 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: huggingface-hub==0.0.8 in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (0.0.8)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,153 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (2.22.0)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,153 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (0.10.3)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,154 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (2022.7.25)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,154 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (3.4.1)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,155 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (20.4)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,155 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: sacremoses in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (0.0.53)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,156 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,157 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (0.8)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,266 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from datasets==1.11.0->-r /opt/ml/model/code/requirements.txt (line 2)) (0.25.0)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,267 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /opt/conda/lib/python3.6/site-packages (from datasets==1.11.0->-r /opt/ml/model/code/requirements.txt (line 2)) (6.0.1)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,268 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.6/site-packages (from datasets==1.11.0->-r /opt/ml/model/code/requirements.txt (line 2)) (2022.1.0)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,268 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: dill in /opt/conda/lib/python3.6/site-packages (from datasets==1.11.0->-r /opt/ml/model/code/requirements.txt (line 2)) (0.3.4)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,269 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: multiprocess in /opt/conda/lib/python3.6/site-packages (from datasets==1.11.0->-r /opt/ml/model/code/requirements.txt (line 2)) (0.70.12.2)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,269 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: xxhash in /opt/conda/lib/python3.6/site-packages (from datasets==1.11.0->-r /opt/ml/model/code/requirements.txt (line 2)) (3.0.0)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,277 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: torch>=1.3 in /opt/conda/lib/python3.6/site-packages (from pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (1.7.1)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,278 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: tensorboard>=1.14 in /opt/conda/lib/python3.6/site-packages (from pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (2.10.0)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,278 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: PyYAML>=5.1 in /opt/conda/lib/python3.6/site-packages (from pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (5.4.1)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,279 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: future>=0.17.1 in /opt/conda/lib/python3.6/site-packages (from pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (0.18.2)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,342 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (2.8)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,343 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (3.0.4)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,343 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (1.25.11)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,344 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (2021.5.30)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,357 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (0.6.1)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,358 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (1.8.1)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,358 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (2.0.3)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,359 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (0.4.6)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,360 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (1.47.0)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,360 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (49.6.0.post20210108)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,361 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (0.37.0)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,362 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (3.19.4)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,363 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (1.2.0)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,364 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (2.10.0)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,365 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (3.3.7)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,393 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from torch>=1.3->pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (4.0.0)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,435 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (3.6.0)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,442 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (3.0.6)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,443 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from packaging->transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,450 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets==1.11.0->-r /opt/ml/model/code/requirements.txt (line 2)) (2.8.2)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,451 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets==1.11.0->-r /opt/ml/model/code/requirements.txt (line 2)) (2021.3)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,458 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (8.0.4)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,459 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (1.0.1)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,485 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (4.2.4)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,487 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (4.7.2)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,488 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (0.2.8)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,494 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (1.3.1)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,583 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (0.4.8)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,590 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (3.2.0)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,357 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (0.6.1)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,358 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (1.8.1)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,358 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (2.0.3)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,359 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (0.4.6)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,360 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (1.47.0)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,360 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (49.6.0.post20210108)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,361 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (0.37.0)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,362 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (3.19.4)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,363 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (1.2.0)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,364 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (2.10.0)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,365 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.6/site-packages (from tensorboard>=1.14->pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (3.3.7)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,393 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from torch>=1.3->pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (4.0.0)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,435 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (3.6.0)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,442 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (3.0.6)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,443 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from packaging->transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,450 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets==1.11.0->-r /opt/ml/model/code/requirements.txt (line 2)) (2.8.2)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,451 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets==1.11.0->-r /opt/ml/model/code/requirements.txt (line 2)) (2021.3)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,458 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (8.0.4)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,459 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.6.0->-r /opt/ml/model/code/requirements.txt (line 1)) (1.0.1)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,485 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (4.2.4)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,487 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (4.7.2)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,488 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (0.2.8)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,494 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (1.3.1)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,583 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (0.4.8)\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,590 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch_lightning==0.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (3.2.0)\u001b[0m\n",
      "\u001b[34m2022-08-12 07:04:54,802 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m2022-08-12 07:04:54,802 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[32m2022-08-12T07:04:53.085:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=SINGLE_RECORD\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:02,208 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - \u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:02,208 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - <<<run train!!\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:02,208 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - all checkpoints:  ['/opt/ml/model/cktepoch=1.ckpt']\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:02,209 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   0%|          | 0.00/1.20k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:02,254 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading: 100%|██████████| 1.20k/1.20k [00:00<00:00, 976kB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:02,255 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - \u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:02,355 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   0%|          | 0.00/892M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:02,455 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   1%|          | 6.50M/892M [00:00<00:13, 65.0MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:02,555 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   2%|▏         | 13.5M/892M [00:00<00:12, 67.8MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:02,655 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   2%|▏         | 20.6M/892M [00:00<00:12, 69.1MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:02,208 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - \u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:02,208 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - <<<run train!!\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:02,208 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - all checkpoints:  ['/opt/ml/model/cktepoch=1.ckpt']\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:02,209 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   0%|          | 0.00/1.20k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:02,254 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading: 100%|██████████| 1.20k/1.20k [00:00<00:00, 976kB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:02,255 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - \u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:02,355 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   0%|          | 0.00/892M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:02,455 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   1%|          | 6.50M/892M [00:00<00:13, 65.0MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:02,555 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   2%|▏         | 13.5M/892M [00:00<00:12, 67.8MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:02,655 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   2%|▏         | 20.6M/892M [00:00<00:12, 69.1MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:02,755 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   3%|▎         | 27.7M/892M [00:00<00:12, 70.1MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:02,855 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   4%|▍         | 34.8M/892M [00:00<00:12, 70.6MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:02,955 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   5%|▍         | 42.0M/892M [00:00<00:12, 70.8MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:03,055 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   6%|▌         | 49.2M/892M [00:00<00:11, 71.3MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:03,155 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   6%|▋         | 56.4M/892M [00:00<00:11, 71.5MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:03,256 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   7%|▋         | 63.5M/892M [00:00<00:11, 71.4MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:03,359 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   8%|▊         | 70.7M/892M [00:01<00:11, 71.3MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:03,459 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   9%|▊         | 77.8M/892M [00:01<00:11, 70.6MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:03,559 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  10%|▉         | 85.0M/892M [00:01<00:11, 70.9MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:03,659 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  10%|█         | 92.1M/892M [00:01<00:11, 71.1MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:02,755 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   3%|▎         | 27.7M/892M [00:00<00:12, 70.1MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:02,855 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   4%|▍         | 34.8M/892M [00:00<00:12, 70.6MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:02,955 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   5%|▍         | 42.0M/892M [00:00<00:12, 70.8MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:03,055 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   6%|▌         | 49.2M/892M [00:00<00:11, 71.3MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:03,155 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   6%|▋         | 56.4M/892M [00:00<00:11, 71.5MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:03,256 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   7%|▋         | 63.5M/892M [00:00<00:11, 71.4MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:03,359 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   8%|▊         | 70.7M/892M [00:01<00:11, 71.3MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:03,459 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   9%|▊         | 77.8M/892M [00:01<00:11, 70.6MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:03,559 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  10%|▉         | 85.0M/892M [00:01<00:11, 70.9MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:03,659 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  10%|█         | 92.1M/892M [00:01<00:11, 71.1MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:03,759 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  11%|█         | 99.3M/892M [00:01<00:11, 71.3MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:03,859 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  12%|█▏        | 106M/892M [00:01<00:11, 71.4MB/s] \u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:03,759 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  11%|█         | 99.3M/892M [00:01<00:11, 71.3MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:03,859 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  12%|█▏        | 106M/892M [00:01<00:11, 71.4MB/s] \u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:03,959 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  13%|█▎        | 114M/892M [00:01<00:10, 71.3MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:04,059 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  14%|█▎        | 121M/892M [00:01<00:10, 71.3MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:04,159 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  14%|█▍        | 128M/892M [00:01<00:10, 71.3MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:04,259 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  15%|█▌        | 135M/892M [00:01<00:10, 71.3MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:04,362 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  16%|█▌        | 142M/892M [00:02<00:10, 71.3MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:04,462 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  17%|█▋        | 149M/892M [00:02<00:10, 70.8MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:04,567 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  18%|█▊        | 156M/892M [00:02<00:10, 71.2MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:04,668 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  18%|█▊        | 164M/892M [00:02<00:10, 70.0MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:03,959 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  13%|█▎        | 114M/892M [00:01<00:10, 71.3MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:04,059 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  14%|█▎        | 121M/892M [00:01<00:10, 71.3MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:04,159 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  14%|█▍        | 128M/892M [00:01<00:10, 71.3MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:04,259 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  15%|█▌        | 135M/892M [00:01<00:10, 71.3MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:04,362 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  16%|█▌        | 142M/892M [00:02<00:10, 71.3MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:04,462 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  17%|█▋        | 149M/892M [00:02<00:10, 70.8MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:04,567 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  18%|█▊        | 156M/892M [00:02<00:10, 71.2MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:04,668 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  18%|█▊        | 164M/892M [00:02<00:10, 70.0MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:04,769 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  19%|█▉        | 171M/892M [00:02<00:10, 69.8MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:04,869 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  20%|█▉        | 178M/892M [00:02<00:10, 70.1MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:04,969 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  21%|██        | 185M/892M [00:02<00:09, 70.7MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:05,069 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  22%|██▏       | 192M/892M [00:02<00:09, 70.6MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:04,769 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  19%|█▉        | 171M/892M [00:02<00:10, 69.8MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:04,869 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  20%|█▉        | 178M/892M [00:02<00:10, 70.1MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:04,969 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  21%|██        | 185M/892M [00:02<00:09, 70.7MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:05,069 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  22%|██▏       | 192M/892M [00:02<00:09, 70.6MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:05,169 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  22%|██▏       | 199M/892M [00:02<00:09, 70.6MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:05,269 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  23%|██▎       | 206M/892M [00:02<00:09, 70.6MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:05,371 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  24%|██▍       | 213M/892M [00:03<00:09, 70.9MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:05,471 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  25%|██▍       | 220M/892M [00:03<00:09, 70.5MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:05,571 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  26%|██▌       | 228M/892M [00:03<00:09, 70.9MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:05,671 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  26%|██▋       | 235M/892M [00:03<00:09, 71.1MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:05,169 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  22%|██▏       | 199M/892M [00:02<00:09, 70.6MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:05,269 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  23%|██▎       | 206M/892M [00:02<00:09, 70.6MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:05,371 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  24%|██▍       | 213M/892M [00:03<00:09, 70.9MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:05,471 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  25%|██▍       | 220M/892M [00:03<00:09, 70.5MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:05,571 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  26%|██▌       | 228M/892M [00:03<00:09, 70.9MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:05,671 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  26%|██▋       | 235M/892M [00:03<00:09, 71.1MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:05,772 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  27%|██▋       | 242M/892M [00:03<00:09, 71.4MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:05,872 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  28%|██▊       | 249M/892M [00:03<00:09, 71.2MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:05,972 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  29%|██▊       | 256M/892M [00:03<00:08, 71.3MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:06,073 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  30%|██▉       | 263M/892M [00:03<00:08, 71.6MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:06,176 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  30%|███       | 271M/892M [00:03<00:08, 71.3MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:06,276 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  31%|███       | 278M/892M [00:03<00:08, 70.8MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:06,376 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  32%|███▏      | 285M/892M [00:04<00:08, 70.8MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:06,476 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  33%|███▎      | 292M/892M [00:04<00:08, 71.1MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:06,576 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  34%|███▎      | 299M/892M [00:04<00:08, 71.4MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:06,676 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  34%|███▍      | 306M/892M [00:04<00:08, 71.7MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:05,772 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  27%|██▋       | 242M/892M [00:03<00:09, 71.4MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:05,872 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  28%|██▊       | 249M/892M [00:03<00:09, 71.2MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:05,972 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  29%|██▊       | 256M/892M [00:03<00:08, 71.3MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:06,073 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  30%|██▉       | 263M/892M [00:03<00:08, 71.6MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:06,176 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  30%|███       | 271M/892M [00:03<00:08, 71.3MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:06,276 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  31%|███       | 278M/892M [00:03<00:08, 70.8MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:06,376 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  32%|███▏      | 285M/892M [00:04<00:08, 70.8MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:06,476 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  33%|███▎      | 292M/892M [00:04<00:08, 71.1MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:06,576 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  34%|███▎      | 299M/892M [00:04<00:08, 71.4MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:06,676 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  34%|███▍      | 306M/892M [00:04<00:08, 71.7MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:06,777 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  35%|███▌      | 314M/892M [00:04<00:08, 71.7MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:06,877 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  36%|███▌      | 321M/892M [00:04<00:07, 71.5MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:06,977 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  37%|███▋      | 328M/892M [00:04<00:07, 71.7MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:07,077 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  38%|███▊      | 335M/892M [00:04<00:07, 71.8MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:07,178 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  38%|███▊      | 342M/892M [00:04<00:07, 71.9MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:07,279 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  39%|███▉      | 350M/892M [00:04<00:07, 71.7MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:07,381 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  40%|████      | 357M/892M [00:05<00:07, 71.5MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:07,481 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  41%|████      | 364M/892M [00:05<00:07, 70.9MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:07,581 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  42%|████▏     | 371M/892M [00:05<00:07, 71.3MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:07,681 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  42%|████▏     | 378M/892M [00:05<00:07, 71.6MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:06,777 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  35%|███▌      | 314M/892M [00:04<00:08, 71.7MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:06,877 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  36%|███▌      | 321M/892M [00:04<00:07, 71.5MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:06,977 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  37%|███▋      | 328M/892M [00:04<00:07, 71.7MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:07,077 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  38%|███▊      | 335M/892M [00:04<00:07, 71.8MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:07,178 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  38%|███▊      | 342M/892M [00:04<00:07, 71.9MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:07,279 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  39%|███▉      | 350M/892M [00:04<00:07, 71.7MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:07,381 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  40%|████      | 357M/892M [00:05<00:07, 71.5MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:07,481 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  41%|████      | 364M/892M [00:05<00:07, 70.9MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:07,581 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  42%|████▏     | 371M/892M [00:05<00:07, 71.3MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:07,681 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  42%|████▏     | 378M/892M [00:05<00:07, 71.6MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:07,782 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  43%|████▎     | 386M/892M [00:05<00:07, 71.6MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:07,882 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  44%|████▍     | 393M/892M [00:05<00:06, 71.6MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:07,982 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  45%|████▍     | 400M/892M [00:05<00:06, 71.6MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:08,082 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  46%|████▌     | 407M/892M [00:05<00:06, 71.7MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:08,184 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  46%|████▋     | 414M/892M [00:05<00:06, 71.6MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:08,286 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  47%|████▋     | 421M/892M [00:05<00:06, 71.3MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:08,388 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  48%|████▊     | 429M/892M [00:06<00:06, 70.9MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:08,488 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  49%|████▉     | 436M/892M [00:06<00:06, 70.4MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:08,588 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  50%|████▉     | 443M/892M [00:06<00:06, 70.5MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:08,688 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  50%|█████     | 450M/892M [00:06<00:06, 70.5MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:07,782 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  43%|████▎     | 386M/892M [00:05<00:07, 71.6MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:07,882 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  44%|████▍     | 393M/892M [00:05<00:06, 71.6MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:07,982 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  45%|████▍     | 400M/892M [00:05<00:06, 71.6MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:08,082 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  46%|████▌     | 407M/892M [00:05<00:06, 71.7MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:08,184 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  46%|████▋     | 414M/892M [00:05<00:06, 71.6MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:08,286 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  47%|████▋     | 421M/892M [00:05<00:06, 71.3MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:08,388 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  48%|████▊     | 429M/892M [00:06<00:06, 70.9MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:08,488 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  49%|████▉     | 436M/892M [00:06<00:06, 70.4MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:08,588 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  50%|████▉     | 443M/892M [00:06<00:06, 70.5MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:08,688 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  50%|█████     | 450M/892M [00:06<00:06, 70.5MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:08,788 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  51%|█████     | 457M/892M [00:06<00:06, 70.8MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:08,888 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  52%|█████▏    | 464M/892M [00:06<00:06, 70.8MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:08,988 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  53%|█████▎    | 471M/892M [00:06<00:05, 71.2MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:09,092 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  54%|█████▎    | 478M/892M [00:06<00:05, 71.3MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:09,192 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  54%|█████▍    | 486M/892M [00:06<00:05, 70.5MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:09,293 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  55%|█████▌    | 493M/892M [00:06<00:05, 70.7MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:09,394 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  56%|█████▌    | 500M/892M [00:07<00:05, 70.6MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:09,494 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  57%|█████▋    | 507M/892M [00:07<00:05, 70.3MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:09,606 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  58%|█████▊    | 514M/892M [00:07<00:05, 70.8MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:09,706 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  58%|█████▊    | 521M/892M [00:07<00:05, 68.4MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:08,788 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  51%|█████     | 457M/892M [00:06<00:06, 70.8MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:08,888 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  52%|█████▏    | 464M/892M [00:06<00:06, 70.8MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:08,988 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  53%|█████▎    | 471M/892M [00:06<00:05, 71.2MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:09,092 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  54%|█████▎    | 478M/892M [00:06<00:05, 71.3MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:09,192 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  54%|█████▍    | 486M/892M [00:06<00:05, 70.5MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:09,293 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  55%|█████▌    | 493M/892M [00:06<00:05, 70.7MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:09,394 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  56%|█████▌    | 500M/892M [00:07<00:05, 70.6MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:09,494 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  57%|█████▋    | 507M/892M [00:07<00:05, 70.3MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:09,606 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  58%|█████▊    | 514M/892M [00:07<00:05, 70.8MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:09,706 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  58%|█████▊    | 521M/892M [00:07<00:05, 68.4MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:09,806 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  59%|█████▉    | 528M/892M [00:07<00:05, 69.3MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:09,906 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  60%|██████    | 535M/892M [00:07<00:05, 69.8MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:10,006 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  61%|██████    | 542M/892M [00:07<00:04, 70.3MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:10,106 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  62%|██████▏   | 550M/892M [00:07<00:04, 70.7MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:10,206 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  62%|██████▏   | 557M/892M [00:07<00:04, 71.0MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:10,306 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  63%|██████▎   | 564M/892M [00:07<00:04, 71.1MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:10,408 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  64%|██████▍   | 571M/892M [00:08<00:04, 71.2MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:10,508 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  65%|██████▍   | 578M/892M [00:08<00:04, 70.7MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:10,608 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  66%|██████▌   | 585M/892M [00:08<00:04, 71.1MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:10,709 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  66%|██████▋   | 593M/892M [00:08<00:04, 71.2MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:09,806 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  59%|█████▉    | 528M/892M [00:07<00:05, 69.3MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:09,906 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  60%|██████    | 535M/892M [00:07<00:05, 69.8MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:10,006 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  61%|██████    | 542M/892M [00:07<00:04, 70.3MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:10,106 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  62%|██████▏   | 550M/892M [00:07<00:04, 70.7MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:10,206 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  62%|██████▏   | 557M/892M [00:07<00:04, 71.0MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:10,306 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  63%|██████▎   | 564M/892M [00:07<00:04, 71.1MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:10,408 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  64%|██████▍   | 571M/892M [00:08<00:04, 71.2MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:10,508 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  65%|██████▍   | 578M/892M [00:08<00:04, 70.7MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:10,608 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  66%|██████▌   | 585M/892M [00:08<00:04, 71.1MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:10,709 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  66%|██████▋   | 593M/892M [00:08<00:04, 71.2MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:10,809 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  67%|██████▋   | 600M/892M [00:08<00:04, 71.1MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:10,909 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  68%|██████▊   | 607M/892M [00:08<00:04, 71.0MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:11,009 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  69%|██████▉   | 614M/892M [00:08<00:03, 71.2MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:11,109 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  70%|██████▉   | 621M/892M [00:08<00:03, 71.2MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:11,209 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  70%|███████   | 628M/892M [00:08<00:03, 71.2MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:11,310 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  71%|███████   | 635M/892M [00:08<00:03, 71.2MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:11,410 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  72%|███████▏  | 642M/892M [00:09<00:03, 71.1MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:11,510 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  73%|███████▎  | 650M/892M [00:09<00:03, 71.1MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:11,610 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  74%|███████▎  | 657M/892M [00:09<00:03, 71.2MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:11,712 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  74%|███████▍  | 664M/892M [00:09<00:03, 71.3MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:10,809 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  67%|██████▋   | 600M/892M [00:08<00:04, 71.1MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:10,909 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  68%|██████▊   | 607M/892M [00:08<00:04, 71.0MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:11,009 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  69%|██████▉   | 614M/892M [00:08<00:03, 71.2MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:11,109 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  70%|██████▉   | 621M/892M [00:08<00:03, 71.2MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:11,209 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  70%|███████   | 628M/892M [00:08<00:03, 71.2MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:11,310 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  71%|███████   | 635M/892M [00:08<00:03, 71.2MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:11,410 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  72%|███████▏  | 642M/892M [00:09<00:03, 71.1MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:11,510 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  73%|███████▎  | 650M/892M [00:09<00:03, 71.1MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:11,610 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  74%|███████▎  | 657M/892M [00:09<00:03, 71.2MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:11,712 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  74%|███████▍  | 664M/892M [00:09<00:03, 71.3MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:12,820 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  83%|████████▎ | 742M/892M [00:10<00:02, 71.3MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:12,920 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  84%|████████▍ | 750M/892M [00:10<00:01, 71.3MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:13,020 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  85%|████████▍ | 757M/892M [00:10<00:01, 71.4MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:13,222 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  86%|████████▌ | 764M/892M [00:10<00:01, 71.6MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:13,345 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  86%|████████▋ | 771M/892M [00:10<00:02, 54.8MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:13,445 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  87%|████████▋ | 777M/892M [00:11<00:02, 53.3MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:13,545 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  88%|████████▊ | 784M/892M [00:11<00:01, 56.9MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:13,645 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  89%|████████▊ | 791M/892M [00:11<00:01, 60.4MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:13,745 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  89%|████████▉ | 798M/892M [00:11<00:01, 63.2MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:12,820 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  83%|████████▎ | 742M/892M [00:10<00:02, 71.3MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:12,920 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  84%|████████▍ | 750M/892M [00:10<00:01, 71.3MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:13,020 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  85%|████████▍ | 757M/892M [00:10<00:01, 71.4MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:13,222 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  86%|████████▌ | 764M/892M [00:10<00:01, 71.6MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:13,345 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  86%|████████▋ | 771M/892M [00:10<00:02, 54.8MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:13,445 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  87%|████████▋ | 777M/892M [00:11<00:02, 53.3MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:13,545 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  88%|████████▊ | 784M/892M [00:11<00:01, 56.9MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:13,645 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  89%|████████▊ | 791M/892M [00:11<00:01, 60.4MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:13,745 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  89%|████████▉ | 798M/892M [00:11<00:01, 63.2MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:13,845 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  90%|█████████ | 805M/892M [00:11<00:01, 65.5MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:13,945 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  91%|█████████ | 812M/892M [00:11<00:01, 67.3MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:14,045 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  92%|█████████▏| 819M/892M [00:11<00:01, 68.4MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:14,145 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  93%|█████████▎| 827M/892M [00:11<00:00, 69.0MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:14,245 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  93%|█████████▎| 834M/892M [00:11<00:00, 69.1MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:14,345 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  94%|█████████▍| 841M/892M [00:11<00:00, 69.7MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:14,447 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  95%|█████████▌| 848M/892M [00:12<00:00, 70.3MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:13,845 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  90%|█████████ | 805M/892M [00:11<00:01, 65.5MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:13,945 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  91%|█████████ | 812M/892M [00:11<00:01, 67.3MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:14,045 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  92%|█████████▏| 819M/892M [00:11<00:01, 68.4MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:14,145 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  93%|█████████▎| 827M/892M [00:11<00:00, 69.0MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:14,245 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  93%|█████████▎| 834M/892M [00:11<00:00, 69.1MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:14,345 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  94%|█████████▍| 841M/892M [00:11<00:00, 69.7MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:14,447 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  95%|█████████▌| 848M/892M [00:12<00:00, 70.3MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:14,547 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  96%|█████████▌| 855M/892M [00:12<00:00, 70.0MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:14,656 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  97%|█████████▋| 862M/892M [00:12<00:00, 70.4MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:14,756 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  97%|█████████▋| 869M/892M [00:12<00:00, 68.8MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:14,547 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  96%|█████████▌| 855M/892M [00:12<00:00, 70.0MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:14,656 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  97%|█████████▋| 862M/892M [00:12<00:00, 70.4MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:14,756 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  97%|█████████▋| 869M/892M [00:12<00:00, 68.8MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:14,856 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  98%|█████████▊| 876M/892M [00:12<00:00, 69.5MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:14,956 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  99%|█████████▉| 883M/892M [00:12<00:00, 70.0MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:14,972 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading: 100%|█████████▉| 890M/892M [00:12<00:00, 70.4MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:14,856 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  98%|█████████▊| 876M/892M [00:12<00:00, 69.5MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:14,956 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  99%|█████████▉| 883M/892M [00:12<00:00, 70.0MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:14,972 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading: 100%|█████████▉| 890M/892M [00:12<00:00, 70.4MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:18,218 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading: 100%|██████████| 892M/892M [00:12<00:00, 70.1MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:18,219 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - \u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:18,232 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   0%|          | 0.00/792k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:18,458 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading: 100%|██████████| 792k/792k [00:00<00:00, 62.0MB/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:18,458 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - \u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:18,521 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   0%|          | 0.00/1.39M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:18,218 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading: 100%|██████████| 892M/892M [00:12<00:00, 70.1MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:18,219 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - \u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:18,232 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   0%|          | 0.00/792k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:18,458 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading: 100%|██████████| 792k/792k [00:00<00:00, 62.0MB/s]\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:18,458 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - \u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:18,521 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   0%|          | 0.00/1.39M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:19,015 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 26111\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:19,015 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 26111\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:19,015 [INFO ] W-9000-model_1 TS_METRICS - W-9000-model_1.ms:26415|#Level:Host|#hostname:501da5bbdb65,timestamp:1660287919\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:19,016 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:85|#Level:Host|#hostname:501da5bbdb65,timestamp:null\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:19,015 [INFO ] W-9000-model_1 TS_METRICS - W-9000-model_1.ms:26415|#Level:Host|#hostname:501da5bbdb65,timestamp:1660287919\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:19,016 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:85|#Level:Host|#hostname:501da5bbdb65,timestamp:null\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:22,179 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3163\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:22,179 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:3161.39|#ModelName:model,Level:Model|#hostname:501da5bbdb65,requestID:c55339aa-92a7-427b-a31d-4dc1f9f32d90,timestamp:1660287922\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:22,180 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:55458 \"POST /invocations HTTP/1.1\" 200 29005\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:22,180 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:501da5bbdb65,timestamp:null\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:22,180 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:25829|#Level:Host|#hostname:501da5bbdb65,timestamp:null\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:22,180 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:501da5bbdb65,timestamp:null\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:22,179 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3163\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:22,179 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:3161.39|#ModelName:model,Level:Model|#hostname:501da5bbdb65,requestID:c55339aa-92a7-427b-a31d-4dc1f9f32d90,timestamp:1660287922\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:22,180 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:55458 \"POST /invocations HTTP/1.1\" 200 29005\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:22,180 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:501da5bbdb65,timestamp:null\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:22,180 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:25829|#Level:Host|#hostname:501da5bbdb65,timestamp:null\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:22,180 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:501da5bbdb65,timestamp:null\u001b[0m\n",
      "\n",
      "\u001b[34m2022-08-12 07:05:31,059 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1939\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:31,059 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1937.81|#ModelName:model,Level:Model|#hostname:501da5bbdb65,requestID:d70ecdcc-3926-4c56-9ee2-5155989f9745,timestamp:1660287931\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:31,059 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:55458 \"POST /invocations HTTP/1.1\" 200 1939\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:31,059 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1939\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:31,059 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1937.81|#ModelName:model,Level:Model|#hostname:501da5bbdb65,requestID:d70ecdcc-3926-4c56-9ee2-5155989f9745,timestamp:1660287931\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:31,059 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:55458 \"POST /invocations HTTP/1.1\" 200 1939\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:31,059 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:501da5bbdb65,timestamp:null\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:31,059 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:501da5bbdb65,timestamp:null\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:31,060 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:501da5bbdb65,timestamp:null\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:31,059 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:501da5bbdb65,timestamp:null\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:31,059 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:501da5bbdb65,timestamp:null\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:31,060 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:501da5bbdb65,timestamp:null\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:34,346 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3281\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:34,346 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:3281.62|#ModelName:model,Level:Model|#hostname:501da5bbdb65,requestID:1c84a789-3b64-4f42-a5e3-17ff44ce5c86,timestamp:1660287934\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:34,347 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:55458 \"POST /invocations HTTP/1.1\" 200 3284\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:34,347 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:501da5bbdb65,timestamp:null\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:34,347 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:501da5bbdb65,timestamp:null\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:34,347 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:501da5bbdb65,timestamp:null\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:34,346 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3281\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:34,346 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:3281.62|#ModelName:model,Level:Model|#hostname:501da5bbdb65,requestID:1c84a789-3b64-4f42-a5e3-17ff44ce5c86,timestamp:1660287934\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:34,347 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:55458 \"POST /invocations HTTP/1.1\" 200 3284\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:34,347 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:501da5bbdb65,timestamp:null\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:34,347 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:501da5bbdb65,timestamp:null\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:34,347 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:501da5bbdb65,timestamp:null\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:35,907 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1555\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:35,907 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1555\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:35,907 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1554.2|#ModelName:model,Level:Model|#hostname:501da5bbdb65,requestID:ca9e3b01-c5a1-4db5-8c6e-ade2f28879ad,timestamp:1660287935\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:35,908 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:55458 \"POST /invocations HTTP/1.1\" 200 1556\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:35,908 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:501da5bbdb65,timestamp:null\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:35,908 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:501da5bbdb65,timestamp:null\u001b[0m\n",
      "\u001b[34m2022-08-12 07:05:35,908 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:501da5bbdb65,timestamp:null\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:35,907 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:1554.2|#ModelName:model,Level:Model|#hostname:501da5bbdb65,requestID:ca9e3b01-c5a1-4db5-8c6e-ade2f28879ad,timestamp:1660287935\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:35,908 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:55458 \"POST /invocations HTTP/1.1\" 200 1556\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:35,908 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:501da5bbdb65,timestamp:null\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:35,908 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:501da5bbdb65,timestamp:null\u001b[0m\n",
      "\u001b[35m2022-08-12 07:05:35,908 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:501da5bbdb65,timestamp:null\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# starts batch transform job and uses S3 data as input\n",
    "batch_job.transform(\n",
    "    data=input_s3_path,\n",
    "    content_type='application/json',    \n",
    "    split_type='Line'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d143ee02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.s3 import S3Downloader\n",
    "from ast import literal_eval\n",
    "# creating s3 uri for result file -> input file + .out\n",
    "output_file = f\"{dataset_jsonl_file}.out\"\n",
    "output_path = s3_path_join(\"s3://sagemaker-us-east-1-726335585155/batch_transform/output\",output_file)\n",
    "\n",
    "local_path = \"output\"  # Where to save the output locally\n",
    "\n",
    "S3Downloader.download(output_path,local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b157ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting jsonlines\n",
      "  Downloading jsonlines-3.1.0-py3-none-any.whl (8.6 kB)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from jsonlines) (21.2.0)\n",
      "Installing collected packages: jsonlines\n",
      "Successfully installed jsonlines-3.1.0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p38/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "024dc569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"result\": \"(extra large, size); (very large, size); (very large, size); (very large, size); (very large, size); (very large, size); (very large, size); (very large, size)\"}{\"result\": \"(liked the look of this dress, feelings); (wasn\\u2019t cute on me, feelings); (liked the look of this dress, feelings); (liked the look of this dress, feelings); (liked the look of this dress, feelings); (liked the look of this dress, feelings); (liked the look of this dress, feelings); (liked the look of this dress, feelings); (liked the look of this dress, feelings); (liked the look of this dress, feelings); (liked the look of this dress, feelings)\"}{\"result\": \"(Beautifully made, quality); (Can't believe it was made in China, China, China)\"}{\"result\": \"(a vow renewal in Vegas, scene); (great quality, quality); (lined, fabric); (light weight, fabric); (very happy with this purchase, feelings); (highly recommend, purchase_behavior)\"}{\"result\": \"(Not true to size, size); (Not true to size, size)\"}\n"
     ]
    }
   ],
   "source": [
    "# Inspect the output\n",
    "\n",
    "import os\n",
    "import jsonlines\n",
    "import json\n",
    "from ast import literal_eval\n",
    "\n",
    "output_file = f\"{dataset_jsonl_file}.out\"\n",
    "\n",
    "batch_transform_result = []\n",
    "\n",
    "path = os.path.join(local_path, output_file)\n",
    "with open(path, \"r\") as f:\n",
    "    for line in f:\n",
    "        print (line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d3ec4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
