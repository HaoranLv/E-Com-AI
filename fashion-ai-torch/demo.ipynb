{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5772e352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "import pandas as pd\n",
    "import ast\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from shutil import copyfile\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cc52a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: openpyxl in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (3.0.9)\n",
      "Requirement already satisfied: et-xmlfile in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from openpyxl) (1.0.1)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p38/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "292e6819",
   "metadata": {},
   "outputs": [],
   "source": [
    "#download data\n",
    "#!mkdir data_0731\n",
    "#!aws s3 cp s3://jackie-test/bumingjueli/data0731/ ./data_0731/ --recursive\n",
    "#!unzip ./data_0731/Sports.zip -d ./data_0731"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e72ab7",
   "metadata": {},
   "source": [
    "# data prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03822cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<< predict for keys:  Size Fit,Fit Type,Details,Sleeve Length,Color,Quantity,Sheer,Lining,Fabric,Warm Lined,Composition,Body,Care Instructions,Pattern Type,Pockets,Length,Type,Hem Shaped,Material,Belt,Neckline,Sleeve Type,Arabian Clothing,Style\n",
      "path of ./train_sample/Women-Sweatshirts is build\n",
      "train size (40, 33), test size(10, 33)\n"
     ]
    }
   ],
   "source": [
    "#filter not 13 list\n",
    "def get_feature_len(x):\n",
    "    t = json.loads(x)\n",
    "    return len(t)\n",
    "\n",
    "def get_key_list(x):\n",
    "    # get key dictionary\n",
    "    t = json.loads(x)\n",
    "    res = [i for i in list(t.values())]\n",
    "    res = [list(i.keys())[0] for i in res]\n",
    "    return res\n",
    "\n",
    "def get_keys(df):\n",
    "    lst = list(df['feature_dict'])\n",
    "    myList = [x for j in lst for x in j]\n",
    "    res = list(set(myList))\n",
    "    #res_str = ','.join(res)\n",
    "    return res\n",
    "    \n",
    "def map_feature(x,leng):\n",
    "    t = json.loads(x)\n",
    "    for i in range(leng):\n",
    "        if str(i) in t.keys():\n",
    "            continue\n",
    "        else:\n",
    "            t[str(i)] = ''\n",
    "    return t\n",
    "\n",
    "def get_res(x):\n",
    "    try:\n",
    "        a = ast.literal_eval(str(x))\n",
    "        return a\n",
    "    except:\n",
    "        return {'res':'others'}\n",
    "        \n",
    "#map back labels\n",
    "def get_label_txt():\n",
    "    with open('./data/label.txt') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    keys =  [i.split('\\t')[0] for i in lines]\n",
    "    keys_update = [str(int(i)-1) for i in keys]\n",
    "    res = [i.split('\\t')[1][:-1] for i in lines ]\n",
    "    dict_res = dict(zip(keys_update, res))\n",
    "    return dict_res\n",
    "\n",
    "def get_key_value(x,i):\n",
    "#x = df['data'][59335]\n",
    "\n",
    "    t = json.loads(x)\n",
    "\n",
    "    res = [i for i in list(t.values())]\n",
    "    keys = [list(i.keys())[0] for i in res]\n",
    "    values = [list(i.values())[0] for i in res]\n",
    "    dict_res = dict(zip(keys, values))\n",
    "    if i in dict_res.keys():\n",
    "        return dict_res[i]\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "\n",
    "def test_path(x,category):\n",
    "    root_path = os.path.join('/home/ec2-user/SageMaker/bumingjueli/img-cls/data/data_0731',category)\n",
    "    img_name = os.path.join(root_path,str(x)+'.png')\n",
    "    #print ('img_name',img_name)\n",
    "    if os.path.exists(img_name):  \n",
    "        # for local training\n",
    "        #return img_name\n",
    "        # for sagemaker only\n",
    "        img = Image.open(img_name)\n",
    "        if len(img.getbands())==3:\n",
    "            return os.path.join('/opt/ml/input/data/training',str(x)+'.png')\n",
    "        else:\n",
    "            return 'none'\n",
    "    else:\n",
    "        return 'none'\n",
    "\n",
    "def self_mkdir(folder):\n",
    "    isExists = os.path.exists(folder)\n",
    "    if not isExists:\n",
    "        os.makedirs(folder)\n",
    "        print('path of %s is build' % (folder))\n",
    "\n",
    "def copy_files(df,category,output_dir):\n",
    "    for i in df['md5_url']:\n",
    "        copyfile(os.path.join('./data_0731',category,i+'.png'),os.path.join(output_dir,i+'.png'))\n",
    "        \n",
    "        \n",
    "def get_data(path,category,output_dir):\n",
    "    df = pd.read_excel(path,engine=\"openpyxl\")\n",
    "    df = df[df['creg']==category]\n",
    "    #df['feature_len'] = df['data'].map(lambda x: get_feature_len(x))\n",
    "    #leng = max(df['feature_len'])\n",
    "    df['feature_dict'] = df['data'].map(lambda x: get_key_list(x))\n",
    "    res_keys = get_keys(df)\n",
    "    print (\"<<< predict for keys: \", ','.join(res_keys))\n",
    "    \n",
    "    for i in res_keys:\n",
    "        df[i] = df['data'].map(lambda x: get_key_value(x,i))\n",
    "    \n",
    "    #repath\n",
    "    df['image_path'] = df['md5_url'].map(lambda x: test_path(x,category))\n",
    "    df = df[df['image_path']!='none']\n",
    "    \n",
    "    #make dir if not exist\n",
    "    self_mkdir(output_dir)\n",
    "    #save data\n",
    "    df[res_keys].to_csv(os.path.join(output_dir, 'total.csv'),index=False)\n",
    "    \n",
    "    #sample\n",
    "    df = df.head(50)\n",
    "    #copy images\n",
    "    copy_files(df,category,output_dir)\n",
    "    \n",
    "    train, test = train_test_split(df,test_size=0.2,random_state=0)\n",
    "    train.to_csv(os.path.join(output_dir, 'train.csv'))\n",
    "    test.to_csv(os.path.join(output_dir, 'test.csv'))\n",
    "    print (\"train size {}, test size{}\".format(train.shape,test.shape))\n",
    "    \n",
    "    return df\n",
    "\n",
    "category = 'Women-Sweatshirts'\n",
    "output_dir = os.path.join(\"./train_sample\",category)\n",
    "df = get_data('./data_0731/shein_info.xlsx',category=category,output_dir = output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7177fbbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./train_sample/Women-Sweatshirts'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893bc637",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac92ad86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker as sage\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "sess = sage.Session()\n",
    "\n",
    "WORK_DIRECTORY = output_dir\n",
    "\n",
    "# S3 prefix\n",
    "prefix = \"bmjl-train-\"+category\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "data_location = sess.upload_data(WORK_DIRECTORY, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ae03814",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"epoch\":1, # test 1\n",
    "    \"batch_size\":4,\n",
    "    \"num_workers\":8,  \n",
    "    'val_epoch':1,\n",
    "    'save_epoch':1,\n",
    "    'model_name':'resnet'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fc183b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_point = 'train_general_sagemaker.py'\n",
    "source_dir = './code'\n",
    "git_config = None\n",
    "role = get_execution_role()\n",
    "framework_version = '1.7.1'\n",
    "py_version='py36'\n",
    "instance_type='ml.p3.2xlarge'\n",
    "#instance_type='local_gpu'\n",
    "instance_count=1\n",
    "volume_size=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5beedadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(\n",
    "    entry_point = entry_point,\n",
    "    source_dir = source_dir,\n",
    "    git_config = git_config,\n",
    "    role = role,\n",
    "    debugger_hook_config=False,\n",
    "    hyperparameters = hyperparameters,\n",
    "    framework_version = framework_version, \n",
    "    py_version = py_version,\n",
    "    instance_type = instance_type,\n",
    "    instance_count = instance_count,\n",
    "    base_job_name = prefix+hyperparameters['model_name'],\n",
    "    volume_size=volume_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d10ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-16 08:59:35 Starting - Starting the training job."
     ]
    }
   ],
   "source": [
    "response = estimator.fit(data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2701bd",
   "metadata": {},
   "source": [
    "# endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b7f2e9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "s3_model = estimator.model_data \n",
    "#s3_model = \"s3://sagemaker-us-east-1-726335585155/bmjl-train-Bottoms-2022-08-08-10-34-18-585/output/model.tar.gz\"\n",
    "\n",
    "pytorch_model = PyTorchModel(model_data=s3_model, \n",
    "                             role=role,\n",
    "                             entry_point='inference.py', \n",
    "                             source_dir='./code', \n",
    "                             framework_version='1.7.1', \n",
    "                             py_version='py36'\n",
    "                ) # TODO set model_server_workers=1 to avoid torchhub bug\n",
    "\n",
    "predictor = pytorch_model.deploy(instance_type=instance_type, initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "077379f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from boto3.session import Session\n",
    "\n",
    "session = Session()\n",
    "runtime = session.client(\"runtime.sagemaker\")\n",
    "\n",
    "with open('train_data/Women-Bottoms/00017534ec95a66ab3518dc71bbc970d.html.png', \"rb\") as f:\n",
    "    payload = f.read()\n",
    "    payload = bytearray(payload)\n",
    "    \n",
    "response = runtime.invoke_endpoint(\n",
    "    EndpointName='pytorch-inference-2022-08-10-08-15-19-340', ContentType=\"application/x-image\", Body=payload\n",
    ")\n",
    "\n",
    "result = response[\"Body\"].read()\n",
    "# result will be in json format and convert it to ndarray\n",
    "result = json.loads(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c45c29cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'result': {'Warm Lined': 'other',\n",
       "  'Details': 'Zipper',\n",
       "  'Type': 'Pullovers',\n",
       "  'Composition': '100% Polyester',\n",
       "  'Quantity': '1 piece',\n",
       "  'Belt': 'other',\n",
       "  'Fit Type': 'Regular Fit',\n",
       "  'Length': 'Regular',\n",
       "  'Sleeve Length': 'Long Sleeve',\n",
       "  'Pattern Type': 'Plain',\n",
       "  'Color': 'Black',\n",
       "  'Lining': 'other',\n",
       "  'Sleeve Type': 'Drop Shoulder',\n",
       "  'Pockets': 'other',\n",
       "  'Sheer': 'No',\n",
       "  'Size Fit': 'other',\n",
       "  'Neckline': 'Hooded',\n",
       "  'Fabric': 'Slight Stretch',\n",
       "  'Material': 'Polyester',\n",
       "  'Hem Shaped': 'other',\n",
       "  'Care Instructions': 'Machine wash or professional dry clean',\n",
       "  'Body': 'other',\n",
       "  'Arabian Clothing': 'other',\n",
       "  'Style': 'Casual'}}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7a2993",
   "metadata": {},
   "source": [
    "# batch transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "41941b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input S3 path: s3://sagemaker-us-east-1-726335585155/batch_transform\n"
     ]
    }
   ],
   "source": [
    "image_dir = './test'\n",
    "inference_prefix = \"batch_transform\"\n",
    "inference_inputs = sess.upload_data(\n",
    "    path=image_dir, key_prefix=inference_prefix\n",
    ")\n",
    "print(\"Input S3 path: {}\".format(inference_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "778a91ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transformer from PyTorchModel object\n",
    "transformer = pytorch_model.transformer(instance_count=1, instance_type=\"ml.m5.xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4e1d0fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................\n",
      "\u001b[34mRequirement already satisfied: matplotlib in /opt/conda/lib/python3.6/site-packages (from -r /opt/ml/model/code/requirements.txt (line 1)) (3.3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from -r /opt/ml/model/code/requirements.txt (line 2)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow in /opt/conda/lib/python3.6/site-packages (from -r /opt/ml/model/code/requirements.txt (line 3)) (8.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.6/site-packages (from -r /opt/ml/model/code/requirements.txt (line 4)) (0.21.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.6/site-packages (from -r /opt/ml/model/code/requirements.txt (line 5)) (1.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchvision in /opt/conda/lib/python3.6/site-packages (from -r /opt/ml/model/code/requirements.txt (line 6)) (0.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from -r /opt/ml/model/code/requirements.txt (line 7)) (4.62.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->-r /opt/ml/model/code/requirements.txt (line 1)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->-r /opt/ml/model/code/requirements.txt (line 1)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/conda/lib/python3.6/site-packages (from matplotlib->-r /opt/ml/model/code/requirements.txt (line 1)) (3.0.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib->-r /opt/ml/model/code/requirements.txt (line 1)) (0.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=0.17.0 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->-r /opt/ml/model/code/requirements.txt (line 4)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->-r /opt/ml/model/code/requirements.txt (line 4)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from torch->-r /opt/ml/model/code/requirements.txt (line 5)) (0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from torch->-r /opt/ml/model/code/requirements.txt (line 5)) (4.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: matplotlib in /opt/conda/lib/python3.6/site-packages (from -r /opt/ml/model/code/requirements.txt (line 1)) (3.3.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from -r /opt/ml/model/code/requirements.txt (line 2)) (1.19.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pillow in /opt/conda/lib/python3.6/site-packages (from -r /opt/ml/model/code/requirements.txt (line 3)) (8.4.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.6/site-packages (from -r /opt/ml/model/code/requirements.txt (line 4)) (0.21.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: torch in /opt/conda/lib/python3.6/site-packages (from -r /opt/ml/model/code/requirements.txt (line 5)) (1.7.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: torchvision in /opt/conda/lib/python3.6/site-packages (from -r /opt/ml/model/code/requirements.txt (line 6)) (0.8.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from -r /opt/ml/model/code/requirements.txt (line 7)) (4.62.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->-r /opt/ml/model/code/requirements.txt (line 1)) (2.8.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->-r /opt/ml/model/code/requirements.txt (line 1)) (1.3.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/conda/lib/python3.6/site-packages (from matplotlib->-r /opt/ml/model/code/requirements.txt (line 1)) (3.0.6)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib->-r /opt/ml/model/code/requirements.txt (line 1)) (0.11.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: scipy>=0.17.0 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->-r /opt/ml/model/code/requirements.txt (line 4)) (1.3.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->-r /opt/ml/model/code/requirements.txt (line 4)) (1.0.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from torch->-r /opt/ml/model/code/requirements.txt (line 5)) (0.8)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from torch->-r /opt/ml/model/code/requirements.txt (line 5)) (4.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.1->matplotlib->-r /opt/ml/model/code/requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2022-08-10 08:58:58,084 [INFO ] main org.pytorch.serve.ModelServer - \u001b[0m\n",
      "\u001b[34mTorchserve version: 0.3.1\u001b[0m\n",
      "\u001b[34mTS Home: /opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mCurrent directory: /\u001b[0m\n",
      "\u001b[34mTemp directory: /home/model-server/tmp\u001b[0m\n",
      "\u001b[34mNumber of GPUs: 0\u001b[0m\n",
      "\u001b[34mNumber of CPUs: 4\u001b[0m\n",
      "\u001b[34mMax heap size: 2950 M\u001b[0m\n",
      "\u001b[34mPython executable: /opt/conda/bin/python3.6\u001b[0m\n",
      "\u001b[34mConfig file: /etc/sagemaker-ts.properties\u001b[0m\n",
      "\u001b[34mInference address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mManagement address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mMetrics address: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[34mModel Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[34mInitial Models: model.mar\u001b[0m\n",
      "\u001b[34mLog dir: /logs\u001b[0m\n",
      "\u001b[34mMetrics dir: /logs\u001b[0m\n",
      "\u001b[34mNetty threads: 0\u001b[0m\n",
      "\u001b[34mNetty client threads: 0\u001b[0m\n",
      "\u001b[34mDefault workers per model: 4\u001b[0m\n",
      "\u001b[34mBlacklist Regex: N/A\u001b[0m\n",
      "\u001b[34mMaximum Response Size: 6553500\u001b[0m\n",
      "\u001b[34mMaximum Request Size: 6553500\u001b[0m\n",
      "\u001b[34mPrefer direct buffer: false\u001b[0m\n",
      "\u001b[34mAllowed Urls: [file://.*|http(s)?://.*]\u001b[0m\n",
      "\u001b[34mCustom python dependency for model allowed: false\u001b[0m\n",
      "\u001b[34mMetrics report format: prometheus\u001b[0m\n",
      "\u001b[34mEnable metrics API: true\u001b[0m\n",
      "\u001b[34m2022-08-10 08:58:58,125 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: model.mar\u001b[0m\n",
      "\u001b[34m2022-08-10 08:58:58,454 [INFO ] main org.pytorch.serve.archive.ModelArchive - eTag e7ee78ee13834363b08b0a433df33154\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.1->matplotlib->-r /opt/ml/model/code/requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m2022-08-10 08:58:58,084 [INFO ] main org.pytorch.serve.ModelServer - \u001b[0m\n",
      "\u001b[35mTorchserve version: 0.3.1\u001b[0m\n",
      "\u001b[35mTS Home: /opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[35mCurrent directory: /\u001b[0m\n",
      "\u001b[35mTemp directory: /home/model-server/tmp\u001b[0m\n",
      "\u001b[35mNumber of GPUs: 0\u001b[0m\n",
      "\u001b[35mNumber of CPUs: 4\u001b[0m\n",
      "\u001b[35mMax heap size: 2950 M\u001b[0m\n",
      "\u001b[35mPython executable: /opt/conda/bin/python3.6\u001b[0m\n",
      "\u001b[35mConfig file: /etc/sagemaker-ts.properties\u001b[0m\n",
      "\u001b[35mInference address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[35mManagement address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[35mMetrics address: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[35mModel Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[35mInitial Models: model.mar\u001b[0m\n",
      "\u001b[35mLog dir: /logs\u001b[0m\n",
      "\u001b[35mMetrics dir: /logs\u001b[0m\n",
      "\u001b[35mNetty threads: 0\u001b[0m\n",
      "\u001b[35mNetty client threads: 0\u001b[0m\n",
      "\u001b[35mDefault workers per model: 4\u001b[0m\n",
      "\u001b[35mBlacklist Regex: N/A\u001b[0m\n",
      "\u001b[35mMaximum Response Size: 6553500\u001b[0m\n",
      "\u001b[35mMaximum Request Size: 6553500\u001b[0m\n",
      "\u001b[35mPrefer direct buffer: false\u001b[0m\n",
      "\u001b[35mAllowed Urls: [file://.*|http(s)?://.*]\u001b[0m\n",
      "\u001b[35mCustom python dependency for model allowed: false\u001b[0m\n",
      "\u001b[35mMetrics report format: prometheus\u001b[0m\n",
      "\u001b[35mEnable metrics API: true\u001b[0m\n",
      "\u001b[35m2022-08-10 08:58:58,125 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: model.mar\u001b[0m\n",
      "\u001b[35m2022-08-10 08:58:58,454 [INFO ] main org.pytorch.serve.archive.ModelArchive - eTag e7ee78ee13834363b08b0a433df33154\u001b[0m\n",
      "\u001b[34m2022-08-10 08:58:58,467 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\u001b[0m\n",
      "\u001b[34m2022-08-10 08:58:58,502 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2022-08-10 08:58:58,772 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2022-08-10 08:58:58,773 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]58\u001b[0m\n",
      "\u001b[34m2022-08-10 08:58:58,773 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-10 08:58:58,773 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[34m2022-08-10 08:58:58,779 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34m2022-08-10 08:58:58,779 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2022-08-10 08:58:58,781 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2022-08-10 08:58:58,793 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-10 08:58:58,803 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[34m2022-08-10 08:58:58,806 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]60\u001b[0m\n",
      "\u001b[34m2022-08-10 08:58:58,806 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-10 08:58:58,806 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[34m2022-08-10 08:58:58,807 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-10 08:58:58,807 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-10 08:58:58,813 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]56\u001b[0m\n",
      "\u001b[34m2022-08-10 08:58:58,814 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-10 08:58:58,814 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[34m2022-08-10 08:58:58,814 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-10 08:58:58,818 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2022-08-10 08:58:58,830 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]57\u001b[0m\n",
      "\u001b[34m2022-08-10 08:58:58,830 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-10 08:58:58,831 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[34m2022-08-10 08:58:58,831 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2022-08-10 08:58:58,851 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2022-08-10 08:58:58,851 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001b[0m\n",
      "\u001b[34m2022-08-10 08:58:58,851 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2022-08-10 08:58:58,853 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[35m2022-08-10 08:58:58,467 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\u001b[0m\n",
      "\u001b[35m2022-08-10 08:58:58,502 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[35m2022-08-10 08:58:58,772 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2022-08-10 08:58:58,773 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]58\u001b[0m\n",
      "\u001b[35m2022-08-10 08:58:58,773 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-10 08:58:58,773 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[35m2022-08-10 08:58:58,779 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[35m2022-08-10 08:58:58,779 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[35m2022-08-10 08:58:58,781 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2022-08-10 08:58:58,793 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2022-08-10 08:58:58,803 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[35m2022-08-10 08:58:58,806 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]60\u001b[0m\n",
      "\u001b[35m2022-08-10 08:58:58,806 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-10 08:58:58,806 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[35m2022-08-10 08:58:58,807 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2022-08-10 08:58:58,807 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2022-08-10 08:58:58,813 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]56\u001b[0m\n",
      "\u001b[35m2022-08-10 08:58:58,814 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-10 08:58:58,814 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[35m2022-08-10 08:58:58,814 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2022-08-10 08:58:58,818 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[35m2022-08-10 08:58:58,830 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]57\u001b[0m\n",
      "\u001b[35m2022-08-10 08:58:58,830 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-10 08:58:58,831 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[35m2022-08-10 08:58:58,831 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[35m2022-08-10 08:58:58,851 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[35m2022-08-10 08:58:58,851 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001b[0m\n",
      "\u001b[35m2022-08-10 08:58:58,851 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[35m2022-08-10 08:58:58,853 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[34mModel server started.\u001b[0m\n",
      "\u001b[34m2022-08-10 08:58:59,606 [INFO ] pool-2-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:ddc095260043,timestamp:1660121939\u001b[0m\n",
      "\u001b[34m2022-08-10 08:58:59,609 [INFO ] pool-2-thread-1 TS_METRICS - DiskAvailable.Gigabytes:48.03485870361328|#Level:Host|#hostname:ddc095260043,timestamp:1660121939\u001b[0m\n",
      "\u001b[34m2022-08-10 08:58:59,610 [INFO ] pool-2-thread-1 TS_METRICS - DiskUsage.Gigabytes:7.830272674560547|#Level:Host|#hostname:ddc095260043,timestamp:1660121939\u001b[0m\n",
      "\u001b[34m2022-08-10 08:58:59,611 [INFO ] pool-2-thread-1 TS_METRICS - DiskUtilization.Percent:14.0|#Level:Host|#hostname:ddc095260043,timestamp:1660121939\u001b[0m\n",
      "\u001b[34m2022-08-10 08:58:59,612 [INFO ] pool-2-thread-1 TS_METRICS - MemoryAvailable.Megabytes:14165.2890625|#Level:Host|#hostname:ddc095260043,timestamp:1660121939\u001b[0m\n",
      "\u001b[34m2022-08-10 08:58:59,613 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUsed.Megabytes:1081.95703125|#Level:Host|#hostname:ddc095260043,timestamp:1660121939\u001b[0m\n",
      "\u001b[34m2022-08-10 08:58:59,613 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUtilization.Percent:9.1|#Level:Host|#hostname:ddc095260043,timestamp:1660121939\u001b[0m\n",
      "\u001b[35mModel server started.\u001b[0m\n",
      "\u001b[35m2022-08-10 08:58:59,606 [INFO ] pool-2-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:ddc095260043,timestamp:1660121939\u001b[0m\n",
      "\u001b[35m2022-08-10 08:58:59,609 [INFO ] pool-2-thread-1 TS_METRICS - DiskAvailable.Gigabytes:48.03485870361328|#Level:Host|#hostname:ddc095260043,timestamp:1660121939\u001b[0m\n",
      "\u001b[35m2022-08-10 08:58:59,610 [INFO ] pool-2-thread-1 TS_METRICS - DiskUsage.Gigabytes:7.830272674560547|#Level:Host|#hostname:ddc095260043,timestamp:1660121939\u001b[0m\n",
      "\u001b[35m2022-08-10 08:58:59,611 [INFO ] pool-2-thread-1 TS_METRICS - DiskUtilization.Percent:14.0|#Level:Host|#hostname:ddc095260043,timestamp:1660121939\u001b[0m\n",
      "\u001b[35m2022-08-10 08:58:59,612 [INFO ] pool-2-thread-1 TS_METRICS - MemoryAvailable.Megabytes:14165.2890625|#Level:Host|#hostname:ddc095260043,timestamp:1660121939\u001b[0m\n",
      "\u001b[35m2022-08-10 08:58:59,613 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUsed.Megabytes:1081.95703125|#Level:Host|#hostname:ddc095260043,timestamp:1660121939\u001b[0m\n",
      "\u001b[35m2022-08-10 08:58:59,613 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUtilization.Percent:9.1|#Level:Host|#hostname:ddc095260043,timestamp:1660121939\u001b[0m\n",
      "\u001b[34m2022-08-10 08:59:00,721 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1736\u001b[0m\n",
      "\u001b[34m2022-08-10 08:59:00,721 [INFO ] W-9003-model_1 TS_METRICS - W-9003-model_1.ms:2223|#Level:Host|#hostname:ddc095260043,timestamp:1660121940\u001b[0m\n",
      "\u001b[35m2022-08-10 08:59:00,721 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1736\u001b[0m\n",
      "\u001b[35m2022-08-10 08:59:00,721 [INFO ] W-9003-model_1 TS_METRICS - W-9003-model_1.ms:2223|#Level:Host|#hostname:ddc095260043,timestamp:1660121940\u001b[0m\n",
      "\u001b[34m2022-08-10 08:59:00,722 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:114|#Level:Host|#hostname:ddc095260043,timestamp:null\u001b[0m\n",
      "\u001b[34m2022-08-10 08:59:00,744 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1742\u001b[0m\n",
      "\u001b[34m2022-08-10 08:59:00,745 [INFO ] W-9002-model_1 TS_METRICS - W-9002-model_1.ms:2247|#Level:Host|#hostname:ddc095260043,timestamp:1660121940\u001b[0m\n",
      "\u001b[34m2022-08-10 08:59:00,745 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:131|#Level:Host|#hostname:ddc095260043,timestamp:null\u001b[0m\n",
      "\u001b[34m2022-08-10 08:59:00,877 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1875\u001b[0m\n",
      "\u001b[34m2022-08-10 08:59:00,877 [INFO ] W-9000-model_1 TS_METRICS - W-9000-model_1.ms:2394|#Level:Host|#hostname:ddc095260043,timestamp:1660121940\u001b[0m\n",
      "\u001b[34m2022-08-10 08:59:00,877 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:130|#Level:Host|#hostname:ddc095260043,timestamp:null\u001b[0m\n",
      "\u001b[34m2022-08-10 08:59:00,884 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1882\u001b[0m\n",
      "\u001b[34m2022-08-10 08:59:00,884 [INFO ] W-9001-model_1 TS_METRICS - W-9001-model_1.ms:2394|#Level:Host|#hostname:ddc095260043,timestamp:1660121940\u001b[0m\n",
      "\u001b[34m2022-08-10 08:59:00,884 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:130|#Level:Host|#hostname:ddc095260043,timestamp:null\u001b[0m\n",
      "\u001b[35m2022-08-10 08:59:00,722 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:114|#Level:Host|#hostname:ddc095260043,timestamp:null\u001b[0m\n",
      "\u001b[35m2022-08-10 08:59:00,744 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1742\u001b[0m\n",
      "\u001b[35m2022-08-10 08:59:00,745 [INFO ] W-9002-model_1 TS_METRICS - W-9002-model_1.ms:2247|#Level:Host|#hostname:ddc095260043,timestamp:1660121940\u001b[0m\n",
      "\u001b[35m2022-08-10 08:59:00,745 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:131|#Level:Host|#hostname:ddc095260043,timestamp:null\u001b[0m\n",
      "\u001b[35m2022-08-10 08:59:00,877 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1875\u001b[0m\n",
      "\u001b[35m2022-08-10 08:59:00,877 [INFO ] W-9000-model_1 TS_METRICS - W-9000-model_1.ms:2394|#Level:Host|#hostname:ddc095260043,timestamp:1660121940\u001b[0m\n",
      "\u001b[35m2022-08-10 08:59:00,877 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:130|#Level:Host|#hostname:ddc095260043,timestamp:null\u001b[0m\n",
      "\u001b[35m2022-08-10 08:59:00,884 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1882\u001b[0m\n",
      "\u001b[35m2022-08-10 08:59:00,884 [INFO ] W-9001-model_1 TS_METRICS - W-9001-model_1.ms:2394|#Level:Host|#hostname:ddc095260043,timestamp:1660121940\u001b[0m\n",
      "\u001b[35m2022-08-10 08:59:00,884 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:130|#Level:Host|#hostname:ddc095260043,timestamp:null\u001b[0m\n",
      "\u001b[34m2022-08-10 08:59:03,036 [INFO ] pool-1-thread-5 ACCESS_LOG - /169.254.255.130:53106 \"GET /ping HTTP/1.1\" 200 13\u001b[0m\n",
      "\u001b[34m2022-08-10 08:59:03,037 [INFO ] pool-1-thread-5 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:ddc095260043,timestamp:null\u001b[0m\n",
      "\u001b[35m2022-08-10 08:59:03,036 [INFO ] pool-1-thread-5 ACCESS_LOG - /169.254.255.130:53106 \"GET /ping HTTP/1.1\" 200 13\u001b[0m\n",
      "\u001b[35m2022-08-10 08:59:03,037 [INFO ] pool-1-thread-5 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:ddc095260043,timestamp:null\u001b[0m\n",
      "\u001b[34m2022-08-10 08:59:03,056 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:53108 \"GET /execution-parameters HTTP/1.1\" 404 1\u001b[0m\n",
      "\u001b[34m2022-08-10 08:59:03,056 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:ddc095260043,timestamp:null\u001b[0m\n",
      "\u001b[35m2022-08-10 08:59:03,056 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:53108 \"GET /execution-parameters HTTP/1.1\" 404 1\u001b[0m\n",
      "\u001b[35m2022-08-10 08:59:03,056 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:ddc095260043,timestamp:null\u001b[0m\n",
      "\u001b[32m2022-08-10T08:59:03.063:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[35m2022-08-10 08:59:05,886 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - all checkpoints:  ['/opt/ml/model/checkpoint-000001.pth']\u001b[0m\n",
      "\u001b[35m2022-08-10 08:59:05,886 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2670\u001b[0m\n",
      "\u001b[35m2022-08-10 08:59:05,886 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - <<< self.key_ls:  ['Warm Lined', 'Details', 'Type', 'Composition', 'Quantity', 'Belt', 'Fit Type', 'Length', 'Sleeve Length', 'Pattern Type', 'Color', 'Lining', 'Sleeve Type', 'Pockets', 'Sheer', 'Size Fit', 'Neckline', 'Fabric', 'Material', 'Hem Shaped', 'Care Instructions', 'Body', 'Arabian Clothing', 'Style']\u001b[0m\n",
      "\u001b[35m2022-08-10 08:59:05,886 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - <<< self.class_len_ls:  [3, 47, 5, 96, 9, 3, 4, 4, 6, 44, 55, 3, 10, 3, 4, 2, 21, 4, 21, 4, 5, 2, 2, 7]\u001b[0m\n",
      "\u001b[35m2022-08-10 08:59:05,886 [INFO ] W-9003-model_1 ACCESS_LOG - /169.254.255.130:53124 \"POST /invocations HTTP/1.1\" 200 2677\u001b[0m\n",
      "\u001b[35m2022-08-10 08:59:05,886 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:2668.95|#ModelName:model,Level:Model|#hostname:ddc095260043,requestID:160c994b-5ed0-4e26-914b-ead18cdda2a4,timestamp:1660121945\u001b[0m\n",
      "\u001b[35m2022-08-10 08:59:05,887 [INFO ] W-9003-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:ddc095260043,timestamp:null\u001b[0m\n",
      "\u001b[35m2022-08-10 08:59:05,887 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:ddc095260043,timestamp:null\u001b[0m\n",
      "\u001b[35m2022-08-10 08:59:05,887 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:ddc095260043,timestamp:null\u001b[0m\n",
      "\u001b[35m2022-08-10 08:59:08,483 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2531\u001b[0m\n",
      "\u001b[35m2022-08-10 08:59:08,484 [INFO ] W-9002-model_1 ACCESS_LOG - /169.254.255.130:53124 \"POST /invocations HTTP/1.1\" 200 2534\u001b[0m\n",
      "\u001b[35m2022-08-10 08:59:08,484 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - all checkpoints:  ['/opt/ml/model/checkpoint-000001.pth']\u001b[0m\n",
      "\u001b[35m2022-08-10 08:59:08,484 [INFO ] W-9002-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:ddc095260043,timestamp:null\u001b[0m\n",
      "\u001b[35m2022-08-10 08:59:08,484 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - <<< self.key_ls:  ['Warm Lined', 'Details', 'Type', 'Composition', 'Quantity', 'Belt', 'Fit Type', 'Length', 'Sleeve Length', 'Pattern Type', 'Color', 'Lining', 'Sleeve Type', 'Pockets', 'Sheer', 'Size Fit', 'Neckline', 'Fabric', 'Material', 'Hem Shaped', 'Care Instructions', 'Body', 'Arabian Clothing', 'Style']\u001b[0m\n",
      "\u001b[35m2022-08-10 08:59:08,484 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - <<< self.class_len_ls:  [3, 47, 5, 96, 9, 3, 4, 4, 6, 44, 55, 3, 10, 3, 4, 2, 21, 4, 21, 4, 5, 2, 2, 7]\u001b[0m\n",
      "\u001b[35m2022-08-10 08:59:08,484 [INFO ] W-9002-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:ddc095260043,timestamp:null\u001b[0m\n",
      "\u001b[35m2022-08-10 08:59:08,484 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:2|#Level:Host|#hostname:ddc095260043,timestamp:null\u001b[0m\n",
      "\u001b[35m2022-08-10 08:59:08,484 [INFO ] W-9002-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:2530.23|#ModelName:model,Level:Model|#hostname:ddc095260043,requestID:12b419a0-597b-4506-830e-557e16afe416,timestamp:1660121948\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "transformer.transform(\n",
    "    data=inference_inputs,\n",
    "    data_type=\"S3Prefix\",\n",
    "    content_type=\"application/x-image\",\n",
    "    wait=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b762c29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CreationTime': datetime.datetime(2022, 8, 10, 8, 54, 8, 975000, tzinfo=tzlocal()),\n",
      " 'LastModifiedTime': datetime.datetime(2022, 8, 10, 8, 59, 9, 644000, tzinfo=tzlocal()),\n",
      " 'TransformEndTime': datetime.datetime(2022, 8, 10, 8, 59, 9, 265000, tzinfo=tzlocal()),\n",
      " 'TransformJobArn': 'arn:aws:sagemaker:us-east-1:726335585155:transform-job/pytorch-inference-2022-08-10-08-54-08-963',\n",
      " 'TransformJobName': 'pytorch-inference-2022-08-10-08-54-08-963',\n",
      " 'TransformJobStatus': 'Completed'}\n",
      "{'CreationTime': datetime.datetime(2020, 1, 7, 8, 6, 7, 55000, tzinfo=tzlocal()),\n",
      " 'LastModifiedTime': datetime.datetime(2020, 1, 7, 8, 11, 17, 354000, tzinfo=tzlocal()),\n",
      " 'TransformEndTime': datetime.datetime(2020, 1, 7, 8, 11, 17, tzinfo=tzlocal()),\n",
      " 'TransformJobArn': 'arn:aws:sagemaker:us-east-1:726335585155:transform-job/image-classification-model-2020-01-07-08-06-02',\n",
      " 'TransformJobName': 'image-classification-model-2020-01-07-08-06-02',\n",
      " 'TransformJobStatus': 'Completed'}\n",
      "{'CreationTime': datetime.datetime(2020, 1, 7, 7, 41, 38, 806000, tzinfo=tzlocal()),\n",
      " 'FailureReason': 'ClientError: See job logs for more information',\n",
      " 'LastModifiedTime': datetime.datetime(2020, 1, 7, 7, 46, 43, 466000, tzinfo=tzlocal()),\n",
      " 'TransformEndTime': datetime.datetime(2020, 1, 7, 7, 46, 43, tzinfo=tzlocal()),\n",
      " 'TransformJobArn': 'arn:aws:sagemaker:us-east-1:726335585155:transform-job/image-classification-model-2020-01-07-07-41-18',\n",
      " 'TransformJobName': 'image-classification-model-2020-01-07-07-41-18',\n",
      " 'TransformJobStatus': 'Failed'}\n",
      "{'CreationTime': datetime.datetime(2020, 1, 6, 8, 16, 47, 650000, tzinfo=tzlocal()),\n",
      " 'LastModifiedTime': datetime.datetime(2020, 1, 6, 8, 22, 33, 342000, tzinfo=tzlocal()),\n",
      " 'TransformEndTime': datetime.datetime(2020, 1, 6, 8, 22, 33, tzinfo=tzlocal()),\n",
      " 'TransformJobArn': 'arn:aws:sagemaker:us-east-1:726335585155:transform-job/image-classification-model-2020-01-06-08-16-45',\n",
      " 'TransformJobName': 'image-classification-model-2020-01-06-08-16-45',\n",
      " 'TransformJobStatus': 'Completed'}\n"
     ]
    }
   ],
   "source": [
    "import pprint as pp\n",
    "sm_cli = sess.sagemaker_client\n",
    "\n",
    "transform_jobs = sm_cli.list_transform_jobs()[\"TransformJobSummaries\"]\n",
    "for job in transform_jobs:\n",
    "    pp.pprint(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d7157d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CreationTime': datetime.datetime(2022, 8, 10, 8, 54, 8, 975000, tzinfo=tzlocal()),\n",
      " 'DataProcessing': {'InputFilter': '$',\n",
      "                    'JoinSource': 'None',\n",
      "                    'OutputFilter': '$'},\n",
      " 'ModelName': 'pytorch-inference-2022-08-10-08-54-07-174',\n",
      " 'ResponseMetadata': {'HTTPHeaders': {'content-length': '895',\n",
      "                                      'content-type': 'application/x-amz-json-1.1',\n",
      "                                      'date': 'Wed, 10 Aug 2022 09:03:53 GMT',\n",
      "                                      'x-amzn-requestid': 'c11b8ac6-4177-452f-a12d-10a3e75bac68'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': 'c11b8ac6-4177-452f-a12d-10a3e75bac68',\n",
      "                      'RetryAttempts': 0},\n",
      " 'TransformEndTime': datetime.datetime(2022, 8, 10, 8, 59, 9, 265000, tzinfo=tzlocal()),\n",
      " 'TransformInput': {'CompressionType': 'None',\n",
      "                    'ContentType': 'application/x-image',\n",
      "                    'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
      "                                                    'S3Uri': 's3://sagemaker-us-east-1-726335585155/batch_transform'}},\n",
      "                    'SplitType': 'None'},\n",
      " 'TransformJobArn': 'arn:aws:sagemaker:us-east-1:726335585155:transform-job/pytorch-inference-2022-08-10-08-54-08-963',\n",
      " 'TransformJobName': 'pytorch-inference-2022-08-10-08-54-08-963',\n",
      " 'TransformJobStatus': 'Completed',\n",
      " 'TransformOutput': {'AssembleWith': 'None',\n",
      "                     'KmsKeyId': '',\n",
      "                     'S3OutputPath': 's3://sagemaker-us-east-1-726335585155/pytorch-inference-2022-08-10-08-54-08-963'},\n",
      " 'TransformResources': {'InstanceCount': 1, 'InstanceType': 'ml.m5.xlarge'},\n",
      " 'TransformStartTime': datetime.datetime(2022, 8, 10, 8, 57, 37, 564000, tzinfo=tzlocal())}\n"
     ]
    }
   ],
   "source": [
    "job_info = sm_cli.describe_transform_job(\n",
    "    TransformJobName=transformer.latest_transform_job.name\n",
    ")\n",
    "\n",
    "pp.pprint(job_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "47891238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-us-east-1-726335585155 pytorch-inference-2022-08-10-08-54-08-963\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def get_bucket_and_prefix(s3_output_path):\n",
    "    trim = re.sub(\"s3://\", \"\", s3_output_path)\n",
    "    bucket, prefix = trim.split(\"/\")\n",
    "    return bucket, prefix\n",
    "\n",
    "\n",
    "local_path = \"output\"  # Where to save the output locally\n",
    "\n",
    "bucket, output_prefix = get_bucket_and_prefix(job_info[\"TransformOutput\"][\"S3OutputPath\"])\n",
    "print(bucket, output_prefix)\n",
    "\n",
    "sess.download_data(path=local_path, bucket=bucket, key_prefix=output_prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c5a1218f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'result': {'Warm Lined': 'other', 'Details': 'Zipper', 'Type': 'Pullovers', 'Composition': '100% Polyester', 'Quantity': '1 piece', 'Belt': 'other', 'Fit Type': 'Regular Fit', 'Length': 'Regular', 'Sleeve Length': 'Long Sleeve', 'Pattern Type': 'Plain', 'Color': 'Black', 'Lining': 'other', 'Sleeve Type': 'Drop Shoulder', 'Pockets': 'other', 'Sheer': 'No', 'Size Fit': 'other', 'Neckline': 'Hooded', 'Fabric': 'Slight Stretch', 'Material': 'Polyester', 'Hem Shaped': 'other', 'Care Instructions': 'Machine wash or professional dry clean', 'Body': 'other', 'Arabian Clothing': 'other', 'Style': 'Casual'}}\n",
      "{'result': {'Warm Lined': 'other', 'Details': 'Zipper', 'Type': 'Pullovers', 'Composition': '100% Polyester', 'Quantity': '1 piece', 'Belt': 'other', 'Fit Type': 'Regular Fit', 'Length': 'Regular', 'Sleeve Length': 'Long Sleeve', 'Pattern Type': 'Plain', 'Color': 'Black', 'Lining': 'other', 'Sleeve Type': 'Drop Shoulder', 'Pockets': 'other', 'Sheer': 'No', 'Size Fit': 'other', 'Neckline': 'Hooded', 'Fabric': 'Slight Stretch', 'Material': 'Polyester', 'Hem Shaped': 'other', 'Care Instructions': 'Machine wash or professional dry clean', 'Body': 'other', 'Arabian Clothing': 'other', 'Style': 'Casual'}}\n"
     ]
    }
   ],
   "source": [
    "# Inspect the output\n",
    "\n",
    "import json\n",
    "\n",
    "for f in os.listdir(local_path):\n",
    "    path = os.path.join(local_path, f)\n",
    "    with open(path, \"r\") as f:\n",
    "        pred = json.load(f)\n",
    "        print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8371857c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
