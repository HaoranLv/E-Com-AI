{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cee6ace4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "315bcb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_positive = pd.read_excel('数据提供：listing篡改_20220428.xlsx',engine='openpyxl',sheet_name='正确')\n",
    "df_negative = pd.read_excel('数据提供：listing篡改_20220428.xlsx',engine='openpyxl',sheet_name='错误')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2631b85e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>SiteInfoID</th>\n",
       "      <th>ListingUrl</th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>类型</th>\n",
       "      <th>品类</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2234</td>\n",
       "      <td>17</td>\n",
       "      <td>https://www.amazon.com/dp/B07B9VKWSL?_encoding...</td>\n",
       "      <td>Stud Finder Wall Scanner  4 in 1 Electric Wood...</td>\n",
       "      <td>25.99</td>\n",
       "      <td>正确</td>\n",
       "      <td>Pickle-Ball Equipment</td>\n",
       "      <td>D05567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2438</td>\n",
       "      <td>17</td>\n",
       "      <td>https://www.amazon.com/dp/B07GT37484?_encoding...</td>\n",
       "      <td>rockspace WiFi Extender  12 Gigabit Wireless S...</td>\n",
       "      <td>59.99</td>\n",
       "      <td>正确</td>\n",
       "      <td>Body Composition Monitors</td>\n",
       "      <td>D05575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2248</td>\n",
       "      <td>17</td>\n",
       "      <td>https://www.amazon.com/dp/B07GVDXW5D?_encoding...</td>\n",
       "      <td>Pickleball Paddles USAPA Pro Graphite Pickleba...</td>\n",
       "      <td>84.99</td>\n",
       "      <td>正确</td>\n",
       "      <td>Body Composition Monitors</td>\n",
       "      <td>D05575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2446</td>\n",
       "      <td>17</td>\n",
       "      <td>https://www.amazon.com/dp/B07M6Y7355?_encoding...</td>\n",
       "      <td>1080P Webcam with Microphone eMeet C960 Web Ca...</td>\n",
       "      <td>33.99</td>\n",
       "      <td>正确</td>\n",
       "      <td>Pet Supplies</td>\n",
       "      <td>D05588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2258</td>\n",
       "      <td>17</td>\n",
       "      <td>https://www.amazon.com/dp/B07R9XZSGL?_encoding...</td>\n",
       "      <td>3Axis Gimbal Stabilizer for iPhone 12 11 PRO M...</td>\n",
       "      <td>99.00</td>\n",
       "      <td>正确</td>\n",
       "      <td>Hobby RC Quadcopters &amp; Multirotors</td>\n",
       "      <td>D05588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID  SiteInfoID                                         ListingUrl  \\\n",
       "0  2234          17  https://www.amazon.com/dp/B07B9VKWSL?_encoding...   \n",
       "1  2438          17  https://www.amazon.com/dp/B07GT37484?_encoding...   \n",
       "2  2248          17  https://www.amazon.com/dp/B07GVDXW5D?_encoding...   \n",
       "3  2446          17  https://www.amazon.com/dp/B07M6Y7355?_encoding...   \n",
       "4  2258          17  https://www.amazon.com/dp/B07R9XZSGL?_encoding...   \n",
       "\n",
       "                                               Title  Price  类型  \\\n",
       "0  Stud Finder Wall Scanner  4 in 1 Electric Wood...  25.99  正确   \n",
       "1  rockspace WiFi Extender  12 Gigabit Wireless S...  59.99  正确   \n",
       "2  Pickleball Paddles USAPA Pro Graphite Pickleba...  84.99  正确   \n",
       "3  1080P Webcam with Microphone eMeet C960 Web Ca...  33.99  正确   \n",
       "4  3Axis Gimbal Stabilizer for iPhone 12 11 PRO M...  99.00  正确   \n",
       "\n",
       "                                   品类    code  \n",
       "0               Pickle-Ball Equipment  D05567  \n",
       "1           Body Composition Monitors  D05575  \n",
       "2           Body Composition Monitors  D05575  \n",
       "3                        Pet Supplies  D05588  \n",
       "4  Hobby RC Quadcopters & Multirotors  D05588  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_positive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fc31d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Asin</th>\n",
       "      <th>SiteInfoID</th>\n",
       "      <th>ListingUrl</th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>类型</th>\n",
       "      <th>类目</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2234</td>\n",
       "      <td>B07B9VKWSL</td>\n",
       "      <td>17</td>\n",
       "      <td>https://www.amazon.com/dp/B07B9VKWSL?_encoding...</td>\n",
       "      <td>Stud Finder Wall Scanner  4 in 1 Electric Wood...</td>\n",
       "      <td>25.99</td>\n",
       "      <td>错误</td>\n",
       "      <td>Stud Finders &amp; Scanners</td>\n",
       "      <td>D05567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2438</td>\n",
       "      <td>B07GT37484</td>\n",
       "      <td>17</td>\n",
       "      <td>https://www.amazon.com/dp/B07GT37484?_encoding...</td>\n",
       "      <td>rockspace WiFi Extender, Air Treatment Units o...</td>\n",
       "      <td>59.99</td>\n",
       "      <td>错误</td>\n",
       "      <td>Repeaters</td>\n",
       "      <td>D05575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2248</td>\n",
       "      <td>B07GVDXW5D</td>\n",
       "      <td>17</td>\n",
       "      <td>https://www.amazon.com/dp/B07GVDXW5D?_encoding...</td>\n",
       "      <td>Water Treatment Units or Filters，Pickleball Pa...</td>\n",
       "      <td>84.99</td>\n",
       "      <td>错误</td>\n",
       "      <td>Pickle-Ball Equipment</td>\n",
       "      <td>D05575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2446</td>\n",
       "      <td>B07M6Y7355</td>\n",
       "      <td>17</td>\n",
       "      <td>https://www.amazon.com/dp/B07M6Y7355?_encoding...</td>\n",
       "      <td>1080P Webcam with Microphone eMeet C960 Web Ca...</td>\n",
       "      <td>33.99</td>\n",
       "      <td>错误</td>\n",
       "      <td>Webcam Mounts</td>\n",
       "      <td>D05588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2258</td>\n",
       "      <td>B07R9XZSGL</td>\n",
       "      <td>17</td>\n",
       "      <td>https://www.amazon.com/dp/B07R9XZSGL?_encoding...</td>\n",
       "      <td>3Axis Gimbal Stabilizer for iPhone 12 11 PRO M...</td>\n",
       "      <td>99.00</td>\n",
       "      <td>错误</td>\n",
       "      <td>Professional Video Stabilizers</td>\n",
       "      <td>D05588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID        Asin  SiteInfoID  \\\n",
       "0  2234  B07B9VKWSL          17   \n",
       "1  2438  B07GT37484          17   \n",
       "2  2248  B07GVDXW5D          17   \n",
       "3  2446  B07M6Y7355          17   \n",
       "4  2258  B07R9XZSGL          17   \n",
       "\n",
       "                                          ListingUrl  \\\n",
       "0  https://www.amazon.com/dp/B07B9VKWSL?_encoding...   \n",
       "1  https://www.amazon.com/dp/B07GT37484?_encoding...   \n",
       "2  https://www.amazon.com/dp/B07GVDXW5D?_encoding...   \n",
       "3  https://www.amazon.com/dp/B07M6Y7355?_encoding...   \n",
       "4  https://www.amazon.com/dp/B07R9XZSGL?_encoding...   \n",
       "\n",
       "                                               Title  Price  类型  \\\n",
       "0  Stud Finder Wall Scanner  4 in 1 Electric Wood...  25.99  错误   \n",
       "1  rockspace WiFi Extender, Air Treatment Units o...  59.99  错误   \n",
       "2  Water Treatment Units or Filters，Pickleball Pa...  84.99  错误   \n",
       "3  1080P Webcam with Microphone eMeet C960 Web Ca...  33.99  错误   \n",
       "4  3Axis Gimbal Stabilizer for iPhone 12 11 PRO M...  99.00  错误   \n",
       "\n",
       "                               类目    code  \n",
       "0         Stud Finders & Scanners  D05567  \n",
       "1                       Repeaters  D05575  \n",
       "2           Pickle-Ball Equipment  D05575  \n",
       "3                   Webcam Mounts  D05588  \n",
       "4  Professional Video Stabilizers  D05588  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_negative.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574fae9d",
   "metadata": {},
   "source": [
    "# listing文案防篡改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d89e3720",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train_p, x_test_p = train_test_split(df_positive,test_size=0.25,random_state=0)\n",
    "x_train_n, x_test_n = train_test_split(df_negative,test_size=0.25,random_state=0)\n",
    "\n",
    "train = pd.concat([x_train_p,x_train_n])\n",
    "test = pd.concat([x_test_p,x_test_n])\n",
    "\n",
    "train = train.rename(columns={\"类型\": \"label\",\"类目\":\"category\"})\n",
    "test = test.rename(columns={\"类型\": \"label\",\"类目\":\"category\"})\n",
    "\n",
    "#train['sentence1_key'] = train.apply(lambda row: row['category']+row['Title'])\n",
    "\n",
    "train[\"category\"] = train[\"category\"].astype(str)\n",
    "\n",
    "train['sentence1_key'] = train[\"category\"] + ': ' + train[\"Title\"]\n",
    "\n",
    "test[\"category\"] = test[\"category\"].astype(str)\n",
    "\n",
    "test['sentence1_key'] = test[\"category\"] + ': ' + test[\"Title\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94feb4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size (50, 11), test size(18, 11)\n"
     ]
    }
   ],
   "source": [
    "print (\"train size {}, test size{}\".format(train.shape,test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cd53e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[[\"label\",\"sentence1_key\"]].to_csv('./data/train.csv',index=False,encoding='utf-8')\n",
    "test[[\"label\",\"sentence1_key\"]].to_csv('./data/test.csv',index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44dc6b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import os\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "prefix='haofang-datalab'\n",
    "\n",
    "bucket = sess.default_bucket() \n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(\n",
    "    os.path.join(prefix, \"train/train.csv\")\n",
    ").upload_file(\"./data/train.csv\")\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(\n",
    "    os.path.join(prefix, \"test/test.csv\")\n",
    ").upload_file(\"./data/test.csv\")\n",
    "\n",
    "training_input_path = f's3://{sess.default_bucket()}/{prefix}/train/train.csv'\n",
    "test_input_path = f's3://{sess.default_bucket()}/{prefix}/test/test.csv'\n",
    "\n",
    "#git_config = {'repo': 'https://github.com/huggingface/transformers.git','branch': 'v4.6.1'} # v4.6.1 is referring to the `transformers_version` you use in the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c58d854a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-13 07:17:23 Starting - Starting the training job...\n",
      "2022-06-13 07:17:49 Starting - Preparing the instances for trainingProfilerReport-1655104643: InProgress\n",
      ".........\n",
      "2022-06-13 07:19:15 Downloading - Downloading input data...\n",
      "2022-06-13 07:19:46 Training - Downloading the training image.....................\n",
      "2022-06-13 07:23:07 Training - Training image download completed. Training in progress.\u001B[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001B[0m\n",
      "\u001B[34mbash: no job control in this shell\u001B[0m\n",
      "\u001B[34m2022-06-13 07:23:09,407 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001B[0m\n",
      "\u001B[34m2022-06-13 07:23:09,430 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001B[0m\n",
      "\u001B[34m2022-06-13 07:23:09,438 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001B[0m\n",
      "\u001B[34m2022-06-13 07:23:09,987 sagemaker-training-toolkit INFO     Invoking user script\u001B[0m\n",
      "\u001B[34mTraining Env:\u001B[0m\n",
      "\u001B[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"do_eval\": true,\n",
      "        \"do_predict\": true,\n",
      "        \"do_train\": true,\n",
      "        \"eval_steps\": 1000,\n",
      "        \"fp16\": false,\n",
      "        \"learning_rate\": 5e-05,\n",
      "        \"model_name_or_path\": \"roberta-large\",\n",
      "        \"num_train_epochs\": 10,\n",
      "        \"output_dir\": \"/opt/ml/model\",\n",
      "        \"per_device_eval_batch_size\": 4,\n",
      "        \"per_device_train_batch_size\": 4,\n",
      "        \"save_total_limit\": 3,\n",
      "        \"seed\": 7,\n",
      "        \"test_file\": \"/opt/ml/input/data/test/test.csv\",\n",
      "        \"train_file\": \"/opt/ml/input/data/train/train.csv\",\n",
      "        \"validation_file\": \"/opt/ml/input/data/test/test.csv\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"haofang-roberta-large-epoch10-2022-06-13-07-17-22-786\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-726335585155/haofang-roberta-large-epoch10-2022-06-13-07-17-22-786/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_glue\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_glue.py\"\u001B[0m\n",
      "\u001B[34m}\u001B[0m\n",
      "\u001B[34mEnvironment variables:\u001B[0m\n",
      "\u001B[34mSM_HOSTS=[\"algo-1\"]\u001B[0m\n",
      "\u001B[34mSM_NETWORK_INTERFACE_NAME=eth0\u001B[0m\n",
      "\u001B[34mSM_HPS={\"do_eval\":true,\"do_predict\":true,\"do_train\":true,\"eval_steps\":1000,\"fp16\":false,\"learning_rate\":5e-05,\"model_name_or_path\":\"roberta-large\",\"num_train_epochs\":10,\"output_dir\":\"/opt/ml/model\",\"per_device_eval_batch_size\":4,\"per_device_train_batch_size\":4,\"save_total_limit\":3,\"seed\":7,\"test_file\":\"/opt/ml/input/data/test/test.csv\",\"train_file\":\"/opt/ml/input/data/train/train.csv\",\"validation_file\":\"/opt/ml/input/data/test/test.csv\"}\u001B[0m\n",
      "\u001B[34mSM_USER_ENTRY_POINT=run_glue.py\u001B[0m\n",
      "\u001B[34mSM_FRAMEWORK_PARAMS={}\u001B[0m\n",
      "\u001B[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001B[0m\n",
      "\u001B[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001B[0m\n",
      "\u001B[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001B[0m\n",
      "\u001B[34mSM_CHANNELS=[\"test\",\"train\"]\u001B[0m\n",
      "\u001B[34mSM_CURRENT_HOST=algo-1\u001B[0m\n",
      "\u001B[34mSM_MODULE_NAME=run_glue\u001B[0m\n",
      "\u001B[34mSM_LOG_LEVEL=20\u001B[0m\n",
      "\u001B[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001B[0m\n",
      "\u001B[34mSM_INPUT_DIR=/opt/ml/input\u001B[0m\n",
      "\u001B[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001B[0m\n",
      "\u001B[34mSM_OUTPUT_DIR=/opt/ml/output\u001B[0m\n",
      "\u001B[34mSM_NUM_CPUS=8\u001B[0m\n",
      "\u001B[34mSM_NUM_GPUS=1\u001B[0m\n",
      "\u001B[34mSM_MODEL_DIR=/opt/ml/model\u001B[0m\n",
      "\u001B[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-726335585155/haofang-roberta-large-epoch10-2022-06-13-07-17-22-786/source/sourcedir.tar.gz\u001B[0m\n",
      "\u001B[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"do_eval\":true,\"do_predict\":true,\"do_train\":true,\"eval_steps\":1000,\"fp16\":false,\"learning_rate\":5e-05,\"model_name_or_path\":\"roberta-large\",\"num_train_epochs\":10,\"output_dir\":\"/opt/ml/model\",\"per_device_eval_batch_size\":4,\"per_device_train_batch_size\":4,\"save_total_limit\":3,\"seed\":7,\"test_file\":\"/opt/ml/input/data/test/test.csv\",\"train_file\":\"/opt/ml/input/data/train/train.csv\",\"validation_file\":\"/opt/ml/input/data/test/test.csv\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"haofang-roberta-large-epoch10-2022-06-13-07-17-22-786\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-726335585155/haofang-roberta-large-epoch10-2022-06-13-07-17-22-786/source/sourcedir.tar.gz\",\"module_name\":\"run_glue\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_glue.py\"}\u001B[0m\n",
      "\u001B[34mSM_USER_ARGS=[\"--do_eval\",\"True\",\"--do_predict\",\"True\",\"--do_train\",\"True\",\"--eval_steps\",\"1000\",\"--fp16\",\"False\",\"--learning_rate\",\"5e-05\",\"--model_name_or_path\",\"roberta-large\",\"--num_train_epochs\",\"10\",\"--output_dir\",\"/opt/ml/model\",\"--per_device_eval_batch_size\",\"4\",\"--per_device_train_batch_size\",\"4\",\"--save_total_limit\",\"3\",\"--seed\",\"7\",\"--test_file\",\"/opt/ml/input/data/test/test.csv\",\"--train_file\",\"/opt/ml/input/data/train/train.csv\",\"--validation_file\",\"/opt/ml/input/data/test/test.csv\"]\u001B[0m\n",
      "\u001B[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001B[0m\n",
      "\u001B[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001B[0m\n",
      "\u001B[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001B[0m\n",
      "\u001B[34mSM_HP_DO_EVAL=true\u001B[0m\n",
      "\u001B[34mSM_HP_DO_PREDICT=true\u001B[0m\n",
      "\u001B[34mSM_HP_DO_TRAIN=true\u001B[0m\n",
      "\u001B[34mSM_HP_EVAL_STEPS=1000\u001B[0m\n",
      "\u001B[34mSM_HP_FP16=false\u001B[0m\n",
      "\u001B[34mSM_HP_LEARNING_RATE=5e-05\u001B[0m\n",
      "\u001B[34mSM_HP_MODEL_NAME_OR_PATH=roberta-large\u001B[0m\n",
      "\u001B[34mSM_HP_NUM_TRAIN_EPOCHS=10\u001B[0m\n",
      "\u001B[34mSM_HP_OUTPUT_DIR=/opt/ml/model\u001B[0m\n",
      "\u001B[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=4\u001B[0m\n",
      "\u001B[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=4\u001B[0m\n",
      "\u001B[34mSM_HP_SAVE_TOTAL_LIMIT=3\u001B[0m\n",
      "\u001B[34mSM_HP_SEED=7\u001B[0m\n",
      "\u001B[34mSM_HP_TEST_FILE=/opt/ml/input/data/test/test.csv\u001B[0m\n",
      "\u001B[34mSM_HP_TRAIN_FILE=/opt/ml/input/data/train/train.csv\u001B[0m\n",
      "\u001B[34mSM_HP_VALIDATION_FILE=/opt/ml/input/data/test/test.csv\u001B[0m\n",
      "\u001B[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001B[0m\n",
      "\u001B[34mInvoking script with the following command:\u001B[0m\n",
      "\u001B[34m/opt/conda/bin/python3.6 run_glue.py --do_eval True --do_predict True --do_train True --eval_steps 1000 --fp16 False --learning_rate 5e-05 --model_name_or_path roberta-large --num_train_epochs 10 --output_dir /opt/ml/model --per_device_eval_batch_size 4 --per_device_train_batch_size 4 --save_total_limit 3 --seed 7 --test_file /opt/ml/input/data/test/test.csv --train_file /opt/ml/input/data/train/train.csv --validation_file /opt/ml/input/data/test/test.csv\u001B[0m\n",
      "\u001B[34m06/13/2022 07:23:15 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\u001B[0m\n",
      "\u001B[34m06/13/2022 07:23:15 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/opt/ml/model, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=4, per_device_eval_batch_size=4, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Jun13_07-23-14_algo-1, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=3, no_cuda=False, seed=7, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=1000, dataloader_num_workers=0, past_index=-1, run_name=/opt/ml/model, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=[], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=1, mp_parameters=)\u001B[0m\n",
      "\u001B[34m06/13/2022 07:23:15 - INFO - __main__ -   load a local file for train: /opt/ml/input/data/train/train.csv\u001B[0m\n",
      "\u001B[34m06/13/2022 07:23:15 - INFO - __main__ -   load a local file for validation: /opt/ml/input/data/test/test.csv\u001B[0m\n",
      "\u001B[34m06/13/2022 07:23:15 - INFO - __main__ -   load a local file for test: /opt/ml/input/data/test/test.csv\u001B[0m\n",
      "\u001B[34m06/13/2022 07:23:15 - WARNING - datasets.builder -   Using custom data configuration default-91a6eb488b025dfd\u001B[0m\n",
      "\u001B[34mDownloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-91a6eb488b025dfd/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\u001B[0m\n",
      "\u001B[34mDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-91a6eb488b025dfd/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\u001B[0m\n",
      "\u001B[34mtotal labels: ['正确', '错误']\u001B[0m\n",
      "\u001B[34m[INFO|file_utils.py:1532] 2022-06-13 07:23:15,255 >> https://huggingface.co/roberta-large/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpjxmn0j17\u001B[0m\n",
      "\u001B[34m[INFO|file_utils.py:1536] 2022-06-13 07:23:15,284 >> storing https://huggingface.co/roberta-large/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001B[0m\n",
      "\u001B[34m[INFO|file_utils.py:1544] 2022-06-13 07:23:15,284 >> creating metadata file for /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001B[0m\n",
      "\u001B[34m[INFO|configuration_utils.py:517] 2022-06-13 07:23:15,284 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001B[0m\n",
      "\u001B[34m[INFO|configuration_utils.py:553] 2022-06-13 07:23:15,285 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.6.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\u001B[0m\n",
      "\u001B[34m}\u001B[0m\n",
      "\u001B[34m[INFO|configuration_utils.py:517] 2022-06-13 07:23:15,310 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001B[0m\n",
      "\u001B[34m[INFO|configuration_utils.py:553] 2022-06-13 07:23:15,311 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.6.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\u001B[0m\n",
      "\u001B[34m}\u001B[0m\n",
      "\u001B[34m[INFO|file_utils.py:1532] 2022-06-13 07:23:15,336 >> https://huggingface.co/roberta-large/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpz3t2jcat\u001B[0m\n",
      "\u001B[34m[INFO|file_utils.py:1536] 2022-06-13 07:23:15,412 >> storing https://huggingface.co/roberta-large/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\u001B[0m\n",
      "\u001B[34m[INFO|file_utils.py:1544] 2022-06-13 07:23:15,412 >> creating metadata file for /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\u001B[0m\n",
      "\u001B[34m[INFO|file_utils.py:1532] 2022-06-13 07:23:15,440 >> https://huggingface.co/roberta-large/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp217uk4hw\u001B[0m\n",
      "\u001B[34m[INFO|file_utils.py:1536] 2022-06-13 07:23:15,500 >> storing https://huggingface.co/roberta-large/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\u001B[0m\n",
      "\u001B[34m[INFO|file_utils.py:1544] 2022-06-13 07:23:15,500 >> creating metadata file for /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\u001B[0m\n",
      "\u001B[34m[INFO|file_utils.py:1532] 2022-06-13 07:23:15,524 >> https://huggingface.co/roberta-large/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpqo6eraz2\u001B[0m\n",
      "\u001B[34m[INFO|file_utils.py:1536] 2022-06-13 07:23:15,622 >> storing https://huggingface.co/roberta-large/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\u001B[0m\n",
      "\u001B[34m[INFO|file_utils.py:1544] 2022-06-13 07:23:15,622 >> creating metadata file for /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\u001B[0m\n",
      "\u001B[34m[INFO|tokenization_utils_base.py:1717] 2022-06-13 07:23:15,711 >> loading file https://huggingface.co/roberta-large/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\u001B[0m\n",
      "\u001B[34m[INFO|tokenization_utils_base.py:1717] 2022-06-13 07:23:15,712 >> loading file https://huggingface.co/roberta-large/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\u001B[0m\n",
      "\u001B[34m[INFO|tokenization_utils_base.py:1717] 2022-06-13 07:23:15,712 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\u001B[0m\n",
      "\u001B[34m[INFO|tokenization_utils_base.py:1717] 2022-06-13 07:23:15,712 >> loading file https://huggingface.co/roberta-large/resolve/main/added_tokens.json from cache at None\u001B[0m\n",
      "\u001B[34m[INFO|tokenization_utils_base.py:1717] 2022-06-13 07:23:15,712 >> loading file https://huggingface.co/roberta-large/resolve/main/special_tokens_map.json from cache at None\u001B[0m\n",
      "\u001B[34m[INFO|tokenization_utils_base.py:1717] 2022-06-13 07:23:15,712 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer_config.json from cache at None\u001B[0m\n",
      "\u001B[34m[INFO|configuration_utils.py:517] 2022-06-13 07:23:15,830 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001B[0m\n",
      "\u001B[34m[INFO|configuration_utils.py:553] 2022-06-13 07:23:15,831 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"\\u6b63\\u786e\",\n",
      "    \"1\": \"\\u9519\\u8bef\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"\\u6b63\\u786e\": \"0\",\n",
      "    \"\\u9519\\u8bef\": \"1\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.6.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\u001B[0m\n",
      "\u001B[34m}\u001B[0m\n",
      "\u001B[34m[INFO|file_utils.py:1532] 2022-06-13 07:23:15,860 >> https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp9_1pvvyq\u001B[0m\n",
      "\u001B[34m[INFO|file_utils.py:1536] 2022-06-13 07:23:46,885 >> storing https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\u001B[0m\n",
      "\u001B[34m[INFO|file_utils.py:1544] 2022-06-13 07:23:46,885 >> creating metadata file for /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\u001B[0m\n",
      "\u001B[34m[INFO|modeling_utils.py:1155] 2022-06-13 07:23:46,886 >> loading weights file https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\u001B[0m\n",
      "\u001B[34m[WARNING|modeling_utils.py:1331] 2022-06-13 07:23:56,726 >> Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.dense.bias']\u001B[0m\n",
      "\u001B[34m- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001B[0m\n",
      "\u001B[34m- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001B[0m\n",
      "\u001B[34m[WARNING|modeling_utils.py:1342] 2022-06-13 07:23:56,727 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\u001B[0m\n",
      "\u001B[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001B[0m\n",
      "\u001B[34m06/13/2022 07:23:56 - INFO - __main__ -   Sample 20 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [0, 10197, 35, 39435, 23328, 1437, 34179, 1722, 25664, 96, 611, 6583, 7951, 21083, 510, 10478, 7773, 347, 16772, 17091, 26569, 14674, 3800, 14828, 19, 30393, 7773, 230, 8330, 6650, 13, 226, 39782, 4985, 4868, 306, 10444, 12091, 30411, 5900, 18652, 28127, 14828, 42626, 1378, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 0, 'sentence1_key': 'nan: Portable Monitor  Lepow 156 Inch Full HD 1080P USB TypeC Computer Display IPS Eye Care Screen with HDMI Type C Speakers for Laptop PC PS4 Xbox Phone Included Smart Cover amp Screen Protector Black'}.\u001B[0m\n",
      "\u001B[34m06/13/2022 07:23:56 - INFO - __main__ -   Sample 9 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [0, 10197, 35, 36081, 38703, 24284, 1506, 10326, 254, 39435, 8, 6207, 36330, 36081, 38703, 14969, 43522, 8114, 13411, 10537, 19, 2668, 254, 13, 24284, 1506, 11533, 5344, 9944, 22022, 15889, 3733, 27355, 8626, 5559, 45069, 11368, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 0, 'sentence1_key': 'nan: Ultrasonic Jewelry Cleaner Portable and Low Noise Ultrasonic Machine Stainless Steel 450ML with Timer for Jewelry Ring Silver Retainer Eyeglass Watches CoinsBlack'}.\u001B[0m\n",
      "\u001B[34m06/13/2022 07:23:56 - INFO - __main__ -   Sample 25 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [0, 487, 3760, 19550, 268, 35, 6090, 19226, 254, 26372, 771, 22575, 10918, 234, 3760, 23418, 487, 3760, 19550, 254, 23493, 11145, 42548, 6208, 45301, 22575, 5737, 19, 204, 2668, 254, 32048, 12221, 39435, 44883, 10415, 10918, 234, 3760, 13287, 495, 1506, 8229, 14969, 1437, 7090, 1211, 5726, 14708, 1628, 2692, 2692, 21706, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 1, 'sentence1_key': 'Nail Dryers: HanKeer 168W UV LED Nail LampNail Dryer Gel Polish Faster Acrylic UV Light with 4 Timer Setting Professional Portable Handle Poly LED Nail QuickDry Auto Machine  Small Red Orange Yellow Green Blue Blue Purple'}.\u001B[0m\n",
      "\u001B[34m[INFO|trainer.py:516] 2022-06-13 07:24:01,113 >> The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1_key.\u001B[0m\n",
      "\u001B[34m[INFO|trainer.py:1156] 2022-06-13 07:24:01,137 >> ***** Running training *****\u001B[0m\n",
      "\u001B[34m[INFO|trainer.py:1157] 2022-06-13 07:24:01,137 >>   Num examples = 50\u001B[0m\n",
      "\u001B[34m[INFO|trainer.py:1158] 2022-06-13 07:24:01,137 >>   Num Epochs = 10\u001B[0m\n",
      "\u001B[34m[INFO|trainer.py:1159] 2022-06-13 07:24:01,137 >>   Instantaneous batch size per device = 4\u001B[0m\n",
      "\u001B[34m[INFO|trainer.py:1160] 2022-06-13 07:24:01,138 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\u001B[0m\n",
      "\u001B[34m[INFO|trainer.py:1161] 2022-06-13 07:24:01,138 >>   Gradient Accumulation steps = 1\u001B[0m\n",
      "\u001B[34m[INFO|trainer.py:1162] 2022-06-13 07:24:01,138 >>   Total optimization steps = 130\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.412 algo-1:26 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.584 algo-1:26 INFO profiler_config_parser.py:102] User has disabled profiler.\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.585 algo-1:26 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.586 algo-1:26 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.587 algo-1:26 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.587 algo-1:26 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.775 algo-1:26 INFO hook.py:591] name:roberta.embeddings.word_embeddings.weight count_params:51471360\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.775 algo-1:26 INFO hook.py:591] name:roberta.embeddings.position_embeddings.weight count_params:526336\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.775 algo-1:26 INFO hook.py:591] name:roberta.embeddings.token_type_embeddings.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.775 algo-1:26 INFO hook.py:591] name:roberta.embeddings.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.775 algo-1:26 INFO hook.py:591] name:roberta.embeddings.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.776 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.0.attention.self.query.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.776 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.0.attention.self.query.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.776 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.0.attention.self.key.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.776 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.0.attention.self.key.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.776 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.0.attention.self.value.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.776 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.0.attention.self.value.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.776 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.776 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.0.attention.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.777 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.777 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.777 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.777 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.0.intermediate.dense.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.777 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.0.output.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.777 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.0.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.777 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.778 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.778 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.1.attention.self.query.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.778 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.1.attention.self.query.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.778 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.1.attention.self.key.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.778 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.1.attention.self.key.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.778 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.1.attention.self.value.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.778 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.1.attention.self.value.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.778 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.779 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.1.attention.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.779 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.779 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.779 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.779 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.1.intermediate.dense.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.779 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.1.output.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.779 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.1.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.780 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.780 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.780 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.2.attention.self.query.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.780 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.2.attention.self.query.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.780 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.2.attention.self.key.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.780 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.2.attention.self.key.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.780 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.2.attention.self.value.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.780 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.2.attention.self.value.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.780 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.781 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.2.attention.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.781 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.781 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.781 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.781 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.2.intermediate.dense.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.781 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.2.output.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.781 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.2.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.781 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.781 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.782 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.3.attention.self.query.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.782 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.3.attention.self.query.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.782 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.3.attention.self.key.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.782 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.3.attention.self.key.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.782 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.3.attention.self.value.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.782 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.3.attention.self.value.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.782 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.782 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.3.attention.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.782 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.783 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.783 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.783 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.3.intermediate.dense.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.783 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.3.output.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.783 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.3.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.783 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.783 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.783 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.4.attention.self.query.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.784 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.4.attention.self.query.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.784 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.4.attention.self.key.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.784 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.4.attention.self.key.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.784 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.4.attention.self.value.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.784 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.4.attention.self.value.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.784 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.784 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.4.attention.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.784 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.784 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.784 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.785 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.4.intermediate.dense.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.785 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.4.output.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.785 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.4.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.785 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.785 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.785 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.5.attention.self.query.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.785 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.5.attention.self.query.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.785 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.5.attention.self.key.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.785 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.5.attention.self.key.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.786 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.5.attention.self.value.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.786 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.5.attention.self.value.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.786 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.786 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.5.attention.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.786 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.787 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.787 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.787 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.5.intermediate.dense.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.787 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.5.output.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.788 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.5.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.788 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.788 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.788 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.6.attention.self.query.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.788 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.6.attention.self.query.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.788 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.6.attention.self.key.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.788 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.6.attention.self.key.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.789 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.6.attention.self.value.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.789 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.6.attention.self.value.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.789 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.789 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.6.attention.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.789 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.789 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.789 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.790 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.6.intermediate.dense.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.790 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.6.output.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.790 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.6.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.790 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.790 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.790 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.7.attention.self.query.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.790 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.7.attention.self.query.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.790 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.7.attention.self.key.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.791 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.7.attention.self.key.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.791 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.7.attention.self.value.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.791 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.7.attention.self.value.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.791 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.791 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.7.attention.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.791 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.791 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.791 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.792 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.7.intermediate.dense.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.792 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.7.output.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.792 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.7.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.792 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.792 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.792 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.8.attention.self.query.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.792 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.8.attention.self.query.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.793 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.8.attention.self.key.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.793 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.8.attention.self.key.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.793 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.8.attention.self.value.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.793 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.8.attention.self.value.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.793 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.794 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.8.attention.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.794 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.794 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.794 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.794 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.8.intermediate.dense.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.794 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.8.output.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.794 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.8.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.795 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.795 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.795 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.9.attention.self.query.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.795 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.9.attention.self.query.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.796 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.9.attention.self.key.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.796 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.9.attention.self.key.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.796 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.9.attention.self.value.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.796 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.9.attention.self.value.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.797 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.797 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.9.attention.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.797 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.797 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.797 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.797 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.9.intermediate.dense.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.797 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.9.output.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.798 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.9.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.798 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.798 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.798 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.10.attention.self.query.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.799 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.10.attention.self.query.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.799 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.10.attention.self.key.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.799 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.10.attention.self.key.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.799 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.10.attention.self.value.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.799 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.10.attention.self.value.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.799 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.799 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.10.attention.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.799 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.800 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.800 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.800 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.10.intermediate.dense.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.800 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.10.output.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.800 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.10.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.800 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.800 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.801 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.11.attention.self.query.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.801 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.11.attention.self.query.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.801 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.11.attention.self.key.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.801 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.11.attention.self.key.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.801 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.11.attention.self.value.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.801 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.11.attention.self.value.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.801 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.802 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.11.attention.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.802 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.802 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.802 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.802 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.11.intermediate.dense.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.802 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.11.output.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.802 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.11.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.802 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.802 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.802 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.12.attention.self.query.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.803 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.12.attention.self.query.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.803 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.12.attention.self.key.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.803 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.12.attention.self.key.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.803 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.12.attention.self.value.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.803 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.12.attention.self.value.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.803 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.804 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.12.attention.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.804 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.804 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.804 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.805 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.12.intermediate.dense.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.805 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.12.output.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.805 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.12.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.805 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.806 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.806 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.13.attention.self.query.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.806 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.13.attention.self.query.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.806 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.13.attention.self.key.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.806 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.13.attention.self.key.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.806 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.13.attention.self.value.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.807 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.13.attention.self.value.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.807 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.807 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.13.attention.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.807 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.807 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.807 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.808 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.13.intermediate.dense.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.808 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.13.output.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.808 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.13.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.808 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.808 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.808 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.14.attention.self.query.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.808 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.14.attention.self.query.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.808 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.14.attention.self.key.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.808 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.14.attention.self.key.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.809 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.14.attention.self.value.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.809 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.14.attention.self.value.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.809 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.809 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.14.attention.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.809 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.809 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.809 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.809 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.14.intermediate.dense.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.809 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.14.output.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.809 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.14.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.810 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.810 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.810 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.15.attention.self.query.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.810 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.15.attention.self.query.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.810 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.15.attention.self.key.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.810 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.15.attention.self.key.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.811 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.15.attention.self.value.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.811 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.15.attention.self.value.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.811 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.811 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.15.attention.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.811 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.811 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.811 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.812 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.15.intermediate.dense.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.812 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.15.output.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.812 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.15.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.812 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.812 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.812 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.16.attention.self.query.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.812 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.16.attention.self.query.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.812 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.16.attention.self.key.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.812 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.16.attention.self.key.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.813 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.16.attention.self.value.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.813 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.16.attention.self.value.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.813 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.813 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.16.attention.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.813 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.813 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.813 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.814 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.16.intermediate.dense.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.814 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.16.output.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.814 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.16.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.814 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.814 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.814 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.17.attention.self.query.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.814 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.17.attention.self.query.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.814 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.17.attention.self.key.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.814 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.17.attention.self.key.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.815 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.17.attention.self.value.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.815 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.17.attention.self.value.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.815 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.815 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.17.attention.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.815 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.815 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.815 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.815 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.17.intermediate.dense.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.816 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.17.output.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.816 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.17.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.816 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.816 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.816 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.18.attention.self.query.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.816 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.18.attention.self.query.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.816 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.18.attention.self.key.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.816 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.18.attention.self.key.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.817 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.18.attention.self.value.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.817 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.18.attention.self.value.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.817 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.817 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.18.attention.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.817 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.817 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.818 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.818 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.18.intermediate.dense.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.818 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.18.output.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.818 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.18.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.818 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.818 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.818 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.19.attention.self.query.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.818 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.19.attention.self.query.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.818 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.19.attention.self.key.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.819 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.19.attention.self.key.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.819 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.19.attention.self.value.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.819 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.19.attention.self.value.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.819 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.819 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.19.attention.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.819 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.819 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.820 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.820 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.19.intermediate.dense.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.820 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.19.output.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.820 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.19.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.820 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.820 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.821 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.20.attention.self.query.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.821 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.20.attention.self.query.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.821 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.20.attention.self.key.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.821 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.20.attention.self.key.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.821 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.20.attention.self.value.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.821 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.20.attention.self.value.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.821 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.821 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.20.attention.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.822 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.822 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.822 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.822 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.20.intermediate.dense.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.822 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.20.output.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.822 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.20.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.822 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.822 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.823 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.21.attention.self.query.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.823 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.21.attention.self.query.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.823 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.21.attention.self.key.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.823 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.21.attention.self.key.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.823 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.21.attention.self.value.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.823 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.21.attention.self.value.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.823 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.823 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.21.attention.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.823 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.824 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.824 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.824 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.21.intermediate.dense.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.824 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.21.output.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.824 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.21.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.824 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.824 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.825 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.22.attention.self.query.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.825 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.22.attention.self.query.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.825 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.22.attention.self.key.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.825 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.22.attention.self.key.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.825 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.22.attention.self.value.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.825 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.22.attention.self.value.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.825 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.825 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.22.attention.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.825 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.826 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.826 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.826 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.22.intermediate.dense.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.826 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.22.output.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.826 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.22.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.826 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.826 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.826 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.23.attention.self.query.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.827 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.23.attention.self.query.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.827 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.23.attention.self.key.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.827 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.23.attention.self.key.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.827 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.23.attention.self.value.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.827 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.23.attention.self.value.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.827 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.827 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.23.attention.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.827 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.828 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.828 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.828 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.23.intermediate.dense.bias count_params:4096\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.828 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.23.output.dense.weight count_params:4194304\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.828 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.23.output.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.828 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.828 algo-1:26 INFO hook.py:591] name:roberta.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.829 algo-1:26 INFO hook.py:591] name:classifier.dense.weight count_params:1048576\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.829 algo-1:26 INFO hook.py:591] name:classifier.dense.bias count_params:1024\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.829 algo-1:26 INFO hook.py:591] name:classifier.out_proj.weight count_params:2048\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.829 algo-1:26 INFO hook.py:591] name:classifier.out_proj.bias count_params:2\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.829 algo-1:26 INFO hook.py:593] Total Trainable Params: 355361794\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.829 algo-1:26 INFO hook.py:425] Monitoring the collections: losses\u001B[0m\n",
      "\u001B[34m[2022-06-13 07:24:01.832 algo-1:26 INFO hook.py:488] Hook is writing from the hook with pid: 26\u001B[0m\n",
      "\u001B[34m[INFO|trainer.py:1352] 2022-06-13 07:25:21,602 >> \u001B[0m\n",
      "\u001B[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001B[0m\n",
      "\u001B[34m{'train_runtime': 80.4643, 'train_samples_per_second': 1.616, 'epoch': 10.0}\u001B[0m\n",
      "\u001B[34m[INFO|trainer.py:1885] 2022-06-13 07:25:21,720 >> Saving model checkpoint to /opt/ml/model\u001B[0m\n",
      "\u001B[34m[INFO|configuration_utils.py:351] 2022-06-13 07:25:21,721 >> Configuration saved in /opt/ml/model/config.json\u001B[0m\n",
      "\u001B[34m[INFO|modeling_utils.py:889] 2022-06-13 07:25:25,854 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001B[0m\n",
      "\u001B[34m[INFO|tokenization_utils_base.py:1924] 2022-06-13 07:25:25,854 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001B[0m\n",
      "\u001B[34m[INFO|tokenization_utils_base.py:1930] 2022-06-13 07:25:25,855 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:907] 2022-06-13 07:25:25,974 >> ***** train metrics *****\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:912] 2022-06-13 07:25:25,974 >>   epoch                      =       10.0\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:912] 2022-06-13 07:25:25,974 >>   init_mem_cpu_alloc_delta   =      133MB\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:912] 2022-06-13 07:25:25,974 >>   init_mem_cpu_peaked_delta  =      962MB\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:912] 2022-06-13 07:25:25,974 >>   init_mem_gpu_alloc_delta   =     1355MB\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:912] 2022-06-13 07:25:25,974 >>   init_mem_gpu_peaked_delta  =        0MB\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:912] 2022-06-13 07:25:25,974 >>   train_mem_cpu_alloc_delta  =      210MB\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:912] 2022-06-13 07:25:25,975 >>   train_mem_cpu_peaked_delta =        0MB\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:912] 2022-06-13 07:25:25,975 >>   train_mem_gpu_alloc_delta  =     4066MB\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:912] 2022-06-13 07:25:25,975 >>   train_mem_gpu_peaked_delta =     1132MB\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:912] 2022-06-13 07:25:25,975 >>   train_runtime              = 0:01:20.46\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:912] 2022-06-13 07:25:25,975 >>   train_samples              =         50\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:912] 2022-06-13 07:25:25,975 >>   train_samples_per_second   =      1.616\u001B[0m\n",
      "\u001B[34m06/13/2022 07:25:25 - INFO - __main__ -   *** Evaluate ***\u001B[0m\n",
      "\u001B[34m[INFO|trainer.py:516] 2022-06-13 07:25:26,042 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1_key.\u001B[0m\n",
      "\u001B[34m[INFO|trainer.py:2115] 2022-06-13 07:25:26,045 >> ***** Running Evaluation *****\u001B[0m\n",
      "\u001B[34m[INFO|trainer.py:2117] 2022-06-13 07:25:26,046 >>   Num examples = 18\u001B[0m\n",
      "\u001B[34m[INFO|trainer.py:2120] 2022-06-13 07:25:26,046 >>   Batch size = 4\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:907] 2022-06-13 07:25:26,728 >> ***** eval metrics *****\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:912] 2022-06-13 07:25:26,728 >>   epoch                     =       10.0\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:912] 2022-06-13 07:25:26,728 >>   eval_accuracy             =        1.0\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:912] 2022-06-13 07:25:26,729 >>   eval_loss                 =        0.0\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:912] 2022-06-13 07:25:26,729 >>   eval_mem_cpu_alloc_delta  =        0MB\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:912] 2022-06-13 07:25:26,729 >>   eval_mem_cpu_peaked_delta =        0MB\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:912] 2022-06-13 07:25:26,729 >>   eval_mem_gpu_alloc_delta  =        0MB\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:912] 2022-06-13 07:25:26,729 >>   eval_mem_gpu_peaked_delta =       22MB\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:912] 2022-06-13 07:25:26,729 >>   eval_runtime              = 0:00:00.61\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:912] 2022-06-13 07:25:26,729 >>   eval_samples              =         18\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:912] 2022-06-13 07:25:26,729 >>   eval_samples_per_second   =     29.328\u001B[0m\n",
      "\u001B[34m06/13/2022 07:25:26 - INFO - __main__ -   *** Predict ***\u001B[0m\n",
      "\u001B[34m[INFO|trainer.py:516] 2022-06-13 07:25:26,797 >> The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1_key.\u001B[0m\n",
      "\u001B[34m[INFO|trainer.py:2115] 2022-06-13 07:25:26,800 >> ***** Running Prediction *****\u001B[0m\n",
      "\u001B[34m[INFO|trainer.py:2117] 2022-06-13 07:25:26,800 >>   Num examples = 18\u001B[0m\n",
      "\u001B[34m[INFO|trainer.py:2120] 2022-06-13 07:25:26,800 >>   Batch size = 4\u001B[0m\n",
      "\u001B[34m06/13/2022 07:25:27 - INFO - __main__ -   ***** Predict results None *****\u001B[0m\n",
      "\u001B[34m#0150 tables [00:00, ? tables/s]#015                            #015#0150 tables [00:00, ? tables/s]#015                            #015#0150 tables [00:00, ? tables/s]#015                            #015[INFO|file_utils.py:1532] 2022-06-13 07:23:15,255 >> https://huggingface.co/roberta-large/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpjxmn0j17\u001B[0m\n",
      "\u001B[34m#015Downloading:   0%|          | 0.00/482 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 482/482 [00:00<00:00, 582kB/s]\u001B[0m\n",
      "\u001B[34m[INFO|file_utils.py:1536] 2022-06-13 07:23:15,284 >> storing https://huggingface.co/roberta-large/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001B[0m\n",
      "\u001B[34m[INFO|file_utils.py:1544] 2022-06-13 07:23:15,284 >> creating metadata file for /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001B[0m\n",
      "\u001B[34m[INFO|configuration_utils.py:517] 2022-06-13 07:23:15,284 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001B[0m\n",
      "\u001B[34m[INFO|configuration_utils.py:553] 2022-06-13 07:23:15,285 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\u001B[0m\n",
      "\u001B[34m2022-06-13 07:25:28,241 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "  \"transformers_version\": \"4.6.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\u001B[0m\n",
      "\u001B[34m}\u001B[0m\n",
      "\u001B[34m[INFO|configuration_utils.py:517] 2022-06-13 07:23:15,310 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001B[0m\n",
      "\u001B[34m[INFO|configuration_utils.py:553] 2022-06-13 07:23:15,311 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.6.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\u001B[0m\n",
      "\u001B[34m}\u001B[0m\n",
      "\u001B[34m[INFO|file_utils.py:1532] 2022-06-13 07:23:15,336 >> https://huggingface.co/roberta-large/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpz3t2jcat\u001B[0m\n",
      "\u001B[34m#015Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]#015Downloading: 100%|██████████| 899k/899k [00:00<00:00, 38.0MB/s]\u001B[0m\n",
      "\u001B[34m[INFO|file_utils.py:1536] 2022-06-13 07:23:15,412 >> storing https://huggingface.co/roberta-large/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\u001B[0m\n",
      "\u001B[34m[INFO|file_utils.py:1544] 2022-06-13 07:23:15,412 >> creating metadata file for /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\u001B[0m\n",
      "\u001B[34m[INFO|file_utils.py:1532] 2022-06-13 07:23:15,440 >> https://huggingface.co/roberta-large/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp217uk4hw\u001B[0m\n",
      "\u001B[34m#015Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]#015Downloading: 100%|██████████| 456k/456k [00:00<00:00, 31.4MB/s]\u001B[0m\n",
      "\u001B[34m[INFO|file_utils.py:1536] 2022-06-13 07:23:15,500 >> storing https://huggingface.co/roberta-large/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\u001B[0m\n",
      "\u001B[34m[INFO|file_utils.py:1544] 2022-06-13 07:23:15,500 >> creating metadata file for /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\u001B[0m\n",
      "\u001B[34m[INFO|file_utils.py:1532] 2022-06-13 07:23:15,524 >> https://huggingface.co/roberta-large/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpqo6eraz2\u001B[0m\n",
      "\u001B[34m#015Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]#015Downloading: 100%|██████████| 1.36M/1.36M [00:00<00:00, 41.5MB/s]\u001B[0m\n",
      "\u001B[34m[INFO|file_utils.py:1536] 2022-06-13 07:23:15,622 >> storing https://huggingface.co/roberta-large/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\u001B[0m\n",
      "\u001B[34m[INFO|file_utils.py:1544] 2022-06-13 07:23:15,622 >> creating metadata file for /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\u001B[0m\n",
      "\u001B[34m[INFO|tokenization_utils_base.py:1717] 2022-06-13 07:23:15,711 >> loading file https://huggingface.co/roberta-large/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\u001B[0m\n",
      "\u001B[34m[INFO|tokenization_utils_base.py:1717] 2022-06-13 07:23:15,712 >> loading file https://huggingface.co/roberta-large/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\u001B[0m\n",
      "\u001B[34m[INFO|tokenization_utils_base.py:1717] 2022-06-13 07:23:15,712 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\u001B[0m\n",
      "\u001B[34m[INFO|tokenization_utils_base.py:1717] 2022-06-13 07:23:15,712 >> loading file https://huggingface.co/roberta-large/resolve/main/added_tokens.json from cache at None\u001B[0m\n",
      "\u001B[34m[INFO|tokenization_utils_base.py:1717] 2022-06-13 07:23:15,712 >> loading file https://huggingface.co/roberta-large/resolve/main/special_tokens_map.json from cache at None\u001B[0m\n",
      "\u001B[34m[INFO|tokenization_utils_base.py:1717] 2022-06-13 07:23:15,712 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer_config.json from cache at None\u001B[0m\n",
      "\u001B[34m[INFO|configuration_utils.py:517] 2022-06-13 07:23:15,830 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\u001B[0m\n",
      "\u001B[34m[INFO|configuration_utils.py:553] 2022-06-13 07:23:15,831 >> Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"\\u6b63\\u786e\",\n",
      "    \"1\": \"\\u9519\\u8bef\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"\\u6b63\\u786e\": \"0\",\n",
      "    \"\\u9519\\u8bef\": \"1\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.6.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\u001B[0m\n",
      "\u001B[34m}\u001B[0m\n",
      "\u001B[34m[INFO|file_utils.py:1532] 2022-06-13 07:23:15,860 >> https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp9_1pvvyq\u001B[0m\n",
      "\u001B[34m#015Downloading:   0%|          | 0.00/1.43G [00:00<?, ?B/s]#015Downloading:   0%|          | 4.61M/1.43G [00:00<00:30, 46.1MB/s]#015Downloading:   1%|          | 9.25M/1.43G [00:00<00:30, 46.2MB/s]#015Downloading:   1%|          | 14.1M/1.43G [00:00<00:30, 46.9MB/s]#015Downloading:   1%|▏         | 19.0M/1.43G [00:00<00:29, 47.5MB/s]#015Downloading:   2%|▏         | 24.0M/1.43G [00:00<00:29, 48.2MB/s]#015Downloading:   2%|▏         | 29.0M/1.43G [00:00<00:28, 48.8MB/s]#015Downloading:   2%|▏         | 34.0M/1.43G [00:00<00:28, 49.1MB/s]#015Downloading:   3%|▎         | 39.1M/1.43G [00:00<00:28, 49.4MB/s]#015Downloading:   3%|▎         | 44.1M/1.43G [00:00<00:27, 49.7MB/s]#015Downloading:   3%|▎         | 49.1M/1.43G [00:01<00:27, 49.9MB/s]#015Downloading:   4%|▍         | 54.1M/1.43G [00:01<00:27, 49.9MB/s]#015Downloading:   4%|▍         | 59.1M/1.43G [00:01<00:27, 50.0MB/s]#015Downloading:   5%|▍         | 64.2M/1.43G [00:01<00:27, 50.2MB/s]#015Downloading:   5%|▍         | 69.3M/1.43G [00:01<00:26, 50.3MB/s]#015Downloading:   5%|▌         | 74.3M/1.43G [00:01<00:26, 50.4MB/s]#015Downloading:   6%|▌         | 79.3M/1.43G [00:01<00:26, 50.2MB/s]#015Downloading:   6%|▌         | 84.3M/1.43G [00:01<00:26, 50.2MB/s]#015Downloading:   6%|▋         | 89.4M/1.43G [00:01<00:27, 49.4MB/s]#015Downloading:   7%|▋         | 94.3M/1.43G [00:01<00:27, 49.0MB/s]#015Downloading:   7%|▋         | 99.3M/1.43G [00:02<00:26, 49.4MB/s]#015Downloading:   7%|▋         | 104M/1.43G [00:02<00:26, 49.6MB/s] #015Downloading:   8%|▊         | 109M/1.43G [00:02<00:26, 49.9MB/s]#015Downloading:   8%|▊         | 114M/1.43G [00:02<00:26, 50.1MB/s]#015Downloading:   8%|▊         | 119M/1.43G [00:02<00:26, 49.8MB/s]#015Downloading:   9%|▊         | 124M/1.43G [00:02<00:26, 49.3MB/s]#015Downloading:   9%|▉         | 129M/1.43G [00:02<00:26, 49.5MB/s]#015Downloading:   9%|▉         | 134M/1.43G [00:02<00:26, 49.4MB/s]#015Downloading:  10%|▉         | 139M/1.43G [00:02<00:26, 49.1MB/s]#015Downloading:  10%|█         | 144M/1.43G [00:02<00:26, 48.7MB/s]#015Downloading:  10%|█         | 149M/1.43G [00:03<00:26, 48.9MB/s]#015Downloading:  11%|█         | 154M/1.43G [00:03<00:25, 49.1MB/s]#015Downloading:  11%|█         | 159M/1.43G [00:03<00:25, 49.5MB/s]#015Downloading:  12%|█▏        | 164M/1.43G [00:03<00:25, 49.7MB/s]#015Downloading:  12%|█▏        | 169M/1.43G [00:03<00:25, 50.0MB/s]#015Downloading:  12%|█▏        | 174M/1.43G [00:03<00:24, 50.2MB/s]#015Downloading:  13%|█▎        | 179M/1.43G [00:03<00:24, 49.9MB/s]#015Downloading:  13%|█▎        | 184M/1.43G [00:03<00:25, 49.6MB/s]#015Downloading:  13%|█▎        | 189M/1.43G [00:03<00:24, 49.6MB/s]#015Downloading:  14%|█▎        | 194M/1.43G [00:03<00:24, 49.3MB/s]#015Downloading:  14%|█▍        | 199M/1.43G [00:04<00:24, 49.3MB/s]#015Downloading:  14%|█▍        | 204M/1.43G [00:04<00:24, 49.3MB/s]#015Downloading:  15%|█▍        | 209M/1.43G [00:04<00:24, 49.4MB/s]#015Downloading:  15%|█▌        | 214M/1.43G [00:04<00:24, 49.2MB/s]#015Downloading:  15%|█▌        | 219M/1.43G [00:04<00:24, 49.5MB/s]#015Downloading:  16%|█▌        | 224M/1.43G [00:04<00:24, 49.7MB/s]#015Downloading:  16%|█▌        | 229M/1.43G [00:04<00:23, 50.0MB/s]#015Downloading:  16%|█▋        | 234M/1.43G [00:04<00:23, 50.1MB/s]#015Downloading:  17%|█▋        | 239M/1.43G [00:04<00:23, 50.3MB/s]#015Downloading:  17%|█▋        | 244M/1.43G [00:04<00:23, 50.2MB/s]#015Downloading:  17%|█▋        | 249M/1.43G [00:05<00:23, 50.3MB/s]#015Downloading:  18%|█▊        | 254M/1.43G [00:05<00:23, 50.3MB/s]#015Downloading:  18%|█▊        | 259M/1.43G [00:05<00:23, 50.2MB/s]#015Downloading:  19%|█▊        | 265M/1.43G [00:05<00:23, 50.5MB/s]#015Downloading:  19%|█▉        | 270M/1.43G [00:05<00:22, 50.4MB/s]#015Downloading:  19%|█▉        | 275M/1.43G [00:05<00:22, 50.4MB/s]#015Downloading:  20%|█▉        | 280M/1.43G [00:05<00:22, 50.3MB/s]#015Downloading:  20%|█▉        | 285M/1.43G [00:05<00:22, 50.3MB/s]#015Downloading:  20%|██        | 290M/1.43G [00:05<00:22, 50.4MB/s]#015Downloading:  21%|██        | 295M/1.43G [00:05<00:22, 50.3MB/s]#015Downloading:  21%|██        | 300M/1.43G [00:06<00:22, 49.8MB/s]#015Downloading:  21%|██▏       | 305M/1.43G [00:06<00:23, 48.3MB/s]#015Downloading:  22%|██▏       | 310M/1.43G [00:06<00:22, 48.6MB/s]#015Downloading:  22%|██▏       | 315M/1.43G [00:06<00:22, 49.3MB/s]#015Downloading:  22%|██▏       | 320M/1.43G [00:06<00:22, 49.5MB/s]#015Downloading:  23%|██▎       | 325M/1.43G [00:06<00:22, 49.8MB/s]#015Downloading:  23%|██▎       | 330M/1.43G [00:06<00:21, 49.9MB/s]#015Downloading:  23%|██▎       | 335M/1.43G [00:06<00:21, 49.9MB/s]#015Downloading:  24%|██▍       | 340M/1.43G [00:06<00:22, 48.7MB/s]#015Downloading:  24%|██▍       | 345M/1.43G [00:06<00:22, 48.1MB/s]#015Downloading:  25%|██▍       | 350M/1.43G [00:07<00:23, 45.9MB/s]#015Downloading:  25%|██▍       | 354M/1.43G [00:07<00:24, 44.5MB/s]#015Downloading:  25%|██▌       | 359M/1.43G [00:07<00:23, 46.2MB/s]#015Downloading:  26%|██▌       | 364M/1.43G [00:07<00:22, 47.4MB/s]#015Downloading:  26%|██▌       | 369M/1.43G [00:07<00:23, 45.7MB/s]#015Downloading:  26%|██▌       | 374M/1.43G [00:07<00:23, 45.6MB/s]#015Downloading:  27%|██▋       | 379M/1.43G [00:07<00:22, 47.3MB/s]#015Downloading:  27%|██▋       | 384M/1.43G [00:07<00:21, 48.8MB/s]#015Downloading:  27%|██▋       | 389M/1.43G [00:07<00:20, 49.7MB/s]#015Downloading:  28%|██▊       | 395M/1.43G [00:07<00:20, 50.4MB/s]#015Downloading:  28%|██▊       | 400M/1.43G [00:08<00:20, 51.0MB/s]#015Downloading:  28%|██▊       | 405M/1.43G [00:08<00:19, 51.4MB/s]#015Downloading:  29%|██▉       | 410M/1.43G [00:08<00:19, 51.6MB/s]#015Downloading:  29%|██▉       | 415M/1.43G [00:08<00:19, 51.6MB/s]#015Downloading:  29%|██▉       | 421M/1.43G [00:08<00:19, 51.3MB/s]#015Downloading:  30%|██▉       | 426M/1.43G [00:08<00:20, 48.6MB/s]#015Downloading:  30%|███       | 431M/1.43G [00:08<00:20, 49.6MB/s]#015Downloading:  31%|███       | 436M/1.43G [00:08<00:19, 50.4MB/s]#015Downloading:  31%|███       | 441M/1.43G [00:08<00:19, 51.0MB/s]#015Downloading:  31%|███▏      | 447M/1.43G [00:09<00:19, 51.3MB/s]#015Downloading:  32%|███▏      | 452M/1.43G [00:09<00:18, 51.4MB/s]#015Downloading:  32%|███▏      | 457M/1.43G [00:09<00:18, 51.8MB/s]#015Downloading:  32%|███▏      | 462M/1.43G [00:09<00:18, 52.0MB/s]#015Downloading:  33%|███▎      | 468M/1.43G [00:09<00:18, 52.0MB/s]#015Downloading:  33%|███▎      | 473M/1.43G [00:09<00:18, 51.9MB/s]#015Downloading:  34%|███▎      | 478M/1.43G [00:09<00:18, 52.0MB/s]#015Downloading:  34%|███▍      | 483M/1.43G [00:09<00:18, 52.1MB/s]#015Downloading:  34%|███▍      | 488M/1.43G [00:09<00:17, 52.1MB/s]#015Downloading:  35%|███▍      | 494M/1.43G [00:09<00:17, 52.3MB/s]#015Downloading:  35%|███▍      | 499M/1.43G [00:10<00:18, 50.8MB/s]#015Downloading:  35%|███▌      | 504M/1.43G [00:10<00:18, 51.1MB/s]#015Downloading:  36%|███▌      | 509M/1.43G [00:10<00:17, 51.3MB/s]#015Downloading:  36%|███▌      | 515M/1.43G [00:10<00:17, 51.6MB/s]#015Downloading:  36%|███▋      | 520M/1.43G [00:10<00:17, 51.4MB/s]#015Downloading:  37%|███▋      | 525M/1.43G [00:10<00:17, 51.4MB/s]#015Downloading:  37%|███▋      | 530M/1.43G [00:10<00:17, 51.5MB/s]#015Downloading:  38%|███▊      | 535M/1.43G [00:10<00:17, 51.8MB/s]#015Downloading:  38%|███▊      | 541M/1.43G [00:10<00:16, 52.2MB/s]#015Downloading:  38%|███▊      | 546M/1.43G [00:10<00:16, 52.3MB/s]#015Downloading:  39%|███▊      | 551M/1.43G [00:11<00:16, 52.1MB/s]#015Downloading:  39%|███▉      | 556M/1.43G [00:11<00:16, 51.9MB/s]#015Downloading:  39%|███▉      | 562M/1.43G [00:11<00:16, 52.1MB/s]#015Downloading:  40%|███▉      | 567M/1.43G [00:11<00:16, 52.3MB/s]#015Downloading:  40%|████      | 572M/1.43G [00:11<00:16, 52.4MB/s]#015Downloading:  40%|████      | 577M/1.43G [00:11<00:16, 52.1MB/s]#015Downloading:  41%|████      | 583M/1.43G [00:11<00:16, 51.9MB/s]#015Downloading:  41%|████      | 588M/1.43G [00:11<00:16, 51.7MB/s]#015Downloading:  42%|████▏     | 593M/1.43G [00:11<00:16, 51.4MB/s]#015Downloading:  42%|████▏     | 598M/1.43G [00:11<00:16, 51.6MB/s]#015Downloading:  42%|████▏     | 603M/1.43G [00:12<00:15, 51.8MB/s]#015Downloading:  43%|████▎     | 609M/1.43G [00:12<00:15, 51.9MB/s]#015Downloading:  43%|████▎     | 614M/1.43G [00:12<00:15, 52.0MB/s]#015Downloading:  43%|████▎     | 619M/1.43G [00:12<00:15, 52.0MB/s]#015Downloading:  44%|████▍     | 624M/1.43G [00:12<00:15, 51.1MB/s]#015Downloading:  44%|████▍     | 629M/1.43G [00:12<00:15, 51.4MB/s]#015Downloading:  45%|████▍     | 635M/1.43G [00:12<00:15, 51.6MB/s]#015Downloading:  45%|████▍     | 640M/1.43G [00:12<00:15, 51.8MB/s]#015Downloading:  45%|████▌     | 645M/1.43G [00:12<00:15, 52.0MB/s]#015Downloading:  46%|████▌     | 650M/1.43G [00:12<00:14, 52.0MB/s]#015Downloading:  46%|████▌     | 655M/1.43G [00:13<00:14, 51.9MB/s]#015Downloading:  46%|████▋     | 661M/1.43G [00:13<00:14, 51.9MB/s]#015Downloading:  47%|████▋     | 666M/1.43G [00:13<00:14, 52.0MB/s]#015Downloading:  47%|████▋     | 671M/1.43G [00:13<00:14, 51.9MB/s]#015Downloading:  47%|████▋     | 676M/1.43G [00:13<00:14, 51.6MB/s]#015Downloading:  48%|████▊     | 681M/1.43G [00:13<00:14, 51.7MB/s]#015Downloading:  48%|████▊     | 687M/1.43G [00:13<00:14, 51.8MB/s]#015Downloading:  49%|████▊     | 692M/1.43G [00:13<00:14, 52.0MB/s]#015Downloading:  49%|████▉     | 697M/1.43G [00:13<00:14, 52.0MB/s]#015Downloading:  49%|████▉     | 702M/1.43G [00:13<00:13, 52.1MB/s]#015Downloading:  50%|████▉     | 708M/1.43G [00:14<00:13, 52.0MB/s]#015Downloading:  50%|████▉     | 713M/1.43G [00:14<00:13, 52.1MB/s]#015Downloading:  50%|█████     | 718M/1.43G [00:14<00:13, 52.2MB/s]#015Downloading:  51%|█████     | 723M/1.43G [00:14<00:13, 52.2MB/s]#015Downloading:  51%|█████     | 728M/1.43G [00:14<00:13, 52.1MB/s]#015Downloading:  51%|█████▏    | 734M/1.43G [00:14<00:13, 52.1MB/s]#015Downloading:  52%|█████▏    | 739M/1.43G [00:14<00:13, 52.2MB/s]#015Downloading:  52%|█████▏    | 744M/1.43G [00:14<00:13, 52.2MB/s]#015Downloading:  53%|█████▎    | 749M/1.43G [00:14<00:12, 52.3MB/s]#015Downloading:  53%|█████▎    | 755M/1.43G [00:14<00:12, 52.4MB/s]#015Downloading:  53%|█████▎    | 760M/1.43G [00:15<00:12, 52.4MB/s]#015Downloading:  54%|█████▎    | 765M/1.43G [00:15<00:13, 50.5MB/s]#015Downloading:  54%|█████▍    | 770M/1.43G [00:15<00:13, 47.5MB/s]#015Downloading:  54%|█████▍    | 775M/1.43G [00:15<00:14, 45.5MB/s]#015Downloading:  55%|█████▍    | 780M/1.43G [00:15<00:14, 46.0MB/s]#015Downloading:  55%|█████▌    | 785M/1.43G [00:15<00:13, 47.5MB/s]#015Downloading:  55%|█████▌    | 790M/1.43G [00:15<00:13, 48.8MB/s]#015Downloading:  56%|█████▌    | 795M/1.43G [00:15<00:12, 49.7MB/s]#015Downloading:  56%|█████▌    | 800M/1.43G [00:15<00:12, 50.3MB/s]#015Downloading:  56%|█████▋    | 805M/1.43G [00:16<00:12, 50.1MB/s]#015Downloading:  57%|█████▋    | 811M/1.43G [00:16<00:12, 47.5MB/s]#015Downloading:  57%|█████▋    | 816M/1.43G [00:16<00:12, 48.4MB/s]#015Downloading:  58%|█████▊    | 821M/1.43G [00:16<00:12, 48.9MB/s]#015Downloading:  58%|█████▊    | 826M/1.43G [00:16<00:12, 49.6MB/s]#015Downloading:  58%|█████▊    | 831M/1.43G [00:16<00:11, 50.4MB/s]#015Downloading:  59%|█████▊    | 836M/1.43G [00:16<00:11, 51.0MB/s]#015Downloading:  59%|█████▉    | 841M/1.43G [00:16<00:11, 51.4MB/s]#015Downloading:  59%|█████▉    | 847M/1.43G [00:16<00:11, 51.4MB/s]#015Downloading:  60%|█████▉    | 852M/1.43G [00:16<00:11, 49.4MB/s]#015Downloading:  60%|██████    | 857M/1.43G [00:17<00:12, 46.6MB/s]#015Downloading:  60%|██████    | 861M/1.43G [00:17<00:12, 44.9MB/s]#015Downloading:  61%|██████    | 866M/1.43G [00:17<00:12, 43.7MB/s]#015Downloading:  61%|██████    | 871M/1.43G [00:17<00:12, 45.3MB/s]#015Downloading:  61%|██████▏   | 876M/1.43G [00:17<00:11, 47.0MB/s]#015Downloading:  62%|██████▏   | 881M/1.43G [00:17<00:11, 48.5MB/s]#015Downloading:  62%|██████▏   | 887M/1.43G [00:17<00:10, 49.6MB/s]#015Downloading:  63%|██████▎   | 892M/1.43G [00:17<00:10, 50.4MB/s]#015Downloading:  63%|██████▎   | 897M/1.43G [00:17<00:10, 50.9MB/s]#015Downloading:  63%|██████▎   | 902M/1.43G [00:17<00:10, 51.4MB/s]#015Downloading:  64%|██████▎   | 907M/1.43G [00:18<00:10, 51.6MB/s]#015Downloading:  64%|██████▍   | 913M/1.43G [00:18<00:10, 50.3MB/s]#015Downloading:  64%|██████▍   | 918M/1.43G [00:18<00:10, 50.6MB/s]#015Downloading:  65%|██████▍   | 923M/1.43G [00:18<00:09, 50.8MB/s]#015Downloading:  65%|██████▌   | 928M/1.43G [00:18<00:09, 51.1MB/s]#015Downloading:  65%|██████▌   | 933M/1.43G [00:18<00:09, 51.5MB/s]#015Downloading:  66%|██████▌   | 939M/1.43G [00:18<00:09, 51.8MB/s]#015Downloading:  66%|██████▌   | 944M/1.43G [00:18<00:09, 52.1MB/s]#015Downloading:  67%|██████▋   | 949M/1.43G [00:18<00:09, 52.1MB/s]#015Downloading:  67%|██████▋   | 954M/1.43G [00:18<00:09, 51.9MB/s]#015Downloading:  67%|██████▋   | 959M/1.43G [00:19<00:09, 51.5MB/s]#015Downloading:  68%|██████▊   | 965M/1.43G [00:19<00:08, 51.5MB/s]#015Downloading:  68%|██████▊   | 970M/1.43G [00:19<00:09, 50.4MB/s]#015Downloading:  68%|██████▊   | 975M/1.43G [00:19<00:08, 50.8MB/s]#015Downloading:  69%|██████▊   | 980M/1.43G [00:19<00:08, 51.1MB/s]#015Downloading:  69%|██████▉   | 985M/1.43G [00:19<00:08, 51.0MB/s]#015Downloading:  69%|██████▉   | 990M/1.43G [00:19<00:08, 49.5MB/s]#015Downloading:  70%|██████▉   | 996M/1.43G [00:19<00:08, 50.2MB/s]#015Downloading:  70%|███████   | 1.00G/1.43G [00:19<00:08, 50.9MB/s]#015Downloading:  71%|███████   | 1.01G/1.43G [00:20<00:08, 50.9MB/s]#015Downloading:  71%|███████   | 1.01G/1.43G [00:20<00:08, 51.3MB/s]#015Downloading:  71%|███████▏  | 1.02G/1.43G [00:20<00:07, 51.3MB/s]#015Downloading:  72%|███████▏  | 1.02G/1.43G [00:20<00:07, 51.4MB/s]#015Downloading:  72%|███████▏  | 1.03G/1.43G [00:20<00:07, 51.2MB/s]#015Downloading:  72%|███████▏  | 1.03G/1.43G [00:20<00:07, 51.2MB/s]#015Downloading:  73%|███████▎  | 1.04G/1.43G [00:20<00:07, 51.4MB/s]#015Downloading:  73%|███████▎  | 1.04G/1.43G [00:20<00:07, 51.8MB/s]#015Downloading:  73%|███████▎  | 1.05G/1.43G [00:20<00:07, 51.7MB/s]#015Downloading:  74%|███████▍  | 1.05G/1.43G [00:20<00:07, 51.7MB/s]#015Downloading:  74%|███████▍  | 1.06G/1.43G [00:21<00:07, 51.6MB/s]#015Downloading:  75%|███████▍  | 1.06G/1.43G [00:21<00:07, 51.6MB/s]#015Downloading:  75%|███████▍  | 1.07G/1.43G [00:21<00:06, 51.8MB/s]#015Downloading:  75%|███████▌  | 1.07G/1.43G [00:21<00:06, 51.9MB/s]#015Downloading:  76%|███████▌  | 1.08G/1.43G [00:21<00:06, 51.7MB/s]#015Downloading:  76%|███████▌  | 1.08G/1.43G [00:21<00:06, 51.9MB/s]#015Downloading:  76%|███████▋  | 1.09G/1.43G [00:21<00:06, 52.0MB/s]#015Downloading:  77%|███████▋  | 1.09G/1.43G [00:21<00:06, 51.8MB/s]#015Downloading:  77%|███████▋  | 1.10G/1.43G [00:21<00:06, 51.6MB/s]#015Downloading:  77%|███████▋  | 1.10G/1.43G [00:21<00:06, 51.5MB/s]#015Downloading:  78%|███████▊  | 1.11G/1.43G [00:22<00:06, 51.1MB/s]#015Downloading:  78%|███████▊  | 1.11G/1.43G [00:22<00:06, 50.9MB/s]#015Downloading:  79%|███████▊  | 1.12G/1.43G [00:22<00:06, 50.1MB/s]#015Downloading:  79%|███████▉  | 1.12G/1.43G [00:22<00:06, 50.1MB/s]#015Downloading:  79%|███████▉  | 1.13G/1.43G [00:22<00:06, 42.7MB/s]#015\u001B[0m\n",
      "\u001B[34mDownloading:  80%|███████▉  | 1.13G/1.43G [00:22<00:06, 44.5MB/s]#015Downloading:  80%|███████▉  | 1.14G/1.43G [00:22<00:06, 46.1MB/s]#015Downloading:  80%|████████  | 1.14G/1.43G [00:22<00:05, 47.2MB/s]#015Downloading:  81%|████████  | 1.15G/1.43G [00:22<00:05, 48.1MB/s]#015Downloading:  81%|████████  | 1.15G/1.43G [00:22<00:05, 48.5MB/s]#015Downloading:  81%|████████▏ | 1.16G/1.43G [00:23<00:05, 49.0MB/s]#015Downloading:  82%|████████▏ | 1.16G/1.43G [00:23<00:05, 49.2MB/s]#015Downloading:  82%|████████▏ | 1.17G/1.43G [00:23<00:05, 49.5MB/s]#015Downloading:  82%|████████▏ | 1.17G/1.43G [00:23<00:05, 49.8MB/s]#015Downloading:  83%|████████▎ | 1.18G/1.43G [00:23<00:04, 50.0MB/s]#015Downloading:  83%|████████▎ | 1.18G/1.43G [00:23<00:04, 50.0MB/s]#015Downloading:  83%|████████▎ | 1.19G/1.43G [00:23<00:04, 50.1MB/s]#015Downloading:  84%|████████▍ | 1.20G/1.43G [00:23<00:04, 50.3MB/s]#015Downloading:  84%|████████▍ | 1.20G/1.43G [00:23<00:04, 50.4MB/s]#015Downloading:  85%|████████▍ | 1.21G/1.43G [00:23<00:04, 50.5MB/s]#015Downloading:  85%|████████▍ | 1.21G/1.43G [00:24<00:04, 50.7MB/s]#015Downloading:  85%|████████▌ | 1.22G/1.43G [00:24<00:04, 50.1MB/s]#015Downloading:  86%|████████▌ | 1.22G/1.43G [00:24<00:04, 46.4MB/s]#015Downloading:  86%|████████▌ | 1.23G/1.43G [00:24<00:04, 44.1MB/s]#015Downloading:  86%|████████▌ | 1.23G/1.43G [00:24<00:04, 42.6MB/s]#015Downloading:  87%|████████▋ | 1.23G/1.43G [00:24<00:04, 43.0MB/s]#015Downloading:  87%|████████▋ | 1.24G/1.43G [00:24<00:04, 44.9MB/s]#015Downloading:  87%|████████▋ | 1.24G/1.43G [00:24<00:03, 46.0MB/s]#015Downloading:  88%|████████▊ | 1.25G/1.43G [00:24<00:03, 47.5MB/s]#015Downloading:  88%|████████▊ | 1.25G/1.43G [00:25<00:03, 48.6MB/s]#015Downloading:  88%|████████▊ | 1.26G/1.43G [00:25<00:03, 49.4MB/s]#015Downloading:  89%|████████▊ | 1.26G/1.43G [00:25<00:03, 50.1MB/s]#015Downloading:  89%|████████▉ | 1.27G/1.43G [00:25<00:03, 50.6MB/s]#015Downloading:  89%|████████▉ | 1.27G/1.43G [00:25<00:02, 50.9MB/s]#015Downloading:  90%|████████▉ | 1.28G/1.43G [00:25<00:02, 51.1MB/s]#015Downloading:  90%|█████████ | 1.29G/1.43G [00:25<00:02, 51.2MB/s]#015Downloading:  90%|█████████ | 1.29G/1.43G [00:25<00:02, 51.4MB/s]#015Downloading:  91%|█████████ | 1.30G/1.43G [00:25<00:02, 51.5MB/s]#015Downloading:  91%|█████████ | 1.30G/1.43G [00:25<00:02, 51.5MB/s]#015Downloading:  92%|█████████▏| 1.31G/1.43G [00:26<00:02, 51.6MB/s]#015Downloading:  92%|█████████▏| 1.31G/1.43G [00:26<00:02, 51.5MB/s]#015Downloading:  92%|█████████▏| 1.32G/1.43G [00:26<00:02, 51.1MB/s]#015Downloading:  93%|█████████▎| 1.32G/1.43G [00:26<00:02, 51.2MB/s]#015Downloading:  93%|█████████▎| 1.33G/1.43G [00:26<00:01, 51.1MB/s]#015Downloading:  93%|█████████▎| 1.33G/1.43G [00:26<00:01, 51.2MB/s]#015Downloading:  94%|█████████▎| 1.34G/1.43G [00:26<00:01, 51.3MB/s]#015Downloading:  94%|█████████▍| 1.34G/1.43G [00:26<00:01, 51.5MB/s]#015Downloading:  94%|█████████▍| 1.35G/1.43G [00:26<00:01, 50.4MB/s]#015Downloading:  95%|█████████▍| 1.35G/1.43G [00:26<00:01, 50.7MB/s]#015Downloading:  95%|█████████▌| 1.36G/1.43G [00:27<00:01, 51.0MB/s]#015Downloading:  96%|█████████▌| 1.36G/1.43G [00:27<00:01, 51.0MB/s]#015Downloading:  96%|█████████▌| 1.37G/1.43G [00:27<00:01, 51.1MB/s]#015Downloading:  96%|█████████▋| 1.37G/1.43G [00:27<00:01, 51.3MB/s]#015Downloading:  97%|█████████▋| 1.38G/1.43G [00:27<00:00, 50.4MB/s]#015Downloading:  97%|█████████▋| 1.38G/1.43G [00:30<00:07, 5.78MB/s]#015Downloading:  97%|█████████▋| 1.39G/1.43G [00:30<00:04, 7.86MB/s]#015Downloading:  98%|█████████▊| 1.39G/1.43G [00:30<00:03, 10.5MB/s]#015Downloading:  98%|█████████▊| 1.40G/1.43G [00:30<00:02, 13.8MB/s]#015Downloading:  98%|█████████▊| 1.40G/1.43G [00:30<00:01, 17.7MB/s]#015Downloading:  99%|█████████▉| 1.41G/1.43G [00:30<00:00, 22.0MB/s]#015Downloading:  99%|█████████▉| 1.41G/1.43G [00:30<00:00, 26.5MB/s]#015Downloading:  99%|█████████▉| 1.42G/1.43G [00:30<00:00, 31.0MB/s]#015Downloading: 100%|█████████▉| 1.42G/1.43G [00:30<00:00, 35.1MB/s]#015Downloading: 100%|██████████| 1.43G/1.43G [00:30<00:00, 46.0MB/s]\u001B[0m\n",
      "\u001B[34m[INFO|file_utils.py:1536] 2022-06-13 07:23:46,885 >> storing https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\u001B[0m\n",
      "\u001B[34m[INFO|file_utils.py:1544] 2022-06-13 07:23:46,885 >> creating metadata file for /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\u001B[0m\n",
      "\u001B[34m[INFO|modeling_utils.py:1155] 2022-06-13 07:23:46,886 >> loading weights file https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\u001B[0m\n",
      "\u001B[34m[WARNING|modeling_utils.py:1331] 2022-06-13 07:23:56,726 >> Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.dense.bias']\u001B[0m\n",
      "\u001B[34m- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001B[0m\n",
      "\u001B[34m- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001B[0m\n",
      "\u001B[34m[WARNING|modeling_utils.py:1342] 2022-06-13 07:23:56,727 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\u001B[0m\n",
      "\u001B[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001B[0m\n",
      "\u001B[34m#015  0%|          | 0/1 [00:00<?, ?ba/s]#015100%|██████████| 1/1 [00:00<00:00, 48.22ba/s]\u001B[0m\n",
      "\u001B[34m#015  0%|          | 0/1 [00:00<?, ?ba/s]#015100%|██████████| 1/1 [00:00<00:00, 272.52ba/s]\u001B[0m\n",
      "\u001B[34m#015  0%|          | 0/1 [00:00<?, ?ba/s]#015100%|██████████| 1/1 [00:00<00:00, 278.25ba/s]\u001B[0m\n",
      "\u001B[34m[INFO|trainer.py:516] 2022-06-13 07:24:01,113 >> The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1_key.\u001B[0m\n",
      "\u001B[34m[INFO|trainer.py:1156] 2022-06-13 07:24:01,137 >> ***** Running training *****\u001B[0m\n",
      "\u001B[34m[INFO|trainer.py:1157] 2022-06-13 07:24:01,137 >>   Num examples = 50\u001B[0m\n",
      "\u001B[34m[INFO|trainer.py:1158] 2022-06-13 07:24:01,137 >>   Num Epochs = 10\u001B[0m\n",
      "\u001B[34m[INFO|trainer.py:1159] 2022-06-13 07:24:01,137 >>   Instantaneous batch size per device = 4\u001B[0m\n",
      "\u001B[34m[INFO|trainer.py:1160] 2022-06-13 07:24:01,138 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\u001B[0m\n",
      "\u001B[34m[INFO|trainer.py:1161] 2022-06-13 07:24:01,138 >>   Gradient Accumulation steps = 1\u001B[0m\n",
      "\u001B[34m[INFO|trainer.py:1162] 2022-06-13 07:24:01,138 >>   Total optimization steps = 130\u001B[0m\n",
      "\u001B[34m#015  0%|          | 0/130 [00:00<?, ?it/s]#015  1%|          | 1/130 [00:02<05:34,  2.59s/it]#015  2%|▏         | 2/130 [00:03<04:20,  2.03s/it]#015  2%|▏         | 3/130 [00:03<03:24,  1.61s/it]#015  3%|▎         | 4/130 [00:04<02:47,  1.33s/it]#015  4%|▍         | 5/130 [00:05<02:24,  1.15s/it]#015  5%|▍         | 6/130 [00:06<02:05,  1.01s/it]#015  5%|▌         | 7/130 [00:06<01:54,  1.08it/s]#015  6%|▌         | 8/130 [00:07<01:43,  1.18it/s]#015  7%|▋         | 9/130 [00:08<01:34,  1.28it/s]#015  8%|▊         | 10/130 [00:08<01:27,  1.37it/s]#015  8%|▊         | 11/130 [00:09<01:22,  1.44it/s]#015  9%|▉         | 12/130 [00:09<01:18,  1.50it/s]#015 10%|█         | 13/130 [00:10<01:16,  1.52it/s]#015 11%|█         | 14/130 [00:11<01:13,  1.57it/s]#015 12%|█▏        | 15/130 [00:11<01:11,  1.61it/s]#015 12%|█▏        | 16/130 [00:12<01:10,  1.63it/s]#015 13%|█▎        | 17/130 [00:12<01:09,  1.62it/s]#015 14%|█▍        | 18/130 [00:13<01:10,  1.59it/s]#015 15%|█▍        | 19/130 [00:14<01:10,  1.58it/s]#015 15%|█▌        | 20/130 [00:14<01:08,  1.59it/s]#015 16%|█▌        | 21/130 [00:15<01:06,  1.64it/s]#015 17%|█▋        | 22/130 [00:16<01:11,  1.52it/s]#015 18%|█▊        | 23/130 [00:16<01:10,  1.53it/s]#015 18%|█▊        | 24/130 [00:17<01:10,  1.51it/s]#015 19%|█▉        | 25/130 [00:18<01:06,  1.57it/s]#015 20%|██        | 26/130 [00:18<01:04,  1.62it/s]#015 21%|██        | 27/130 [00:19<01:05,  1.58it/s]#015 22%|██▏       | 28/130 [00:19<01:05,  1.57it/s]#015 22%|██▏       | 29/130 [00:20<01:04,  1.57it/s]#015 23%|██▎       | 30/130 [00:21<01:02,  1.59it/s]#015 24%|██▍       | 31/130 [00:21<01:01,  1.61it/s]#015 25%|██▍       | 32/130 [00:22<01:02,  1.57it/s]#015 25%|██▌       | 33/130 [00:23<01:01,  1.57it/s]#015 26%|██▌       | 34/130 [00:23<01:02,  1.54it/s]#015 27%|██▋       | 35/130 [00:24<00:58,  1.61it/s]#015 28%|██▊       | 36/130 [00:24<00:56,  1.66it/s]#015 28%|██▊       | 37/130 [00:25<00:55,  1.68it/s]#015 29%|██▉       | 38/130 [00:26<00:54,  1.67it/s]#015 30%|███       | 39/130 [00:26<00:53,  1.70it/s]#015 31%|███       | 40/130 [00:27<00:55,  1.63it/s]#015 32%|███▏      | 41/130 [00:27<00:52,  1.68it/s]#015 32%|███▏      | 42/130 [00:28<00:51,  1.70it/s]#015 33%|███▎      | 43/130 [00:29<00:51,  1.70it/s]#015 34%|███▍      | 44/130 [00:29<00:51,  1.66it/s]#015 35%|███▍      | 45/130 [00:30<00:49,  1.71it/s]#015 35%|███▌      | 46/130 [00:30<00:47,  1.75it/s]#015 36%|███▌      | 47/130 [00:31<00:46,  1.77it/s]#015 37%|███▋      | 48/130 [00:31<00:45,  1.79it/s]#015 38%|███▊      | 49/130 [00:32<00:45,  1.78it/s]#015 38%|███▊      | 50/130 [00:33<00:47,  1.70it/s]#015 39%|███▉      | 51/130 [00:33<00:45,  1.72it/s]#015 40%|████      | 52/130 [00:34<00:46,  1.66it/s]#015 41%|████      | 53/130 [00:34<00:45,  1.71it/s]#015 42%|████▏     | 54/130 [00:35<00:43,  1.73it/s]#015 42%|████▏     | 55/130 [00:36<00:44,  1.69it/s]#015 43%|████▎     | 56/130 [00:36<00:43,  1.70it/s]#015 44%|████▍     | 57/130 [00:37<00:44,  1.66it/s]#015 45%|████▍     | 58/130 [00:37<00:43,  1.67it/s]#015 45%|████▌     | 59/130 [00:38<00:41,  1.70it/s]#015 46%|████▌     | 60/130 [00:38<00:40,  1.74it/s]#015 47%|████▋     | 61/130 [00:39<00:41,  1.66it/s]#015 48%|████▊     | 62/130 [00:40<00:40,  1.68it/s]#015 48%|████▊     | 63/130 [00:40<00:41,  1.61it/s]#015 49%|████▉     | 64/130 [00:41<00:42,  1.56it/s]#015 50%|█████     | 65/130 [00:42<00:40,  1.62it/s]#015 51%|█████     | 66/130 [00:42<00:38,  1.66it/s]#015 52%|█████▏    | 67/130 [00:43<00:37,  1.67it/s]#015 52%|█████▏    | 68/130 [00:43<00:37,  1.64it/s]#015 53%|█████▎    | 69/130 [00:44<00:36,  1.69it/s]#015 54%|█████▍    | 70/130 [00:44<00:34,  1.73it/s]#015 55%|█████▍    | 71/130 [00:45<00:34,  1.71it/s]#015 55%|█████▌    | 72/130 [00:46<00:33,  1.72it/s]#015 56%|█████▌    | 73/130 [00:46<00:34,  1.68it/s]#015 57%|█████▋    | 74/130 [00:47<00:32,  1.71it/s]#015 58%|█████▊    | 75/130 [00:47<00:31,  1.73it/s]#015 58%|█████▊    | 76/130 [00:48<00:30,  1.74it/s]#015 59%|█████▉    | 77/130 [00:49<00:30,  1.77it/s]#015 60%|██████    | 78/130 [00:49<00:29,  1.74it/s]#015 61%|██████    | 79/130 [00:50<00:29,  1.76it/s]#015 62%|██████▏   | 80/130 [00:50<00:28,  1.74it/s]#015 62%|██████▏   | 81/130 [00:51<00:27,  1.76it/s]#015 63%|██████▎   | 82/130 [00:51<00:27,  1.74it/s]#015 64%|██████▍   | 83/130 [00:52<00:27,  1.70it/s]#015 65%|██████▍   | 84/130 [00:53<00:26,  1.73it/s]#015 65%|██████▌   | 85/130 [00:53<00:25,  1.74it/s]#015 66%|██████▌   | 86/130 [00:54<00:25,  1.72it/s]#015 67%|██████▋   | 87/130 [00:54<00:25,  1.71it/s]#015 68%|██████▊   | 88/130 [00:55<00:24,  1.72it/s]#015 68%|██████▊   | 89/130 [00:56<00:24,  1.68it/s]#015 69%|██████▉   | 90/130 [00:56<00:23,  1.71it/s]#015 70%|███████   | 91/130 [00:57<00:23,  1.66it/s]#015 71%|███████   | 92/130 [00:57<00:23,  1.58it/s]#015 72%|███████▏  | 93/130 [00:58<00:22,  1.65it/s]#015 72%|███████▏  | 94/130 [00:59<00:21,  1.68it/s]#015 73%|███████▎  | 95/130 [00:59<00:20,  1.74it/s]#015 74%|███████▍  | 96/130 [01:00<00:19,  1.77it/s]#015 75%|███████▍  | 97/130 [01:00<00:19,  1.70it/s]#015 75%|███████▌  | 98/130 [01:01<00:18,  1.69it/s]#015 76%|███████▌  | 99/130 [01:02<00:19,  1.62it/s]#015 77%|███████▋  | 100/130 [01:02<00:18,  1.63it/s]#015 78%|███████▊  | 101/130 [01:03<00:18,  1.61it/s]#015 78%|███████▊  | 102/130 [01:03<00:17,  1.57it/s]#015 79%|███████▉  | 103/130 [01:04<00:16,  1.60it/s]#015 80%|████████  | 104/130 [01:05<00:16,  1.54it/s]#015 81%|████████  | 105/130 [01:05<00:15,  1.59it/s]#015 82%|████████▏ | 106/130 [01:06<00:14,  1.63it/s]#015 82%|████████▏ | 107/130 [01:07<00:14,  1.59it/s]#015 83%|████████▎ | 108/130 [01:07<00:13,  1.63it/s]#015 84%|████████▍ | 109/130 [01:08<00:12,  1.68it/s]#015 85%|████████▍ | 110/130 [01:08<00:11,  1.68it/s]#015 85%|████████▌ | 111/130 [01:09<00:11,  1.64it/s]#015 86%|████████▌ | 112/130 [01:10<00:10,  1.68it/s]#015 87%|████████▋ | 113/130 [01:10<00:10,  1.68it/s]#015 88%|████████▊ | 114/130 [01:11<00:09,  1.69it/s]#015 88%|████████▊ | 115/130 [01:11<00:08,  1.72it/s]#015 89%|████████▉ | 116/130 [01:12<00:08,  1.73it/s]#015 90%|█████████ | 117/130 [01:12<00:07,  1.68it/s]#015 91%|█████████ | 118/130 [01:13<00:06,  1.72it/s]#015 92%|█████████▏| 119/130 [01:14<00:06,  1.70it/s]#015 92%|█████████▏| 120/130 [01:14<00:05,  1.68it/s]#015 93%|█████████▎| 121/130 [01:15<00:05,  1.69it/s]#015 94%|█████████▍| 122/130 [01:15<00:04,  1.71it/s]#015 95%|█████████▍| 123/130 [01:16<00:04,  1.73it/s]#015 95%|█████████▌| 124/130 [01:17<00:03,  1.76it/s]#015 96%|█████████▌| 125/130 [01:17<00:02,  1.78it/s]#015 97%|█████████▋| 126/130 [01:18<00:02,  1.78it/s]#015 98%|█████████▊| 127/130 [01:18<00:01,  1.78it/s]#015 98%|█████████▊| 128/130 [01:19<00:01,  1.72it/s]#015 99%|█████████▉| 129/130 [01:19<00:00,  1.72it/s]#015100%|██████████| 130/130 [01:20<00:00,  1.73it/s][INFO|trainer.py:1352] 2022-06-13 07:25:21,602 >> \u001B[0m\n",
      "\u001B[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001B[0m\n",
      "\u001B[34m#015                                                 #015#015100%|██████████| 130/130 [01:20<00:00,  1.73it/s]#015100%|██████████| 130/130 [01:20<00:00,  1.62it/s]\u001B[0m\n",
      "\u001B[34m[INFO|trainer.py:1885] 2022-06-13 07:25:21,720 >> Saving model checkpoint to /opt/ml/model\u001B[0m\n",
      "\u001B[34m[INFO|configuration_utils.py:351] 2022-06-13 07:25:21,721 >> Configuration saved in /opt/ml/model/config.json\u001B[0m\n",
      "\u001B[34m[INFO|modeling_utils.py:889] 2022-06-13 07:25:25,854 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001B[0m\n",
      "\u001B[34m[INFO|tokenization_utils_base.py:1924] 2022-06-13 07:25:25,854 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001B[0m\n",
      "\u001B[34m[INFO|tokenization_utils_base.py:1930] 2022-06-13 07:25:25,855 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:907] 2022-06-13 07:25:25,974 >> ***** train metrics *****\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:912] 2022-06-13 07:25:25,974 >>   epoch                      =       10.0\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:912] 2022-06-13 07:25:25,974 >>   init_mem_cpu_alloc_delta   =      133MB\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:912] 2022-06-13 07:25:25,974 >>   init_mem_cpu_peaked_delta  =      962MB\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:912] 2022-06-13 07:25:25,974 >>   init_mem_gpu_alloc_delta   =     1355MB\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:912] 2022-06-13 07:25:25,974 >>   init_mem_gpu_peaked_delta  =        0MB\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:912] 2022-06-13 07:25:25,974 >>   train_mem_cpu_alloc_delta  =      210MB\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:912] 2022-06-13 07:25:25,975 >>   train_mem_cpu_peaked_delta =        0MB\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:912] 2022-06-13 07:25:25,975 >>   train_mem_gpu_alloc_delta  =     4066MB\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:912] 2022-06-13 07:25:25,975 >>   train_mem_gpu_peaked_delta =     1132MB\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:912] 2022-06-13 07:25:25,975 >>   train_runtime              = 0:01:20.46\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:912] 2022-06-13 07:25:25,975 >>   train_samples              =         50\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:912] 2022-06-13 07:25:25,975 >>   train_samples_per_second   =      1.616\u001B[0m\n",
      "\u001B[34m[INFO|trainer.py:516] 2022-06-13 07:25:26,042 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1_key.\u001B[0m\n",
      "\u001B[34m[INFO|trainer.py:2115] 2022-06-13 07:25:26,045 >> ***** Running Evaluation *****\u001B[0m\n",
      "\u001B[34m[INFO|trainer.py:2117] 2022-06-13 07:25:26,046 >>   Num examples = 18\u001B[0m\n",
      "\u001B[34m[INFO|trainer.py:2120] 2022-06-13 07:25:26,046 >>   Batch size = 4\u001B[0m\n",
      "\u001B[34m#015  0%|          | 0/5 [00:00<?, ?it/s]#015 40%|████      | 2/5 [00:00<00:00, 18.01it/s]#015 60%|██████    | 3/5 [00:00<00:00, 12.97it/s]#015 80%|████████  | 4/5 [00:00<00:00, 10.76it/s]#015100%|██████████| 5/5 [00:00<00:00, 10.32it/s]#015100%|██████████| 5/5 [00:00<00:00, 10.52it/s]\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:907] 2022-06-13 07:25:26,728 >> ***** eval metrics *****\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:912] 2022-06-13 07:25:26,728 >>   epoch                     =       10.0\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:912] 2022-06-13 07:25:26,728 >>   eval_accuracy             =        1.0\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:912] 2022-06-13 07:25:26,729 >>   eval_loss                 =        0.0\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:912] 2022-06-13 07:25:26,729 >>   eval_mem_cpu_alloc_delta  =        0MB\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:912] 2022-06-13 07:25:26,729 >>   eval_mem_cpu_peaked_delta =        0MB\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:912] 2022-06-13 07:25:26,729 >>   eval_mem_gpu_alloc_delta  =        0MB\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:912] 2022-06-13 07:25:26,729 >>   eval_mem_gpu_peaked_delta =       22MB\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:912] 2022-06-13 07:25:26,729 >>   eval_runtime              = 0:00:00.61\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:912] 2022-06-13 07:25:26,729 >>   eval_samples              =         18\u001B[0m\n",
      "\u001B[34m[INFO|trainer_pt_utils.py:912] 2022-06-13 07:25:26,729 >>   eval_samples_per_second   =     29.328\u001B[0m\n",
      "\u001B[34mrun_glue.py:510: FutureWarning: remove_columns_ is deprecated and will be removed in the next major version of datasets. Use Dataset.remove_columns instead.\n",
      "  predict_dataset.remove_columns_(\"label\")\u001B[0m\n",
      "\u001B[34m[INFO|trainer.py:516] 2022-06-13 07:25:26,797 >> The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1_key.\u001B[0m\n",
      "\u001B[34m[INFO|trainer.py:2115] 2022-06-13 07:25:26,800 >> ***** Running Prediction *****\u001B[0m\n",
      "\u001B[34m[INFO|trainer.py:2117] 2022-06-13 07:25:26,800 >>   Num examples = 18\u001B[0m\n",
      "\u001B[34m[INFO|trainer.py:2120] 2022-06-13 07:25:26,800 >>   Batch size = 4\u001B[0m\n",
      "\u001B[34m#015  0%|          | 0/5 [00:00<?, ?it/s]#015 40%|████      | 2/5 [00:00<00:00, 17.92it/s]#015 60%|██████    | 3/5 [00:00<00:00, 13.57it/s]#015 80%|████████  | 4/5 [00:00<00:00, 11.88it/s]#015100%|██████████| 5/5 [00:00<00:00, 10.56it/s]#015100%|██████████| 5/5 [00:00<00:00,  9.47it/s]\u001B[0m\n",
      "\n",
      "2022-06-13 07:25:48 Uploading - Uploading generated training model\n",
      "2022-06-13 07:30:09 Completed - Training job completed\n",
      "Training seconds: 653\n",
      "Billable seconds: 653\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "hyperparameters={'per_device_train_batch_size':4,\n",
    "                 'per_device_eval_batch_size': 4,\n",
    "                 'model_name_or_path': 'roberta-large',\n",
    "                 'train_file':'/opt/ml/input/data/train/train.csv',\n",
    "                 'validation_file':'/opt/ml/input/data/test/test.csv',\n",
    "                 'test_file':'/opt/ml/input/data/test/test.csv',\n",
    "                 'do_train': True,\n",
    "                 'do_predict': True,\n",
    "                 'do_eval': True,\n",
    "                 'save_total_limit':3,\n",
    "                 'num_train_epochs': 3,\n",
    "                 'output_dir': '/opt/ml/model',\n",
    "                 'num_train_epochs': 10,\n",
    "                 'learning_rate': 5e-5,\n",
    "                 'seed': 7,\n",
    "                 'fp16': False,\n",
    "                 'eval_steps': 1000,\n",
    "                 }\n",
    "\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "      entry_point='run_glue.py', # script\n",
    "      source_dir='./', # relative path to example\n",
    "      instance_type='ml.p3.2xlarge',\n",
    "      instance_count=1,\n",
    "      volume_size=500,\n",
    "      transformers_version='4.6',\n",
    "      pytorch_version='1.7',\n",
    "      py_version='py36',\n",
    "      role=role,\n",
    "      base_job_name='haofang-roberta-large-epoch10',\n",
    "      hyperparameters = hyperparameters\n",
    ")\n",
    "\n",
    "huggingface_estimator.fit({'train':training_input_path,'test':test_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdb30fe",
   "metadata": {},
   "source": [
    "# deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e35b80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data=huggingface_estimator.model_data,  # path to your trained sagemaker model\n",
    "   role=role, # iam role with permissions to create an Endpoint\n",
    "   transformers_version=\"4.6\", # transformers version used\n",
    "   pytorch_version=\"1.7\", # pytorch version used\n",
    "   py_version=\"py36\", # python version of the DLC\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96bb14b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!"
     ]
    }
   ],
   "source": [
    "predictor = huggingface_model.deploy(\n",
    "   initial_instance_count=1,\n",
    "   instance_type=\"ml.g4dn.xlarge\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0aac738a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.2 ms, sys: 60 µs, total: 13.3 ms\n",
      "Wall time: 695 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': '错误', 'score': 0.9999964833259583}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# example request, you always need to define \"inputs\"\n",
    "data = {\n",
    "   \"inputs\": \"Desktop Photo Printers: Liene Photo Printer Paper amp Cartridge Cartridge Refill amp Change\\xa0Modify Photo Paper 40 Pack\"\n",
    "}\n",
    "\n",
    "# request\n",
    "predictor.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44ac579",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}